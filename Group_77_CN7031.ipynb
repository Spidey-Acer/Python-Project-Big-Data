{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx9-Fre4FMda"
   },
   "source": [
    "# **Big Data Analytics [CN7031] CRWK 2024-25**\n",
    "# **Group ID: 77**\n",
    "- Student 1 (Name : Prasad Nalla Nalla, ID : u2697719 )\n",
    "- Student 2 (Name : Venkata Lakshminarasim NANDURI, ID : u2714568 )\n",
    "- Student 3 (Name : Ongoi Bonface ONGERI, ID : u2705226 )\n",
    "- Student 4 (Name : jayanth Sai Kishan PUJAR, ID : u2717764 )\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdMZR-9QTwG3"
   },
   "source": [
    "\n",
    "# **Initiate and Configure Spark**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wbXV70D6xbl",
    "outputId": "5967feb0-d9f0-4ccb-a76e-0dd7ec651280"
   },
   "outputs": [],
   "source": [
    "#import required PKGS\n",
    "# !pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "print(pyspark.__version__)\n",
    "#heck pyspark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "rnrxcLoCsyWo",
    "outputId": "ae5640a2-47c3-4c95-f056-82a1c326367c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RjT7_UHAqic"
   },
   "source": [
    "\n",
    "# **Task 1: Data Processing using PySpark DF [40 marks]**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ndqga6Qzm6HI"
   },
   "source": [
    "# Student 1 (Name : Prasad Nalla Nalla, ID : u2697719 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6P2CZVl6TOQX"
   },
   "source": [
    "### **Load Unstructured Data**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCszPXyxm6HL"
   },
   "source": [
    "## Student 1 (Name : Ongoi Bonface ONGERI, ID : u2705226 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stid1pg6m6HO",
    "outputId": "74cae1d5-41e6-4f59-b595-1d7b22df0d60"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, to_timestamp, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Parser with PySpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read log file\n",
    "logs_df = spark.read.text('web.log')\n",
    "\n",
    "# Regex pattern for parsing logs\n",
    "log_pattern = (\n",
    "    r'([\\d\\.]+) - - \\[([^\\]]+)\\] '\n",
    "    r'\"([A-Z]+) ([^\\s]+) ([^\\\"]+)\" '\n",
    "    r'(\\d{3}) (\\d+) (.+)'\n",
    ")\n",
    "\n",
    "# Parse logs into structured DataFrame\n",
    "parsed_df = logs_df.select(\n",
    "    regexp_extract('value', log_pattern, 1).alias('Host'),\n",
    "    regexp_extract('value', log_pattern, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', log_pattern, 3).alias('HTTP_Method'),\n",
    "    regexp_extract('value', log_pattern, 4).alias('URL'),\n",
    "    regexp_extract('value', log_pattern, 5).alias('HTTP_Version'),\n",
    "    regexp_extract('value', log_pattern, 6).alias('HTTP_Status_Code'),\n",
    "    regexp_extract('value', log_pattern, 7).alias('Bytes_in_Reply'),\n",
    "    regexp_extract('value', log_pattern, 8).alias('Message')\n",
    ").withColumn('Bytes_in_Reply', col('Bytes_in_Reply').cast('int')) \\\n",
    " .withColumn('HTTP_Status_Code', col('HTTP_Status_Code').cast('int')) \\\n",
    " .withColumn('Timestamp', to_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss'))\n",
    "\n",
    "# Display a preview of the parsed DataFrame\n",
    "parsed_df.limit(10).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "on780fzRm6HP",
    "outputId": "9626c8d3-24e2-41d4-9fdd-9590bd486402"
   },
   "outputs": [],
   "source": [
    "\n",
    "# HTTP Status Code Distribution\n",
    "http_status_counts = parsed_df.groupBy('HTTP_Status_Code').count().orderBy('HTTP_Status_Code')\n",
    "http_status_counts_pd = http_status_counts.toPandas()\n",
    "\n",
    "# Visualization for HTTP Status Code Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = sns.color_palette(\"tab10\", len(http_status_counts_pd))\n",
    "bars = plt.bar(\n",
    "    http_status_counts_pd['HTTP_Status_Code'].astype(str),\n",
    "    http_status_counts_pd['count'],\n",
    "    color=colors\n",
    ")\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height}', ha='center', va='bottom')\n",
    "plt.title('HTTP Status Code Distribution', fontsize=16)\n",
    "plt.xlabel('HTTP Status Code', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 549
    },
    "id": "jabVGGmem6HS",
    "outputId": "51afe4e7-a857-4f4a-c0ba-6e7a330ca16e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Total Bytes in Reply by HTTP Method\n",
    "bytes_by_method = parsed_df.groupBy('HTTP_Method') \\\n",
    "                           .agg(_sum('Bytes_in_Reply').alias('Total_Bytes')) \\\n",
    "                           .orderBy('Total_Bytes', ascending=False)\n",
    "bytes_by_method_pd = bytes_by_method.toPandas()\n",
    "\n",
    "# Bar chart for Bytes by HTTP Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(bytes_by_method_pd['HTTP_Method'], bytes_by_method_pd['Total_Bytes'], color=colors)\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width, bar.get_y() + bar.get_height() / 2, f'{width:,}', ha='left', va='center')\n",
    "plt.title('Total Bytes in Reply by HTTP Method', fontsize=16)\n",
    "plt.xlabel('Total Bytes', fontsize=12)\n",
    "plt.ylabel('HTTP Method', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "u9mDWwbYm6HT",
    "outputId": "17ff3b95-b5a8-4ff5-bc32-6faa222e2e7c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Line plot for trend analysis of bytes\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(bytes_by_method_pd['HTTP_Method'], bytes_by_method_pd['Total_Bytes'], marker='o', color='blue',\n",
    "         linestyle='-', linewidth=2, markersize=8, label='Total Bytes')\n",
    "for i, txt in enumerate(bytes_by_method_pd['Total_Bytes']):\n",
    "    plt.text(i, bytes_by_method_pd['Total_Bytes'][i], f'{txt:,}', fontsize=10, ha='center', va='bottom')\n",
    "plt.title('Trend Analysis: Total Bytes by HTTP Method', fontsize=16)\n",
    "plt.xlabel('HTTP Method', fontsize=12)\n",
    "plt.ylabel('Total Bytes', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb3hlQgIm6HU"
   },
   "source": [
    "# Student 2 (Name : Prasad Nalla Nalla, ID : u2697719 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbgwKR85m6HW",
    "outputId": "6431633b-283b-4a65-c73f-052a285a5b20"
   },
   "outputs": [],
   "source": [
    "## TASK 1\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WebLogAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the log file\n",
    "log_file_path = \"web.log\"\n",
    "logs_df = spark.read.text(log_file_path)\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"http_method\", StringType(), True),\n",
    "    StructField(\"http_status_code\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Extract fields using regex\n",
    "logs_with_metrics_df = logs_df.select(\n",
    "    regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),\n",
    "    regexp_extract('value', r'\"(.*?) (.*?) HTTP.*?\"', 1).alias('http_method'),\n",
    "    regexp_extract('value', r'\"(.*?) (.*?) HTTP.*?\"', 2).alias('http_status_code')\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "logs_with_metrics_df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZBXBEXkm6HX",
    "outputId": "99a00e03-2c5c-4e8a-ada3-f3339258d1ee"
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "logs_with_metrics_df.createOrReplaceTempView(\"logs\")\n",
    "# SQL query to calculate rolling hourly traffic\n",
    "hourly_traffic_query = \"\"\"\n",
    "SELECT\n",
    "    to_date(substring(timestamp, 1, 11), 'dd/MMM/yyyy') AS date,\n",
    "    COUNT(*) AS traffic_count,\n",
    "    SUM(CASE WHEN http_status_code = 200 THEN 1 ELSE 0 END) AS successful_requests\n",
    "FROM logs\n",
    "GROUP BY date\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "hourly_traffic_df = spark.sql(hourly_traffic_query)\n",
    "hourly_traffic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoLIFb4vm6HY",
    "outputId": "0a9cc7be-2928-411b-fac9-83ca3592008c"
   },
   "outputs": [],
   "source": [
    "# 1. Count Distinct HTTP Methods by Date\n",
    "daily_unique_visitors_query = \"\"\"\n",
    "SELECT\n",
    "    to_date(substring(timestamp, 1, 11), 'dd/MMM/yyyy') AS date,\n",
    "    COUNT(DISTINCT http_method) AS distinct_http_methods\n",
    "FROM logs\n",
    "GROUP BY date\n",
    "ORDER BY date\n",
    "\"\"\"\n",
    "daily_unique_visitors_df = spark.sql(daily_unique_visitors_query)\n",
    "daily_unique_visitors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0hvhQC0m6HZ",
    "outputId": "4924d534-bae4-4340-b7f9-368e0ec7e5ef"
   },
   "outputs": [],
   "source": [
    "# Count Distinct Visitors by HTTP Method\n",
    "daily_unique_visitors_by_method = \"\"\"\n",
    "SELECT\n",
    "to_date(substring(timestamp, 1, 11), 'dd/MMM/yyyy') AS date,\n",
    "http_method,\n",
    "COUNT(http_status_code) AS distinct_visits\n",
    "FROM logs\n",
    "GROUP BY date, http_method\n",
    "ORDER BY date, http_method\n",
    "\"\"\"\n",
    "# Execute the SQL query\n",
    "daily_unique_visitors_by_method_df = spark.sql(daily_unique_visitors_by_method)\n",
    "daily_unique_visitors_by_method_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0SxXVgFm6Ha"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "mfMf_Dt-m6Hb",
    "outputId": "a41fc7e8-633f-45ff-d6d9-f7721e523930"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "daily_unique_visitors_by_method_pd = daily_unique_visitors_by_method_df.toPandas()\n",
    "fig = px.line(\n",
    "    daily_unique_visitors_by_method_pd,\n",
    "    x='date',\n",
    "    y='distinct_visits',\n",
    "    color='http_method',\n",
    "    markers=True,\n",
    "    title='Distinct Visitors by HTTP Method Over Time',\n",
    "    labels={'distinct_visits': 'Distinct Visitors', 'date': 'Date'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Distinct Visitors',\n",
    "    legend_title='HTTP Method',\n",
    "    xaxis=dict(tickformat=\"%Y-%m-%d\"),\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSE7bNND4caH"
   },
   "source": [
    "# Student 3 (Name : Venkata Lakshminarasim NANDURI, ID : u2714568 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GqR8OAZlm6Hc",
    "outputId": "828dbeb2-b324-4140-e5c0-decb5c745e60"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WebLogAnalysisStudent_2\").getOrCreate()\n",
    "# Load the log file\n",
    "log_file_path = \"web.log\"\n",
    "logs_df = spark.read.text(log_file_path)\n",
    "\n",
    "# Extract fields using regex\n",
    "logs_with_metrics_df = logs_df.select(\n",
    "    regexp_extract('value', r'(\\d+\\.\\d+\\.\\d+\\.\\d+)', 1).alias('ip_address'),  # Extract IP address\n",
    "    regexp_extract('value', r'\\[(.*?)\\]', 1).alias('timestamp'),               # Extract timestamp\n",
    "    regexp_extract('value', r'\"(.*?) (.*?) HTTP.*?\"', 1).alias('http_method'), # HTTP method\n",
    "    regexp_extract('value', r'(\\d{3})\\s(\\d+)', 1).alias('http_status_code')   # HTTP status code\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view for querying\n",
    "logs_with_metrics_df.createOrReplaceTempView(\"Weblogs\")\n",
    "\n",
    "# Now, we can run the unique visitors query\n",
    "daily_unique_visitors_df = spark.sql('''SELECT * FROM Weblogs LIMIT 10''')\n",
    "daily_unique_visitors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klM1QslOm6Hd",
    "outputId": "a7974574-8dbb-4ab6-d92e-ac8dde7318f2"
   },
   "outputs": [],
   "source": [
    "# Total Post By Ip Address\n",
    "# Run the SQL query to get total visits by IP address ordered by date\n",
    "total_visits_by_ip_df = spark.sql('''\n",
    "    SELECT\n",
    "        ip_address,\n",
    "        TO_DATE(SUBSTRING(timestamp, 1, 11), 'dd/MMM/yyyy') AS visit_date,\n",
    "        COUNT(*) AS total_visits\n",
    "    FROM\n",
    "        Weblogs\n",
    "    GROUP BY\n",
    "        ip_address, visit_date\n",
    "    ORDER BY\n",
    "        visit_date, ip_address\n",
    "''')\n",
    "total_visits_by_ip_df.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5JGC_rKm6He",
    "outputId": "32de515b-3f08-4113-f329-aad7dc8959e0"
   },
   "outputs": [],
   "source": [
    "# Run the SQL query to calculate total successful requests (e.g., HTTP 200) by IP address ordered by date\n",
    "successful_visits_by_ip_df = spark.sql('''\n",
    "    SELECT\n",
    "        ip_address,\n",
    "        TO_DATE(SUBSTRING(timestamp, 1, 11), 'dd/MMM/yyyy') AS visit_date,\n",
    "        COUNT(*) AS successful_visits\n",
    "    FROM\n",
    "        Weblogs\n",
    "    WHERE\n",
    "        http_status_code = 200  -- Filter for successful status codes\n",
    "    GROUP BY\n",
    "        ip_address, visit_date\n",
    "    ORDER BY\n",
    "        visit_date, ip_address\n",
    "''')\n",
    "# Display the top 10 results\n",
    "successful_visits_by_ip_df.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "wv8a3zcNm6Hf",
    "outputId": "9c9230ba-f24d-484b-e791-c3958e4f2cfa"
   },
   "outputs": [],
   "source": [
    "successful_visits_by_ip_pd = successful_visits_by_ip_df.toPandas()\n",
    "successful_visits_by_ip_pd.groupby(['visit_date'])['successful_visits'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cUAKr2g8m6Hf",
    "outputId": "44c104e8-273f-4d92-8be9-2ddfdac86731"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Ensure you group the data properly and reset the index for plotting\n",
    "grouped_data = successful_visits_by_ip_pd.groupby(['visit_date'])['successful_visits'].sum().reset_index()\n",
    "display(grouped_data)\n",
    "# Plot the grouped data\n",
    "plt.figure(figsize=(10, 5))  # Use a smaller, reasonable size\n",
    "sns.lineplot(\n",
    "    x='visit_date',\n",
    "    y='successful_visits',\n",
    "    data=grouped_data,\n",
    "    marker='o',\n",
    "    palette='tab10'\n",
    ")\n",
    "\n",
    "# Adding labels and titles\n",
    "plt.title('Trend of Successful Visits Over Time by IP Address', fontsize=16)\n",
    "plt.xlabel('Visit Date', fontsize=12)\n",
    "plt.ylabel('Successful Visits', fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Use a fixed location for the legend\n",
    "plt.legend(title='IP Address', loc='upper right')\n",
    "plt.show()  # No tight_layout to avoid warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7AQAa574xnx"
   },
   "source": [
    "## Student 4 (Name : jayanth Sai Kishan PUJAR, ID : u2717764 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KsnRrDK4zRB",
    "outputId": "d04e0392-9e5c-442c-8c36-71f77e969b80"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, to_timestamp\n",
    "def create_spark_session(app_name, executor_memory, driver_memory, executor_cores, timeout):\n",
    "    \"\"\"Create and return a Spark session.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.executor.memory\", executor_memory) \\\n",
    "        .config(\"spark.driver.memory\", driver_memory) \\\n",
    "        .config(\"spark.executor.cores\", executor_cores) \\\n",
    "        .config(\"spark.python.worker.timeout\", timeout) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_log_data(spark, file_path):\n",
    "    \"\"\"Read log data from the specified file into a DataFrame.\"\"\"\n",
    "    return spark.read.text(file_path)\n",
    "\n",
    "def parse_log_fields(logs_df, log_pattern):\n",
    "    \"\"\"Extract log fields into separate columns.\"\"\"\n",
    "    return logs_df.select(\n",
    "        regexp_extract('value', log_pattern, 1).alias('Host'),\n",
    "        regexp_extract('value', log_pattern, 2).alias('Timestamp'),\n",
    "        regexp_extract('value', log_pattern, 3).alias('HTTP_Method'),\n",
    "        regexp_extract('value', log_pattern, 4).alias('URL')\n",
    "    )\n",
    "\n",
    "def convert_data_types(parsed_df):\n",
    "    \"\"\"Convert data types for analysis.\"\"\"\n",
    "    return (parsed_df\n",
    "            .withColumn('Timestamp', to_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss')))\n",
    "def main():\n",
    "    \"\"\"Main function to execute the log parsing.\"\"\"\n",
    "    spark = create_spark_session(\"Log Parser with PySpark\", \"4g\", \"4g\", \"2\", \"300\")\n",
    "    logs_df = read_log_data(spark, 'web.log')\n",
    "\n",
    "    log_pattern = (\n",
    "        r'([\\d\\.]+) - - \\[([^\\]]+)\\] '\n",
    "        r'\"([A-Z]+) ([^\\s]+) ([^\\\"]+)\" '\n",
    "        r'(\\d{3}) (\\d+) (.+)'\n",
    "    )\n",
    "    dataset = parse_log_fields(logs_df, log_pattern)\n",
    "    return convert_data_types(dataset)\n",
    "\n",
    "# Execute the main function and display the first 5 rows of the dataset\n",
    "dataset = main()\n",
    "dataset.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QG_0kGc8m6Hh",
    "outputId": "ce938c03-200d-4d1d-9db1-9ddefbabe6b4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import count, hour\n",
    "\n",
    "# Register DataFrame as a SQL table\n",
    "dataset.createOrReplaceTempView(\"logs\")\n",
    "# Query using window function\n",
    "hourly_traffic = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Timestamp,\n",
    "        COUNT(*) OVER (PARTITION BY HOUR(Timestamp) ORDER BY Timestamp) AS HourlyTraffic\n",
    "    FROM logs\n",
    "\"\"\")\n",
    "hourly_traffic.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rz-BwMGBm6Hi",
    "outputId": "b1cb4a62-f1ab-4108-ddd6-ff632cd7e6b0"
   },
   "outputs": [],
   "source": [
    "custom_dataset = dataset.withColumn('Timestamp', to_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss'))\n",
    "custom_dataset.createOrReplaceTempView(\"logs_with_host\")\n",
    "\n",
    "# Query for daily unique visitors\n",
    "daily_unique_visitors = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DATE(Timestamp) AS Date,\n",
    "        COUNT(DISTINCT Host) AS UniqueVisitors\n",
    "    FROM logs_with_host\n",
    "    GROUP BY DATE(Timestamp)\n",
    "    ORDER BY Date\n",
    "\"\"\")\n",
    "daily_unique_visitors.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "aG8zBfX_m6Hj",
    "outputId": "d714217f-a571-4af2-fa2c-59fe55a67f50"
   },
   "outputs": [],
   "source": [
    "##  Data  Visualization\n",
    "hourly_traffic_pd = hourly_traffic.toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_traffic_pd['Timestamp'], hourly_traffic_pd['HourlyTraffic'], marker='o', color='blue')\n",
    "plt.title('Rolling Hourly Traffic')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Hourly Traffic')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "oIGib106m6Hj",
    "outputId": "9a7fedaf-a8c7-4780-9772-a61f04ab37eb"
   },
   "outputs": [],
   "source": [
    "# Convert daily_unique_visitors to Pandas for visualization\n",
    "daily_unique_visitors_pd = daily_unique_visitors.toPandas()\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(daily_unique_visitors_pd['Date'], daily_unique_visitors_pd['UniqueVisitors'], color='green')\n",
    "plt.title('Daily Unique Visitors')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Unique Visitors')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcJhGbI2BKpx"
   },
   "source": [
    "\n",
    "# **Task 2 - Data Processing using PySpark RDD [40 marks]**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDEDGQOh450o"
   },
   "source": [
    "## Student 1 (Name : Prasad Nalla Nalla, ID : u2697719 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7GOt9-am6Hm",
    "outputId": "29c5c473-faa5-4545-a6f6-acc93f924635"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Configure Regex Pattern to Extract the dataset\n",
    "log_pattern = re.compile(r'(?P<Host>[\\d\\.]+) - - \\[(?P<Timestamp>[^\\]]+)\\] \"(?P<HTTP_Method>[A-Z]+) (?P<URL>[^\\s]+) (?P<HTTP_Version>[^\\\"]+)\" (?P<HTTP_Status_Code>\\d{3}) (?P<Bytes_in_Reply>\\d+) (?P<Message>.+)')\n",
    "\n",
    "# Reading the datafile logs\n",
    "with open('web.log', 'r') as file:\n",
    "    logs = file.readlines()\n",
    "\n",
    "# Logs Entries Parser Config\n",
    "parsed_logs = []\n",
    "for log in logs:\n",
    "    match = log_pattern.match(log)\n",
    "    if match:\n",
    "        parsed_logs.append(match.groupdict())\n",
    "\n",
    "# DataFrame Object Creation\n",
    "pdf = pd.DataFrame(parsed_logs)\n",
    "spark = SparkSession.builder.appName(\"LogDataProcessing\").getOrCreate()\n",
    "spark_df = spark.createDataFrame(pdf)\n",
    "spark_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIdO5kgJm6Hn",
    "outputId": "5a408c0a-c7a8-43b8-953b-965f7b533e99"
   },
   "outputs": [],
   "source": [
    "# Verify the conversion\n",
    "spark_df.limit(5).show()\n",
    "\n",
    "# One Basic RDD Analysis: Calculate Total Number of Requests\n",
    "rdd = spark_df.rdd\n",
    "total_requests = rdd.count()\n",
    "print(f\"Total Number of Requests: {total_requests}\")\n",
    "\n",
    "# Two Advanced RDD Analyses\n",
    "# 1. Identify Failed Requests\n",
    "failed_requests = rdd.filter(lambda x: x['HTTP_Status_Code'].startswith('4') or x['HTTP_Status_Code'].startswith('5')).count()\n",
    "print(f\"Total Failed Requests: {failed_requests}\")\n",
    "\n",
    "# 2. Calculate Average Response Time per URL (assuming Bytes_in_Reply represents response time)\n",
    "average_response_time = rdd.map(lambda x: (x['URL'], int(x['Bytes_in_Reply']))).aggregateByKey((0, 0),\n",
    "    lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])).mapValues(lambda x: x[0] / x[1])\n",
    "average_response_time.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92RPdoeV5SHz"
   },
   "source": [
    "## Student 2 (Name : Venkata Lakshminarasim NANDURI, ID : u2714568 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ_-hgdeiMle",
    "outputId": "c5f6589f-6bbb-45d1-be07-d0b1512c5d92"
   },
   "outputs": [],
   "source": [
    "# Task 2 - Student 2\n",
    "# One Basic RDD Analysis: Extract Unique IP Addresses\n",
    "unique_ips = rdd.map(lambda x: x['Host']).distinct().count()\n",
    "print(f\"Unique IP Addresses: {unique_ips}\")\n",
    "\n",
    "# Two Advanced RDD Analyses\n",
    "# 1. Hourly Visit Counts\n",
    "hourly_visits = rdd.map(lambda x: (x['Timestamp'].split(':')[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "hourly_visits.collect()\n",
    "\n",
    "# 2. Top 5 Most Visited URLs\n",
    "top_urls = rdd.map(lambda x: (x['URL'], 1)).reduceByKey(lambda a, b: a + b).takeOrdered(5, key=lambda x: -x[1])\n",
    "print(\"Top 5 Most Visited URLs:\", top_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7MY1leq5USZ"
   },
   "source": [
    "## Student 3 (Name : Ongoi Bonface ONGERI, ID : u2705226 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "2JGQHXYliMK5",
    "outputId": "debd3271-a4b3-4e7a-a94d-fc976f0771ab"
   },
   "outputs": [],
   "source": [
    "# Initialize (total size, count, min size, max size)\n",
    "response_size_stats = rdd.map(lambda x: (x['URL'], int(x['Bytes_in_Reply']) if x['Bytes_in_Reply'].isdigit() else 0)) \\\n",
    "    .aggregateByKey(\n",
    "        (0, 0, float('inf'), float('-inf')),\n",
    "        lambda acc, value: (\n",
    "            acc[0] + value,  # Total size\n",
    "            acc[1] + 1,      # Count\n",
    "            min(acc[2], value),  # Minimum size\n",
    "            max(acc[3], value)   # Maximum size\n",
    "        ),\n",
    "        lambda acc1, acc2: (\n",
    "            acc1[0] + acc2[0],  # Combine total sizes\n",
    "            acc1[1] + acc2[1],  # Combine counts\n",
    "            min(acc1[2], acc2[2]),  # Combine minimum sizes\n",
    "            max(acc1[3], acc2[3])   # Combine maximum sizes\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Response Size Statistics (URL):\")\n",
    "print(response_size_stats.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9iy_eRQqpik"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8G2vN3g5Vua"
   },
   "source": [
    "## Student 4 (Name : jayanth Sai Kishan PUJAR, ID : u2717764 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "A5mwMvIsBQlX",
    "outputId": "c3619bc3-852c-4d18-db9d-9701fdde6019"
   },
   "outputs": [],
   "source": [
    "# Task 2 - Student 4\n",
    "# One Basic RDD Analysis: Count Requests Per Status Code\n",
    "status_code_counts = rdd.map(lambda x: (x['HTTP_Status_Code'], 1)).reduceByKey(lambda a, b: a + b)\n",
    "status_code_counts.collect()\n",
    "\n",
    "# Two Advanced RDD Analyses\n",
    "# 1. Identify Peak Traffic Hours\n",
    "peak_hours = rdd.map(lambda x: (x['Timestamp'].split(':')[0], 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: -x[1])\n",
    "peak_hours.collect()\n",
    "\n",
    "# 2. Calculate User Retention\n",
    "user_retention = rdd.map(lambda x: (x['Host'], x['Timestamp'])).groupByKey().mapValues(lambda visits: len(set(visits)))\n",
    "user_retention.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHft1Jht1Qxl"
   },
   "source": [
    "# **(3) Optimization and LSEPI (Legal, Social, Ethical, and Professional Issues) Considerations [10 marks]**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95m9jb8f5d_s"
   },
   "source": [
    "## Student 1 (Name : Prasad Nalla Nalla, ID : u2697719 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dbo5dG25ra2"
   },
   "source": [
    "# Task 3 -\n",
    "**Different Partition Strategies**\n",
    "The concept of partition is an important one in computer science and different partition strategies are regularly used Different Partition Strategies Overview\n",
    "Partitioning is an important aspect of distributed data processing mechanisms that seek to split a massive dataset into sub-sets known as partitions. The motivation behind partitioning is to attempt the execution of operations in fragments of the whole because this is more efficient because in this way one can execute the operations in parallel in the various fragments. A usual approach is range partitioning, in which data is divided using some specific range key, which can be dates or numbers. This cuts down the time to perform range queries but it may possess some draw backs if the data is skewed.\n",
    "\n",
    "The next kind of partitioning is called a hash partitioning that divides data according to the results of the hash function in relation to one of the key columns. This technique is used to make sure that partitions contain equal amount of data, in order to ease the challenge of load balancing after a query. Still, its use may be suboptimal for range queries because data is not generally ordered, which may make the range query span across the entire disk. Another approach is called round-robin partitioning, which puts data in partitions in turn and is easy but not suitable for particular query types. The way and the criteria for deciding which strategy to use for data partitioning involves understanding the type of attributes in the data, the types of queries which are to be answered, and the amount of computation power available. All of the solutions have their strengths and weaknesses, and the best one should be selected depending on such parameters as query workload, data distribution, and system characteristics.\n",
    "\n",
    "**Caching vs. No Caching**\n",
    "Caching is an optimization method of storing computed value intermediate or often used data that can be used to enhance the speed of data retrieval instead of computing it afresh or going to the disk. In caching if a data operation is done, the result is stored in memory so when the data is needed again it will be retrieved from memory rather than the hard disk which can be time consuming. This is important for iteration, repeated query, large data set where disk read and computation are time consuming. In fact, cachingFaTap 2: Key points that need to be addressed To make caching effective, results need to be stored in memory so that processing time can be decreased significantly.\n",
    "\n",
    "On the other hand, no caching implies that every subsequent data operation or query has to be done from the first point, or in other words from the start in some cases; this is not ideal especially several similar data points are used. In the absence of caching, every time a query is run, the program needs to go back to disk or compute something costly. No caching can guarantee that only fresh data is utilized but the use of cache results in increased latency and slower response time when running more complicated queries or using Enormous data. caching in general pays off in terms of memory consumption when dealing with repetitive data access patters or large data sets but comes with memory consumption issues to consider. Caching is when frequently used data is stored temporarily in a computer’s memory, and when poor eviction methods are used caching maybe overdone and may lead to issues such as memory overflow reducing the whole systems performance. Thus, depending on the specific requirements of an application caching should be used or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQpYG-4k5rrq"
   },
   "source": [
    "## Student 2 (Name : Venkata Lakshminarasim NANDURI, ID : u2714568 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZTAGJiz5tIX"
   },
   "source": [
    "# Task 3\n",
    "**Caching vs. No Caching**\n",
    "Caching strikes a higher notch in performance because it saves frequently or at some point intermediate accessed data in memory. As a way of caching computation and disk access, the operations eliminate unnecessary computations and disk reads, making it efficient for use in iterative queries and repeated access largely. This results to better latency and increased effectiveness particularly when dealing with vast data.\n",
    "On the other hand, in scenarios where caching is not applied, each query, or operation is carried out from the bottom up, which augments time required for processing and overloads the I/O resources. This approach assures the data used is the most up to date one though it hags some draw backs especially in repeated queries. So, to make the right decision regarding the usage of caching, it is strategic to address the aspects that are associated with memory and performance, and the type of workload that will be addressed by an application.\n",
    "\n",
    "**Bucketing and Indexing**\n",
    "Optimization methods associated with the process of bucketing and indexing are useful approaches that improve data architecture and rapid data search. Bucketing applies a fixed size bucketing on a column hashed value which enables minimal data shuffling and for joining operations. Over large data sets, this method is particularly useful especially when issues like join or aggregation on bucketed columns.\n",
    "\n",
    "Indexing, on the other hand, creates a searchable paradigm over the set of data, while substantially enhancing query responses rate. Most databases allow results of certain queries to be stored as indexes of the database so the location of the rows that match the query can be located in B-trees or bitmap indexes, for instance, and does not require going through the entire database. In attaching and bucketing both helps in reducing the amount of time taken by the query and manages the number of accesses to the indexes and buckets making it compulsory for large data handling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thZJwceS5tX7"
   },
   "source": [
    "## Student 3 (Name : Ongoi Bonface ONGERI, ID : u2705226 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOFn2U7F5urh"
   },
   "source": [
    "# Task 3\n",
    "**Caching vs. No Caching**\n",
    "In memory-metrics compute frameworks such as Apache Spark, caching is one of the most important optimizations for data operations as data grows in size. Acronyms for argument mean caching which is a method as Spark in which partial results, once derived, are stored in memory for future use. This has the advantage of saving time done through repetitive computation and disk read hence its would be most effective in iterative jobs where the same data sets are repeatedly used. Spark has an array possibility for cache, such as MEMORY_ONLY, MEMORY_AND_DISK, and DISK_ONLY, which decide the data storing manner according to the resource quantity available.\n",
    "\n",
    "However, if caching is not applied, even the most straightforward operation on a DataFrame or RDD must recalculate the whole result from the beginning. This can result in a significant amount of performance overhead where several of these transformations will kick off new jobs to extract and process data over again. The processing time is also affected with large or complex datasets which are both time-consuming and a pressure for I/O disk resources. This is because caching them is not possible, and as a consequence, time elapsed during the execution of subsequent operations on the same data collection raises computational inefficiency.\n",
    "\n",
    "In the end then, while caching imperative can actually reduce the execution time for any repetitive processes, it normally has its own set of problems. Caching occupies memory and when the size of the dataset workout the memory’s capacity, there arise OOM and Spark has to resort to disk which nullifies storage in-memory. Thus, caching provides the best results in those cases where the data is used repeatedly, the algorithms are recurrent, and the available memory allows for using the cached data. Calculate the effectiveness of caching by comparing the running time of using caching algorithms with the running time of using the algorithms without caching so as to show the distinct gains in performance realized by a proper application of caching algorithms.\n",
    "\n",
    "**Bucketing Vs Indexing**\n",
    "Although bucketing and indexing are considerations aimed at improving data operations, operation efficiency especially when dealing with large amount of data. Bucketing means splitting a dataset into convenient portions by taking the hash of a given column. This method serves to decrease the amount of information that must be read during query processing because the records usually related are grouped in the same bucket. As a result, bucketing enhances the effectiveness of activities for example joins and aggregations in big data because it reduces on the amount of data movement necessary for the task. That way, an increase in the amount of data and the computational load for query execution can be minimized.\n",
    "\n",
    "On the other hand, the indexing technique could allow creation of a search index on one or more columns in the standard instantiating a record into the dataset for the purpose of record retrieval without having to use the indexing technique to scan the entire dataset. Indexes can be described as a directory to help find records that meet certain condition quickly which, make them more suitable for operations such as sorting, filtering, and lookups on volumes of tables. As a result, through the use of indexing, databases are able to respond to queries within a shorter time than it would have taken if the system would have to search over most of the records for the same information. This capability is most crucial in situations where access to the vast data is important in sustaining performance and user satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uX-rH0Uz5u-2"
   },
   "source": [
    "## Student 4 (Name : jayanth Sai Kishan PUJAR, ID : u2717764 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gu3ere9c5wJ4"
   },
   "source": [
    "# Task 3\n",
    "**Caching vs. No Caching** (5 Marks)\n",
    "For memory centric compute frameworks such as Apache Spark, it is important to improve the performance of data operations especially when handling big data.\n",
    " Caching is one of the best optimization techniques in spark since the system is able to store intermediate values in cache for higher retrieval rate.\n",
    " The evaluation of this study therefore seeks to compare Caching and No Caching impacts with regards to data operations in Spark.\n",
    "\n",
    "- What is Caching?\n",
    "In Spark, Caching is a form of result storing in memory after a specific computation has been arrived at for the first time. If a DataFrame or RDD is cached,\n",
    "then Spark continues to keep the data in memory available to later stages of the computation. This means that rather than each time the data was requested Spark accesses\n",
    "and recompute it, it just accesses it directly from the memory cache and thus takes much less time and is more efficient especially in iterative jobs.\n",
    "Cache is mostly effective where a data set is reused because Spark does not have to compute and read the data set from disk again. Spark partitioning caching\n",
    "levels include MEMORY_ONLY, MEMORY_AND_DISK and DISK_ONLY which determines where data should be stored whether it is in memory or disk depending on the available resource.\n",
    "\n",
    "- No Cache (No Optimisation)\n",
    "If caching is not done, every time an operation is to be performed on a DataFrame / RDD, Spark will recompute data from ground up. This can lead to a lot of performance overhead,\n",
    "sometimes exorbitant when dealing with large systems or large complex sets of data. Spark doesn’t keep in-memory data; every transformation or action they make, fires multiple job,\n",
    "extract data, transform and generate result.\n",
    "For example Spark is programmed to recalculate each transformation if the data set is filtered or grouped more than once or undergoes whatever transformation is required.\n",
    " This can lead to considerable amount of wastage when the same data set is manipulated many times over. In cases where a certain operation is sensitive to the number of times\n",
    " it must be carried out at different stages of the workflow, the cumulative time it will spend in reprocessing, reduces the overall operating speed.\n",
    "\n",
    "\n",
    " - With Caching (Optimization)\n",
    "Caching, when applied, means that Spark will store the results to memory after its first use. This affords a bypass of repeating the same operations since the cached data is used in subsequent actions.\n",
    " It can be stored in memory, on disk and or both depending on the caching level that has been adopted. In cases where the same data is later used or when the data is a target of mutiple actions then caching is beneficial for instance caching in iterative learning algorithms.\n",
    "\n",
    "When a dataset is cached, Spark avoids recomputing the value and improves the efficiency on the usage of the resource; hence, it improves the rate at which the dataset is computed.\n",
    "For instance, in case when you manipulate data, running some procedures involving the same data set more than one time, the data set will be cached at the first run.\n",
    "Subsequent operations on that dataset will pull from the cache created by this process, increasing the speed at which such data is recovered.\n",
    "This is especially beneficial in those platforms and systems wherein I/O operation is costly, it helps in avoiding multiple read operations on disks or calculations.\n",
    "\n",
    "\n",
    "- Performance Impact\n",
    "The main benefit of caching is the cut of the time needed to execute operations that occurred multiple times. When the caching is uses the time used in computing for caching may take a\n",
    " while but the subsequent operations will be selecting since data access is fast. However, in case we don’t cache, each operation demands Spark to compute the whole dataset all over again, resulting into longer processing time.\n",
    "\n",
    "Still, the role of caching multiplies if we are dealing with iterative algorithms that often utilise in machine learning or graph processing. In these cases, caching enables the interrelated\n",
    "components or subproducts of a computation to be stored in memory so each aliquot round can occur more rapidly, without the additional time it takes to repeat calculations familiar to it.\n",
    "\n",
    "However, caching is not for everyone. It comes with trade-offs. Caching consumes memory resources and whenever data is larger enough to be stored in memory, it causes OOM errors or else\n",
    "spark starts to spill the data to the disk defeating the purpose of caching. Hence, caching should only be done if there is availability of systematic resources and that the data being processed is very sensitive.\n",
    "\n",
    "- When Caching is Most Effective\n",
    "Caching is most effective when:\n",
    "It is necessary to use the data for frequent returns. For example, in situations where simultaneous operations or actions are performed over a data set, caching is able to spare the time generally used to perform computations on the same data set.\n",
    "Algorithms are being recurrent in nature or algorithms are being iterative. In ml and graph processing, where traversal is iterative and passes over same pieces of data may be in series or parallel, caching helps in storing such results in cache memory.\n",
    "They can comfortably hold small to medium sized data sets within memory. When working with larger data sets caching may have to be supported by other optimization techniques such as partitioning or data pruning to facilitate efficient use of memory.\n",
    "However, caching does not appear to have much of a positive effect in cases in which the data is used only once or when the system requires less amount of memory. In such situations, one may think that caching (for example, storing data in memory)\n",
    "may be ineffective, and in this case, the operations can be made without caching.\n",
    "\n",
    "- Measuring the Impact: Execution Time Comparison\n",
    "Cache performance can therefore be evaluated by comparing the operation time of the operations with the cache and without the cache. When, for example, caching is involved, we are usually able to perceive significant decrease of the repeated operation time.\n",
    "Thanks to Spark, one can get the information out of memory directly, rather than computationally retrieve it reducing the time to process big data and/or perform complex transformations.\n",
    "\n",
    "The usefulness of caching can be measured by making application of the same series of transformations and actions on a dataset without and with caching, and measuring the time taken. This can be further analyzed using his her Web UI or logging functions to track that job execution of the stages and the time taken for each.\n",
    "\n",
    "**Bucketing and Indexing Overview**\n",
    "\n",
    "Bucketing and indexing are two optimization methods which can improve the speed of perform data operations with large data sets. Buckets require partitioning the entire dataset into easily manageable divided segments from the result of a hash function applied to a particular column of data. It aids in minimizing the quantity of the data that has to be examined in that queries since data in the same bucket is likely to be stored together. Bucketing can greatly enhance the performance of join and aggregations since all associated data is stored in the same bucket and hence do not require lots of shuffles in order to be processed in a query.\n",
    "Another method referred to as indexing also enhances the query performance time by producing an index of one or more columns of a data set. An index enables the system to find a specific record rather than going through the whole record in the search for the data. This is especially useful in query filtering, sorting and doing lookup on massive tables. While in bucketing groups are created into set partitions for storage purposes, indexing provide fast search and retrieval. When properly employed, any of the two techniques can minimize the number of I/O operations as well as the amount of computation needed, while improving response times to queries and general performance. However, they also need rigorous designing since overuse of indexing and bucketing might bring overhead in storing and managing the collected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIM6uLApSxi2"
   },
   "source": [
    "# **Convert ipynb to HTML for Turnitin submission [10 marks]**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrQu11N_DCfZ",
    "outputId": "9644e019-53cc-4a44-dd6a-6995a19352d8"
   },
   "outputs": [],
   "source": [
    "# install nbconvert\n",
    "!pip3 install nbconvert\n",
    "# convert ipynb to html\n",
    "!jupyter nbconvert Group_77_CN7031.ipynb --to html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Lx9-Fre4FMda"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "daviski",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
