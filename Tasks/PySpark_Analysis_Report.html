<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PySpark Coursework Report - Revamped</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        line-height: 1.6;
        margin: 0;
        padding: 0 20px;
      }
      header,
      footer {
        background: #333;
        color: white;
        text-align: center;
        padding: 10px 0;
      }
      section {
        margin-bottom: 20px;
      }
      article {
        margin-bottom: 20px;
      }
      pre {
        background: #f4f4f4;
        padding: 10px;
        border: 1px solid #ddd;
        overflow-x: auto;
      }
      code {
        background: #f4f4f4;
        padding: 2px 4px;
        border-radius: 4px;
      }
      h1,
      h2,
      h3 {
        color: #333;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>PySpark Coursework Report - Revamped</h1>
    </header>

    <section>
      <h2>Introduction</h2>
      <p>
        This report presents the implementation of PySpark data processing tasks
        assigned to team members Navya Athoti, Phalguna Avalagunta, Nikhil Sai
        Damera, and Sai Kishore Dodda.
      </p>
    </section>

    <section>
      <h2>Individual Contributions</h2>

      <article>
        <h3>Navya Athoti</h3>
        <p>
          <strong>Task:</strong> DF Creation with REGEX to extract IP address,
          timestamp, and HTTP method.
        </p>
        <pre><code>
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType
import re

# Initialize Spark session
spark = SparkSession.builder.appName("NavyaAthoti_Regex").getOrCreate()

# Sample log data
data = [
    "127.0.0.1 - - [10/Dec/2024:13:55:36 +0000] \\"GET /index.html HTTP/1.1\\" 200 2326",
]

# Define schema and REGEX
schema = StructType([
    StructField("ip", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("method", StringType(), True)
])

log_regex = r'(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\"([A-Z]+)'

# Parse logs
rdd = spark.sparkContext.parallelize(data)
parsed_rdd = rdd.map(lambda line: re.search(log_regex, line).groups())
df = spark.createDataFrame(parsed_rdd, schema)
df.show()
        </code></pre>
      </article>

      <article>
        <h3>Phalguna Avalagunta</h3>
        <p>
          <strong>Task:</strong> Advanced DataFrame Analysis using window
          functions for rolling hourly traffic.
        </p>
        <pre><code>
from pyspark.sql import SparkSession
from pyspark.sql.functions import window

# Initialize Spark session
spark = SparkSession.builder.appName("PhalgunaAvalagunta_Analysis").getOrCreate()

# Sample DataFrame
data = [
    ("2024-12-10 10:00:00", "192.168.1.1", 200, 1234),
    ("2024-12-10 10:01:00", "192.168.1.2", 404, 2345),
]
schema = ["timestamp", "ip", "status", "size"]
df = spark.createDataFrame(data, schema)

# Rolling hourly traffic analysis
df.groupBy(window("timestamp", "1 hour")).count().show()
        </code></pre>
      </article>

      <article>
        <h3>Nikhil Sai Damera</h3>
        <p>
          <strong>Task:</strong> Basic RDD Analysis for parsing and extracting
          log information.
        </p>
        <pre><code>
from pyspark import SparkContext

# Initialize Spark context
sc = SparkContext()

# Sample data
data = [
    "127.0.0.1 - - [10/Dec/2024:13:55:36 +0000] \\"GET /index.html HTTP/1.1\\" 200 2326",
]

# Parse logs
rdd = sc.parallelize(data).map(lambda line: line.split(" "))
rdd.foreach(print)
        </code></pre>
      </article>

      <article>
        <h3>Sai Kishore Dodda</h3>
        <p>
          <strong>Task:</strong> Optimization techniques including caching for
          performance improvement.
        </p>
        <pre><code>
from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("SaiKishoreDodda_Optimization").getOrCreate()

# Sample DataFrame
data = [
    ("2024-12-10 10:00:00", "192.168.1.1", 200, 1234),
    ("2024-12-10 10:01:00", "192.168.1.2", 404, 2345),
]
schema = ["timestamp", "ip", "status", "size"]
df = spark.createDataFrame(data, schema)

# Apply caching
df.cache()
df.show()
        </code></pre>
      </article>
    </section>

    <footer>
      <p>&copy; 2024 PySpark Team</p>
    </footer>
  </body>
</html>
