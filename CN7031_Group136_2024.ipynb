{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n",
      "\n",
      "Student 1 Analysis - Web Traffic Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Data Quality Check:\n",
      "Total Records: 3000000\n",
      "Null Values: 0\n",
      "\n",
      "Hourly Traffic Pattern Sample:\n",
      "+--------------------+--------------+-------------+\n",
      "|              window|    IP_Address|request_count|\n",
      "+--------------------+--------------+-------------+\n",
      "|{2022-01-01 00:00...| 169.29.157.48|            1|\n",
      "|{2022-01-01 00:00...|213.11.169.161|            1|\n",
      "|{2022-01-01 00:00...|   95.4.86.192|            1|\n",
      "|{2022-01-01 00:00...|  207.75.52.45|            1|\n",
      "|{2022-01-01 00:00...| 38.254.36.191|            1|\n",
      "+--------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "HTTP Method Distribution:\n",
      "+-----------+--------------+----------+\n",
      "|HTTP_Method|total_requests|unique_ips|\n",
      "+-----------+--------------+----------+\n",
      "|        GET|       1001043|   1000932|\n",
      "|       POST|       1000505|   1000390|\n",
      "|        PUT|        998452|    998328|\n",
      "+-----------+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 2 Analysis - Response Analysis\n",
      "==================================================\n",
      "\n",
      "Student 3 Analysis - URL Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 3 DataFrame structure:\n",
      "root\n",
      " |-- URL_Path: string (nullable = true)\n",
      " |-- IP_Address: string (nullable = true)\n",
      " |-- Response_Size: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------+----------+-------------+\n",
      "|URL_Path|IP_Address|Response_Size|\n",
      "+--------+----------+-------------+\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "+--------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Analysis - Log Message Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 4 DataFrame structure:\n",
      "root\n",
      " |-- HTTP_Status_Code: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Log_Message: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+----------------+---------+-----------+\n",
      "|HTTP_Status_Code|Timestamp|Log_Message|\n",
      "+----------------+---------+-----------+\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "+----------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Session Analysis Sample:\n",
      "+---------+----------------+-----------------+\n",
      "|timestamp|session_requests|avg_response_size|\n",
      "+---------+----------------+-----------------+\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "+---------+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Response Size Analysis:\n",
      "+-----------+-------------+-----------------+-----------------+\n",
      "|Status_Code|request_count|avg_response_size|max_response_size|\n",
      "+-----------+-------------+-----------------+-----------------+\n",
      "|           |      3000000|             NULL|                 |\n",
      "+-----------+-------------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "Student 1 RDD Analysis - Traffic Pattern Mining\n",
      "==================================================\n",
      "\n",
      "Hourly Traffic Sample:\n",
      "01/Apr/2022:0: 1869 requests\n",
      "01/Apr/2022:1: 1839 requests\n",
      "01/Apr/2022:2: 729 requests\n",
      "01/Apr/2023:0: 1889 requests\n",
      "01/Apr/2023:1: 1879 requests\n",
      "\n",
      "IP Pattern Analysis Sample:\n",
      "\n",
      "IP: 220.182.78.75\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n",
      "\n",
      "IP: 143.238.50.180\n",
      "Total Requests: 1\n",
      "Method Distribution: {'POST': 1}\n",
      "\n",
      "IP: 155.22.118.135\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n",
      "\n",
      "Student 1 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 5.11 seconds\n",
      "Optimized execution time: 5.44 seconds\n",
      "Performance improvement: -6.49%\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 15.52 seconds\n",
      "Cached execution time: 5.19 seconds\n",
      "Caching improvement: 66.59%\n",
      "\n",
      "Student 2 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 19.54 seconds\n",
      "Cached execution time: 0.88 seconds\n",
      "Caching improvement: 95.48%\n",
      "\n",
      "Bucketing Strategy Evaluation\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.sql.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:419)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 398\u001b[0m\n\u001b[0;32m    395\u001b[0m     bucketed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery time with bucketing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucketed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 398\u001b[0m evaluate_bucketing_strategy_student2()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# Student 3 (Nikhil Sai Damera u2810262)\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStudent 3 Optimization Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 385\u001b[0m, in \u001b[0;36mevaluate_bucketing_strategy_student2\u001b[1;34m()\u001b[0m\n\u001b[0;32m    382\u001b[0m df_student2\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# Create bucketed table\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124mCREATE TABLE IF NOT EXISTS bucketed_logs\u001b[39m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124mUSING parquet\u001b[39m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124mCLUSTERED BY (Status_Code) INTO 4 BUCKETS\u001b[39m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;124mAS SELECT * FROM logs\u001b[39m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Measure query performance\u001b[39;00m\n\u001b[0;32m    393\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.sql.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:419)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "# Group ID: CN7031_Group136_2024\n",
    "\n",
    "# Student Information:\n",
    "# 1. Student 1: Navya Athoti u2793047@uel.ac.uk\n",
    "# 2. Student 2: Phalguna Avalagunta u2811669@uel.ac.uk\n",
    "# 3. Student 3: Nikhil Sai Damera u2810262@uel.ac.uk\n",
    "# 4. Student 4: Sai Kishore Dodda u2773584@uel.ac.uk\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_extract, col, window, count, avg, sum, \n",
    "    unix_timestamp, hour, date_format, countDistinct\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import max as spark_max # Avoid conflict with Python's max function\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session with optimized configuration\n",
    "def initialize_spark():\n",
    "    conf = SparkConf().setAppName('CN7031_Group136_2024') \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    \n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "    return sc, spark\n",
    "\n",
    "sc, spark = initialize_spark()\n",
    "\n",
    "# Load and validate the dataset\n",
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "data = load_data(spark, path=\"web.log\")\n",
    "\n",
    "# Task 1: Data Processing using PySpark DF [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Analysis - Web Traffic Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student1 = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)\"\n",
    "df_student1 = data.select(\n",
    "    regexp_extract('value', regex_student1, 1).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student1, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student1, 3).alias('HTTP_Method')\n",
    ").cache()  # Cache for performance\n",
    "\n",
    "# Validate extracted data\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total Records: {df_student1.count()}\")\n",
    "print(f\"Null Values: {df_student1.filter(col('IP_Address') == '').count()}\")\n",
    "\n",
    "# Advanced Analysis 1: Rolling Window Analysis (10 marks)\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withWatermark('timestamp', '1 hour') \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour'),\n",
    "        'IP_Address'\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window.start')\n",
    "\n",
    "print(\"\\nHourly Traffic Pattern Sample:\")\n",
    "windowed_traffic.show(5)\n",
    "\n",
    "# Advanced Analysis 2: HTTP Method Distribution (10 marks)\n",
    "method_distribution = df_student1 \\\n",
    "    .groupBy('HTTP_Method') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_requests'),\n",
    "        countDistinct('IP_Address').alias('unique_ips')\n",
    "    ).orderBy(col('total_requests').desc())\n",
    "\n",
    "print(\"\\nHTTP Method Distribution:\")\n",
    "method_distribution.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# For Student 1's visualization\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x.start)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create time series plot with proper column names\n",
    "    sns.lineplot(data=df_pandas, \n",
    "                x='time', \n",
    "                y='request_count',\n",
    "                marker='o')\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Request Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student1_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "# Modify the windowed traffic query\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window')\n",
    "\n",
    "# Create visualization\n",
    "create_traffic_visualization(windowed_traffic)\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Analysis - Response Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student2 = r\"\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]\"\n",
    "df_student2 = data.select(\n",
    "    regexp_extract('value', regex_student2, 1).alias('Status_Code'),\n",
    "    regexp_extract('value', regex_student2, 2).alias('Response_Size'),\n",
    "    regexp_extract('value', regex_student2, 3).alias('Timestamp')\n",
    ").cache()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Analysis - URL Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student3 = r\"\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)\"\n",
    "df_student3 = data.select(\n",
    "    regexp_extract('value', regex_student3, 1).alias('URL_Path'),\n",
    "    regexp_extract('value', regex_student3, 2).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student3, 3).alias('Response_Size')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 3 DataFrame structure:\")\n",
    "df_student3.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student3.show(5)\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Analysis - Log Message Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student4 = r\"\\\".*\\\" (\\d+) .*? \\[(.*?)\\] (.*)\"\n",
    "df_student4 = data.select(\n",
    "    regexp_extract('value', regex_student4, 1).alias('HTTP_Status_Code'),\n",
    "    regexp_extract('value', regex_student4, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student4, 3).alias('Log_Message')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 4 DataFrame structure:\")\n",
    "df_student4.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student4.show(5)\n",
    "\n",
    "# Advanced Analysis 1: Session Analysis (10 marks)\n",
    "session_analysis = df_student2 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('long')) \\\n",
    "    .withColumn(\n",
    "        'session_requests',\n",
    "        count('*').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)  # 30-minute window in seconds\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        'avg_response_size',\n",
    "        avg('Response_Size').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nSession Analysis Sample:\")\n",
    "session_analysis.select('timestamp', 'session_requests', 'avg_response_size').show(5)\n",
    "\n",
    "# Advanced Analysis 2: Response Size Analysis (10 marks)\n",
    "response_analysis = df_student2 \\\n",
    "    .groupBy('Status_Code') \\\n",
    "    .agg(\n",
    "        count('*').alias('request_count'),\n",
    "        avg('Response_Size').alias('avg_response_size'),\n",
    "        spark_max('Response_Size').alias('max_response_size')  # Use spark_max instead of max\n",
    "    ).orderBy('Status_Code')\n",
    "\n",
    "print(\"\\nResponse Size Analysis:\")\n",
    "response_analysis.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "def create_response_visualization(df):\n",
    "    # Convert to pandas\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert Status_Code to string for better plotting\n",
    "    df_pandas['Status_Code'] = df_pandas['Status_Code'].astype(str)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(\n",
    "        data=df_pandas,\n",
    "        x='Status_Code',\n",
    "        y='avg_response_size',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    \n",
    "    plt.title('Average Response Size by Status Code')\n",
    "    plt.xlabel('HTTP Status Code')\n",
    "    plt.ylabel('Average Response Size (bytes)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student2_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "create_response_visualization(response_analysis)\n",
    "\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 RDD Analysis - Traffic Pattern Mining\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic RDD Analysis: Parse and Extract (10 marks)\n",
    "def parse_log_entry(line):\n",
    "    import re\n",
    "    try:\n",
    "        pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\d+).*\\[(.*?)\\].*\\\"([A-Z]+)'\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            return {\n",
    "                'ip': match.group(1),\n",
    "                'timestamp': match.group(2),\n",
    "                'method': match.group(3)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "base_rdd = data.rdd.map(lambda x: x['value']) \\\n",
    "                   .map(parse_log_entry) \\\n",
    "                   .filter(lambda x: x is not None)\n",
    "\n",
    "# Advanced Analysis 1: Time-based Traffic Analysis (15 marks)\n",
    "hourly_traffic = base_rdd \\\n",
    "    .map(lambda x: (x['timestamp'][:13], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "\n",
    "print(\"\\nHourly Traffic Sample:\")\n",
    "for hour, count in hourly_traffic.take(5):\n",
    "    print(f\"{hour}: {count} requests\")\n",
    "\n",
    "# Advanced Analysis 2: IP-based Pattern Analysis (15 marks)\n",
    "ip_patterns = base_rdd \\\n",
    "    .map(lambda x: (x['ip'], x['method'])) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda methods: {\n",
    "        'total_requests': len(list(methods)),\n",
    "        'method_distribution': dict(pd.Series(list(methods)).value_counts())\n",
    "    })\n",
    "\n",
    "print(\"\\nIP Pattern Analysis Sample:\")\n",
    "for ip, stats in ip_patterns.take(3):\n",
    "    print(f\"\\nIP: {ip}\")\n",
    "    print(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print(\"Method Distribution:\", stats['method_distribution'])\n",
    "\n",
    "\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies (5 marks)\n",
    "def evaluate_partition_strategy():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline - Default partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.groupBy('IP_Address').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.repartition(8, 'IP_Address').groupBy('IP_Address').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy()\n",
    "\n",
    "# Method 2: Caching Strategy (5 marks)\n",
    "def evaluate_caching_strategy():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student1.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Method').count().count()\n",
    "    df_uncached.groupBy('IP_Address').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student1.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Method').count().count()\n",
    "    df_cached.groupBy('IP_Address').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy()\n",
    "\n",
    "# Continue with Students 2-4 Task 3 implementations\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student2():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student2.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('Status_Code').count().count()\n",
    "    df_uncached.groupBy('Response_Size').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student2.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('Status_Code').count().count()\n",
    "    df_cached.groupBy('Response_Size').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student2()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student2():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    # Create temporary view\n",
    "    df_student2.createOrReplaceTempView(\"logs\")\n",
    "    \n",
    "    # Create bucketed table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bucketed_logs\n",
    "    USING parquet\n",
    "    CLUSTERED BY (Status_Code) INTO 4 BUCKETS\n",
    "    AS SELECT * FROM logs\n",
    "    \"\"\")\n",
    "    \n",
    "    # Measure query performance\n",
    "    start_time = time.time()\n",
    "    spark.sql(\"SELECT Status_Code, COUNT(*) FROM bucketed_logs GROUP BY Status_Code\").show()\n",
    "    bucketed_time = time.time() - start_time\n",
    "    print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "\n",
    "evaluate_bucketing_strategy_student2()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies\n",
    "def evaluate_partition_strategy_student3():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student3.groupBy('URL_Path').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student3.repartition(10, 'URL_Path').groupBy('URL_Path').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student3()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student3():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    df_student3.createOrReplaceTempView(\"url_logs\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bucketed_url_logs\n",
    "    USING parquet\n",
    "    CLUSTERED BY (URL_Path) INTO 4 BUCKETS\n",
    "    AS SELECT * FROM url_logs\n",
    "    \"\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    spark.sql(\"SELECT URL_Path, COUNT(*) FROM bucketed_url_logs GROUP BY URL_Path\").show()\n",
    "    bucketed_time = time.time() - start_time\n",
    "    print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "\n",
    "evaluate_bucketing_strategy_student3()\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student4():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student4.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Status_Code').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student4.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Status_Code').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student4()\n",
    "\n",
    "# Method 2: Partition Strategies\n",
    "def evaluate_partition_strategy_student4():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student4.repartition(8, 'HTTP_Status_Code').groupBy('HTTP_Status_Code').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student4()\n",
    "\n",
    "# Clean up resources\n",
    "def cleanup():\n",
    "    try:\n",
    "        # Drop temporary tables\n",
    "        spark.sql(\"DROP TABLE IF EXISTS bucketed_logs\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS bucketed_url_logs\")\n",
    "        \n",
    "        # Stop Spark session\n",
    "        sc.stop()\n",
    "        print(\"\\nSpark session successfully closed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
