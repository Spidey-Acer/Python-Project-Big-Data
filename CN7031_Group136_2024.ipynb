{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Big Data Analytics [CN7031] CRWK 2024-25\\n# Group ID: CN7031_Group136_2024\\n\\n1. Student 1: Navya Athoti u2793047@uel.ac.uk\\n2. Student 2: Phalguna Avalagunta u2811669@uel.ac.uk\\n3. Student 3: Nikhil Sai Damera u2810262@uel.ac.uk\\n4. Student 4: Sai Kishore Dodda u2773584@uel.ac.uk\\n\\n---\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1 [Markdown]:\n",
    "\"\"\"\n",
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "# Group ID: CN7031_Group136_2024\n",
    "\n",
    "1. Student 1: Navya Athoti u2793047@uel.ac.uk\n",
    "2. Student 2: Phalguna Avalagunta u2811669@uel.ac.uk\n",
    "3. Student 3: Nikhil Sai Damera u2810262@uel.ac.uk\n",
    "4. Student 4: Sai Kishore Dodda u2773584@uel.ac.uk\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initiate and Configure Spark\\n---\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 [Markdown]:\n",
    "'''\n",
    "# Initiate and Configure Spark\n",
    "---\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 3 [Code]:\n",
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "import sys\n",
    "\n",
    "# environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = (SparkSession.builder\n",
    "            .appName('CN7031_Group136_2024')\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load Unstructured Data\\n---\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 [Markdown]:\n",
    "'''\n",
    "# Load Unstructured Data\n",
    "---\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 6 [Code]:\n",
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the data loading\n",
    "try:\n",
    "    data = load_data(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Task 1: Data Processing using PySpark DF [40 marks]\\n---\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 [Markdown]:\n",
    "'''\n",
    "# Task 1: Data Processing using PySpark DF [40 marks]\n",
    "---\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Student 1 (Navya Athoti u2793047)\\n- DF Creation with REGEX (10 marks)\\n- Two advanced DF Analysis (20 marks)\\n- Utilize data visualization (10 marks)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8 [Markdown]:\n",
    "'''\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "- DF Creation with REGEX (10 marks)\n",
    "- Two advanced DF Analysis (20 marks)\n",
    "- Utilize data visualization (10 marks)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 Analysis - Web Traffic Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Data Quality Check:\n",
      "Total Records: 3000000\n",
      "Null Values: 0\n",
      "\n",
      "Hourly Traffic Pattern Sample:\n",
      "+--------------------+--------------+-------------+\n",
      "|              window|    IP_Address|request_count|\n",
      "+--------------------+--------------+-------------+\n",
      "|{2022-01-01 00:00...| 169.29.157.48|            1|\n",
      "|{2022-01-01 00:00...|213.11.169.161|            1|\n",
      "|{2022-01-01 00:00...|122.246.208.74|            1|\n",
      "|{2022-01-01 00:00...|  207.75.52.45|            1|\n",
      "|{2022-01-01 00:00...|   95.4.86.192|            1|\n",
      "+--------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "HTTP Method Distribution:\n",
      "+-----------+--------------+----------+\n",
      "|HTTP_Method|total_requests|unique_ips|\n",
      "+-----------+--------------+----------+\n",
      "|        GET|       1001043|   1000932|\n",
      "|       POST|       1000505|   1000390|\n",
      "|        PUT|        998452|    998328|\n",
      "+-----------+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 2 Analysis - Response Analysis\n",
      "==================================================\n",
      "\n",
      "Student 3 Analysis - URL Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 3 DataFrame structure:\n",
      "root\n",
      " |-- URL_Path: string (nullable = true)\n",
      " |-- IP_Address: string (nullable = true)\n",
      " |-- Response_Size: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------+----------+-------------+\n",
      "|URL_Path|IP_Address|Response_Size|\n",
      "+--------+----------+-------------+\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "+--------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Analysis - Log Message Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 4 DataFrame structure:\n",
      "root\n",
      " |-- HTTP_Status_Code: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Log_Message: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+----------------+---------+-----------+\n",
      "|HTTP_Status_Code|Timestamp|Log_Message|\n",
      "+----------------+---------+-----------+\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "+----------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Session Analysis Sample:\n",
      "+---------+----------------+-----------------+\n",
      "|timestamp|session_requests|avg_response_size|\n",
      "+---------+----------------+-----------------+\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "+---------+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Response Size Analysis:\n",
      "+-----------+-------------+-----------------+\n",
      "|Status_Code|request_count|max_response_size|\n",
      "+-----------+-------------+-----------------+\n",
      "|           |      3000000|                 |\n",
      "+-----------+-------------+-----------------+\n",
      "\n",
      "Visualization created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 9 [Code]:\n",
    "print(\"\\nStudent 1 Analysis - Web Traffic Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student1 = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)\"\n",
    "df_student1 = data.select(\n",
    "    regexp_extract('value', regex_student1, 1).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student1, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student1, 3).alias('HTTP_Method')\n",
    ").cache()  # Cache for performance\n",
    "\n",
    "# Validate extracted data\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total Records: {df_student1.count()}\")\n",
    "print(f\"Null Values: {df_student1.filter(col('IP_Address') == '').count()}\")\n",
    "\n",
    "# Advanced Analysis 1: Rolling Window Analysis (10 marks)\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withWatermark('timestamp', '1 hour') \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour'),\n",
    "        'IP_Address'\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window.start')\n",
    "\n",
    "print(\"\\nHourly Traffic Pattern Sample:\")\n",
    "windowed_traffic.show(5)\n",
    "\n",
    "# Advanced Analysis 2: HTTP Method Distribution (10 marks)\n",
    "method_distribution = df_student1 \\\n",
    "    .groupBy('HTTP_Method') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_requests'),\n",
    "        countDistinct('IP_Address').alias('unique_ips')\n",
    "    ).orderBy(col('total_requests').desc())\n",
    "\n",
    "print(\"\\nHTTP Method Distribution:\")\n",
    "method_distribution.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# For Student 1's visualization\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x.start)\n",
    "    \n",
    "    # Ensure request_count is numeric\n",
    "    df_pandas['request_count'] = pd.to_numeric(df_pandas['request_count'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create time series plot with proper column names\n",
    "    sns.lineplot(data=df_pandas, \n",
    "                x='time', \n",
    "                y='request_count',\n",
    "                marker='o')\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Request Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student1_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "# Modify the windowed traffic query\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window')\n",
    "\n",
    "# Create visualization\n",
    "create_traffic_visualization(windowed_traffic)\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Analysis - Response Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student2 = r\"\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]\"\n",
    "df_student2 = data.select(\n",
    "    regexp_extract('value', regex_student2, 1).alias('Status_Code'),\n",
    "    regexp_extract('value', regex_student2, 2).alias('Response_Size'),\n",
    "    regexp_extract('value', regex_student2, 3).alias('Timestamp')\n",
    ").cache()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Analysis - URL Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student3 = r\"\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)\"\n",
    "df_student3 = data.select(\n",
    "    regexp_extract('value', regex_student3, 1).alias('URL_Path'),\n",
    "    regexp_extract('value', regex_student3, 2).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student3, 3).alias('Response_Size')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 3 DataFrame structure:\")\n",
    "df_student3.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student3.show(5)\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Analysis - Log Message Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student4 = r\"\\\".*\\\" (\\d+) .*? \\[(.*?)\\] (.*)\"\n",
    "df_student4 = data.select(\n",
    "    regexp_extract('value', regex_student4, 1).alias('HTTP_Status_Code'),\n",
    "    regexp_extract('value', regex_student4, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student4, 3).alias('Log_Message')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 4 DataFrame structure:\")\n",
    "df_student4.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student4.show(5)\n",
    "\n",
    "# Advanced Analysis 1: Session Analysis (10 marks)\n",
    "session_analysis = df_student2 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('long')) \\\n",
    "    .withColumn(\n",
    "        'session_requests',\n",
    "        count('*').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)  # 30-minute window in seconds\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        'avg_response_size',\n",
    "        avg('Response_Size').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nSession Analysis Sample:\")\n",
    "session_analysis.select('timestamp', 'session_requests', 'avg_response_size').show(5)\n",
    "\n",
    "# Advanced Analysis 2: Response Size Analysis (10 marks)\n",
    "response_analysis = df_student2 \\\n",
    "    .groupBy('Status_Code') \\\n",
    "    .agg(\n",
    "        count('*').alias('request_count'),\n",
    "        spark_max('Response_Size').alias('max_response_size')  # Use spark_max instead of max\n",
    "    ).orderBy('Status_Code')\n",
    "\n",
    "print(\"\\nResponse Size Analysis:\")\n",
    "response_analysis.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# Visualization code for Student 1\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime more safely\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x['start'] if isinstance(x, dict) else x.start)\n",
    "    \n",
    "    # Handle potential infinite values\n",
    "    df_pandas['request_count'] = pd.to_numeric(df_pandas['request_count'], errors='coerce')\n",
    "    df_pandas = df_pandas.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use plt instead of seaborn to avoid deprecation warning\n",
    "    plt.plot(df_pandas['time'], \n",
    "            df_pandas['request_count'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            markersize=6)\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern', fontsize=14, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Request Count', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization with higher DPI for better quality\n",
    "    plt.savefig('student1_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Make sure to add these imports at the top of your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modify the windowed traffic query to ensure clean data\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window') \\\n",
    "    .na.fill(0)  # Fill null values with 0\n",
    "\n",
    "# Create visualization\n",
    "try:\n",
    "    create_traffic_visualization(windowed_traffic)\n",
    "    print(\"Visualization created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 Analysis - Web Traffic Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Data Quality Check:\n",
      "Total Records: 3000000\n",
      "Null Values: 0\n",
      "\n",
      "Hourly Traffic Pattern Sample:\n",
      "+--------------------+---------------+-------------+\n",
      "|              window|     IP_Address|request_count|\n",
      "+--------------------+---------------+-------------+\n",
      "|{2022-01-01 00:00...| 213.11.169.161|            1|\n",
      "|{2022-01-01 00:00...|228.230.206.189|            1|\n",
      "|{2022-01-01 00:00...|  169.29.157.48|            1|\n",
      "|{2022-01-01 00:00...|  78.28.179.123|            1|\n",
      "|{2022-01-01 00:00...|   5.252.83.212|            1|\n",
      "+--------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "HTTP Method Distribution:\n",
      "+-----------+--------------+----------+\n",
      "|HTTP_Method|total_requests|unique_ips|\n",
      "+-----------+--------------+----------+\n",
      "|        GET|       1001043|   1000932|\n",
      "|       POST|       1000505|   1000390|\n",
      "|        PUT|        998452|    998328|\n",
      "+-----------+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 2 Analysis - Response Analysis\n",
      "==================================================\n",
      "\n",
      "Student 3 Analysis - URL Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 3 DataFrame structure:\n",
      "root\n",
      " |-- URL_Path: string (nullable = true)\n",
      " |-- IP_Address: string (nullable = true)\n",
      " |-- Response_Size: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------+----------+-------------+\n",
      "|URL_Path|IP_Address|Response_Size|\n",
      "+--------+----------+-------------+\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "|        |          |             |\n",
      "+--------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Analysis - Log Message Analysis\n",
      "==================================================\n",
      "\n",
      "Verifying Student 4 DataFrame structure:\n",
      "root\n",
      " |-- HTTP_Status_Code: string (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Log_Message: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample data:\n",
      "+----------------+---------+-----------+\n",
      "|HTTP_Status_Code|Timestamp|Log_Message|\n",
      "+----------------+---------+-----------+\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "+----------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Session Analysis Sample:\n",
      "+---------+----------------+-----------------+\n",
      "|timestamp|session_requests|avg_response_size|\n",
      "+---------+----------------+-----------------+\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "|     NULL|         3000000|             NULL|\n",
      "+---------+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Response Size Analysis:\n",
      "+-----------+-------------+-----------------+\n",
      "|Status_Code|request_count|max_response_size|\n",
      "+-----------+-------------+-----------------+\n",
      "|           |      3000000|                 |\n",
      "+-----------+-------------+-----------------+\n",
      "\n",
      "Visualization created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Cell 8 [Markdown]:\n",
    "'''\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "- DF Creation with REGEX (10 marks)\n",
    "- Two advanced DF Analysis (20 marks)\n",
    "- Utilize data visualization (10 marks)\n",
    "'''\n",
    "\n",
    "# Cell 9 [Code]:\n",
    "print(\"\\nStudent 1 Analysis - Web Traffic Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student1 = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)\"\n",
    "df_student1 = data.select(\n",
    "    regexp_extract('value', regex_student1, 1).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student1, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student1, 3).alias('HTTP_Method')\n",
    ").cache()  # Cache for performance\n",
    "\n",
    "# Validate extracted data\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total Records: {df_student1.count()}\")\n",
    "print(f\"Null Values: {df_student1.filter(col('IP_Address') == '').count()}\")\n",
    "\n",
    "# Advanced Analysis 1: Rolling Window Analysis (10 marks)\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withWatermark('timestamp', '1 hour') \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour'),\n",
    "        'IP_Address'\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window.start')\n",
    "\n",
    "print(\"\\nHourly Traffic Pattern Sample:\")\n",
    "windowed_traffic.show(5)\n",
    "\n",
    "# Advanced Analysis 2: HTTP Method Distribution (10 marks)\n",
    "method_distribution = df_student1 \\\n",
    "    .groupBy('HTTP_Method') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_requests'),\n",
    "        countDistinct('IP_Address').alias('unique_ips')\n",
    "    ).orderBy(col('total_requests').desc())\n",
    "\n",
    "print(\"\\nHTTP Method Distribution:\")\n",
    "method_distribution.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# For Student 1's visualization\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x.start)\n",
    "    \n",
    "    # Ensure request_count is numeric\n",
    "    df_pandas['request_count'] = pd.to_numeric(df_pandas['request_count'])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create time series plot with proper column names\n",
    "    sns.lineplot(data=df_pandas, \n",
    "                x='time', \n",
    "                y='request_count',\n",
    "                marker='o')\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Request Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student1_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "# Modify the windowed traffic query\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window')\n",
    "\n",
    "# Create visualization\n",
    "create_traffic_visualization(windowed_traffic)\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Analysis - Response Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student2 = r\"\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]\"\n",
    "df_student2 = data.select(\n",
    "    regexp_extract('value', regex_student2, 1).alias('Status_Code'),\n",
    "    regexp_extract('value', regex_student2, 2).alias('Response_Size'),\n",
    "    regexp_extract('value', regex_student2, 3).alias('Timestamp')\n",
    ").cache()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Analysis - URL Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student3 = r\"\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)\"\n",
    "df_student3 = data.select(\n",
    "    regexp_extract('value', regex_student3, 1).alias('URL_Path'),\n",
    "    regexp_extract('value', regex_student3, 2).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student3, 3).alias('Response_Size')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 3 DataFrame structure:\")\n",
    "df_student3.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student3.show(5)\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Analysis - Log Message Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student4 = r\"\\\".*\\\" (\\d+) .*? \\[(.*?)\\] (.*)\"\n",
    "df_student4 = data.select(\n",
    "    regexp_extract('value', regex_student4, 1).alias('HTTP_Status_Code'),\n",
    "    regexp_extract('value', regex_student4, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student4, 3).alias('Log_Message')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 4 DataFrame structure:\")\n",
    "df_student4.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student4.show(5)\n",
    "\n",
    "# Advanced Analysis 1: Session Analysis (10 marks)\n",
    "session_analysis = df_student2 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('long')) \\\n",
    "    .withColumn(\n",
    "        'session_requests',\n",
    "        count('*').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)  # 30-minute window in seconds\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        'avg_response_size',\n",
    "        avg('Response_Size').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nSession Analysis Sample:\")\n",
    "session_analysis.select('timestamp', 'session_requests', 'avg_response_size').show(5)\n",
    "\n",
    "# Advanced Analysis 2: Response Size Analysis (10 marks)\n",
    "response_analysis = df_student2 \\\n",
    "    .groupBy('Status_Code') \\\n",
    "    .agg(\n",
    "        count('*').alias('request_count'),\n",
    "        spark_max('Response_Size').alias('max_response_size')  # Use spark_max instead of max\n",
    "    ).orderBy('Status_Code')\n",
    "\n",
    "print(\"\\nResponse Size Analysis:\")\n",
    "response_analysis.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# Visualization code for Student 1\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime more safely\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x['start'] if isinstance(x, dict) else x.start)\n",
    "    \n",
    "    # Handle potential infinite values\n",
    "    df_pandas['request_count'] = pd.to_numeric(df_pandas['request_count'], errors='coerce')\n",
    "    df_pandas = df_pandas.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Use plt instead of seaborn to avoid deprecation warning\n",
    "    plt.plot(df_pandas['time'], \n",
    "            df_pandas['request_count'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            linewidth=2,\n",
    "            markersize=6)\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern', fontsize=14, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Request Count', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization with higher DPI for better quality\n",
    "    plt.savefig('student1_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Make sure to add these imports at the top of your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modify the windowed traffic query to ensure clean data\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window') \\\n",
    "    .na.fill(0)  # Fill null values with 0\n",
    "\n",
    "# Create visualization\n",
    "try:\n",
    "    create_traffic_visualization(windowed_traffic)\n",
    "    print(\"Visualization created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating visualization: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 RDD Analysis - Traffic Pattern Mining\n",
      "==================================================\n",
      "\n",
      "Hourly Traffic Sample:\n",
      "01/Apr/2022:0: 1869 requests\n",
      "01/Apr/2022:1: 1839 requests\n",
      "01/Apr/2022:2: 729 requests\n",
      "01/Apr/2023:0: 1889 requests\n",
      "01/Apr/2023:1: 1879 requests\n",
      "\n",
      "IP Pattern Analysis Sample:\n",
      "\n",
      "IP: 220.182.78.75\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n",
      "\n",
      "IP: 143.238.50.180\n",
      "Total Requests: 1\n",
      "Method Distribution: {'POST': 1}\n",
      "\n",
      "IP: 155.22.118.135\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 [Markdown]:\n",
    "'''\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "---\n",
    "'''\n",
    "\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 RDD Analysis - Traffic Pattern Mining\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic RDD Analysis: Parse and Extract (10 marks)\n",
    "def parse_log_entry(line):\n",
    "    import re\n",
    "    try:\n",
    "        pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\d+).*\\[(.*?)\\].*\\\"([A-Z]+)'\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            return {\n",
    "                'ip': match.group(1),\n",
    "                'timestamp': match.group(2),\n",
    "                'method': match.group(3)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "base_rdd = data.rdd.map(lambda x: x['value']) \\\n",
    "                   .map(parse_log_entry) \\\n",
    "                   .filter(lambda x: x is not None)\n",
    "\n",
    "# Advanced Analysis 1: Time-based Traffic Analysis (15 marks)\n",
    "hourly_traffic = base_rdd \\\n",
    "    .map(lambda x: (x['timestamp'][:13], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "\n",
    "print(\"\\nHourly Traffic Sample:\")\n",
    "for hour, count in hourly_traffic.take(5):\n",
    "    print(f\"{hour}: {count} requests\")\n",
    "\n",
    "# Advanced Analysis 2: IP-based Pattern Analysis (15 marks)\n",
    "ip_patterns = base_rdd \\\n",
    "    .map(lambda x: (x['ip'], x['method'])) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda methods: {\n",
    "        'total_requests': len(list(methods)),\n",
    "        'method_distribution': dict(pd.Series(list(methods)).value_counts())\n",
    "    })\n",
    "\n",
    "print(\"\\nIP Pattern Analysis Sample:\")\n",
    "for ip, stats in ip_patterns.take(3):\n",
    "    print(f\"\\nIP: {ip}\")\n",
    "    print(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print(\"Method Distribution:\", stats['method_distribution'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 2.65 seconds\n",
      "Optimized execution time: 2.52 seconds\n",
      "Performance improvement: 4.87%\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 6.63 seconds\n",
      "Cached execution time: 2.19 seconds\n",
      "Caching improvement: 66.96%\n",
      "\n",
      "Student 2 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 10.27 seconds\n",
      "Cached execution time: 0.40 seconds\n",
      "Caching improvement: 96.08%\n",
      "\n",
      "Student 3 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 0.20 seconds\n",
      "Optimized execution time: 0.96 seconds\n",
      "Performance improvement: -391.56%\n",
      "\n",
      "Student 4 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 5.69 seconds\n",
      "Cached execution time: 0.19 seconds\n",
      "Caching improvement: 96.62%\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 0.21 seconds\n",
      "Optimized execution time: 1.04 seconds\n",
      "Performance improvement: -387.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook CN7031_Group136_2024.ipynb to html\n",
      "[NbConvertApp] Writing 403863 bytes to CN7031_Group136_2024.html\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 [Markdown]:\n",
    "'''\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "---\n",
    "'''\n",
    "\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies (5 marks)\n",
    "def evaluate_partition_strategy():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline - Default partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.groupBy('IP_Address').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.repartition(8, 'IP_Address').groupBy('IP_Address').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy()\n",
    "\n",
    "# Method 2: Caching Strategy (5 marks)\n",
    "def evaluate_caching_strategy():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student1.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Method').count().count()\n",
    "    df_uncached.groupBy('IP_Address').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student1.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Method').count().count()\n",
    "    df_cached.groupBy('IP_Address').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy()\n",
    "\n",
    "# Continue with Students 2-4 Task 3 implementations\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student2():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student2.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('Status_Code').count().count()\n",
    "    df_uncached.groupBy('Response_Size').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student2.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('Status_Code').count().count()\n",
    "    df_cached.groupBy('Response_Size').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student2()\n",
    "\n",
    "def evaluate_bucketing_strategy_student2():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student2.select(\n",
    "            col(\"Status_Code\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\"),\n",
    "            col(\"Timestamp\").cast(\"string\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM logs GROUP BY Status_Code\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"Status_Code\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM bucketed_logs GROUP BY Status_Code\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies\n",
    "def evaluate_partition_strategy_student3():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student3.groupBy('URL_Path').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student3.repartition(10, 'URL_Path').groupBy('URL_Path').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student3()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student3():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student3.select(\n",
    "            col(\"URL_Path\").cast(\"string\"),\n",
    "            col(\"IP_Address\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"url_logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM url_logs GROUP BY URL_Path\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"URL_Path\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_url_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM bucketed_url_logs GROUP BY URL_Path\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student4():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student4.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Status_Code').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student4.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Status_Code').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student4()\n",
    "\n",
    "# Method 2: Partition Strategies\n",
    "def evaluate_partition_strategy_student4():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student4.repartition(8, 'HTTP_Status_Code').groupBy('HTTP_Status_Code').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student4()\n",
    "\n",
    "# Clean up resources\n",
    "def cleanup():\n",
    "    try:\n",
    "        # Unpersist cached DataFrames\n",
    "        df_student1.unpersist()\n",
    "        df_student2.unpersist()\n",
    "        df_student3.unpersist()\n",
    "        df_student4.unpersist()\n",
    "        \n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "        print(\"\\nSpark session successfully closed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "# Final Cell [Code]:\n",
    "# Convert notebook to HTML\n",
    "!jupyter nbconvert --to html CN7031_Group136_2024.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
