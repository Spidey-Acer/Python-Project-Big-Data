{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 [Markdown]:\n",
    "'''\n",
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "# Group ID: CN7031_Group136_2024\n",
    "\n",
    "1. Student 1: Navya Athoti u2793047@uel.ac.uk\n",
    "2. Student 2: Phalguna Avalagunta u2811669@uel.ac.uk\n",
    "3. Student 3: Nikhil Sai Damera u2810262@uel.ac.uk\n",
    "4. Student 4: Sai Kishore Dodda u2773584@uel.ac.uk\n",
    "\n",
    "---\n",
    "'''\n",
    "\n",
    "# Cell 2 [Markdown]:\n",
    "'''\n",
    "# Initiate and Configure Spark\n",
    "---\n",
    "'''\n",
    "\n",
    "# Cell 3 [Code]:\n",
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark\n",
    "def initialize_spark():\n",
    "    conf = SparkConf().setAppName('CN7031_Group136_2024') \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    \n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "    return sc, spark\n",
    "\n",
    "sc, spark = initialize_spark()\n",
    "\n",
    "# Cell 5 [Markdown]:\n",
    "'''\n",
    "# Load Unstructured Data\n",
    "---\n",
    "'''\n",
    "\n",
    "# Cell 6 [Code]:\n",
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "data = load_data(spark)\n",
    "\n",
    "# Cell 7 [Markdown]:\n",
    "'''\n",
    "# Task 1: Data Processing using PySpark DF [40 marks]\n",
    "---\n",
    "'''\n",
    "\n",
    "# Cell 8 [Markdown]:\n",
    "'''\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "- DF Creation with REGEX (10 marks)\n",
    "- Two advanced DF Analysis (20 marks)\n",
    "- Utilize data visualization (10 marks)\n",
    "'''\n",
    "\n",
    "# Cell 9 [Code]:\n",
    "print(\"\\nStudent 1 Analysis - Web Traffic Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student1 = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)\"\n",
    "df_student1 = data.select(\n",
    "    regexp_extract('value', regex_student1, 1).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student1, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student1, 3).alias('HTTP_Method')\n",
    ").cache()  # Cache for performance\n",
    "\n",
    "# Validate extracted data\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total Records: {df_student1.count()}\")\n",
    "print(f\"Null Values: {df_student1.filter(col('IP_Address') == '').count()}\")\n",
    "\n",
    "# Advanced Analysis 1: Rolling Window Analysis (10 marks)\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withWatermark('timestamp', '1 hour') \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour'),\n",
    "        'IP_Address'\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window.start')\n",
    "\n",
    "print(\"\\nHourly Traffic Pattern Sample:\")\n",
    "windowed_traffic.show(5)\n",
    "\n",
    "# Advanced Analysis 2: HTTP Method Distribution (10 marks)\n",
    "method_distribution = df_student1 \\\n",
    "    .groupBy('HTTP_Method') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_requests'),\n",
    "        countDistinct('IP_Address').alias('unique_ips')\n",
    "    ).orderBy(col('total_requests').desc())\n",
    "\n",
    "print(\"\\nHTTP Method Distribution:\")\n",
    "method_distribution.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# For Student 1's visualization\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x.start)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create time series plot with proper column names\n",
    "    sns.lineplot(data=df_pandas, \n",
    "                x='time', \n",
    "                y='request_count',\n",
    "                marker='o')\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Request Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student1_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "# Modify the windowed traffic query\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window')\n",
    "\n",
    "# Create visualization\n",
    "create_traffic_visualization(windowed_traffic)\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Analysis - Response Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student2 = r\"\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]\"\n",
    "df_student2 = data.select(\n",
    "    regexp_extract('value', regex_student2, 1).alias('Status_Code'),\n",
    "    regexp_extract('value', regex_student2, 2).alias('Response_Size'),\n",
    "    regexp_extract('value', regex_student2, 3).alias('Timestamp')\n",
    ").cache()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Analysis - URL Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student3 = r\"\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)\"\n",
    "df_student3 = data.select(\n",
    "    regexp_extract('value', regex_student3, 1).alias('URL_Path'),\n",
    "    regexp_extract('value', regex_student3, 2).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student3, 3).alias('Response_Size')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 3 DataFrame structure:\")\n",
    "df_student3.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student3.show(5)\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Analysis - Log Message Analysis\")\n",
    "print(\"=\" * 50)\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student4 = r\"\\\".*\\\" (\\d+) .*? \\[(.*?)\\] (.*)\"\n",
    "df_student4 = data.select(\n",
    "    regexp_extract('value', regex_student4, 1).alias('HTTP_Status_Code'),\n",
    "    regexp_extract('value', regex_student4, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student4, 3).alias('Log_Message')\n",
    ").cache()\n",
    "# Verify DataFrame creation\n",
    "print(\"\\nVerifying Student 4 DataFrame structure:\")\n",
    "df_student4.printSchema()\n",
    "print(\"\\nSample data:\")\n",
    "df_student4.show(5)\n",
    "\n",
    "# Advanced Analysis 1: Session Analysis (10 marks)\n",
    "session_analysis = df_student2 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('long')) \\\n",
    "    .withColumn(\n",
    "        'session_requests',\n",
    "        count('*').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)  # 30-minute window in seconds\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        'avg_response_size',\n",
    "        avg('Response_Size').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nSession Analysis Sample:\")\n",
    "session_analysis.select('timestamp', 'session_requests', 'avg_response_size').show(5)\n",
    "\n",
    "# Advanced Analysis 2: Response Size Analysis (10 marks)\n",
    "response_analysis = df_student2 \\\n",
    "    .groupBy('Status_Code') \\\n",
    "    .agg(\n",
    "        count('*').alias('request_count'),\n",
    "        avg('Response_Size').alias('avg_response_size'),\n",
    "        spark_max('Response_Size').alias('max_response_size')  # Use spark_max instead of max\n",
    "    ).orderBy('Status_Code')\n",
    "\n",
    "print(\"\\nResponse Size Analysis:\")\n",
    "response_analysis.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "def create_response_visualization(df):\n",
    "    # Convert to pandas\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert Status_Code to string for better plotting\n",
    "    df_pandas['Status_Code'] = df_pandas['Status_Code'].astype(str)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(\n",
    "        data=df_pandas,\n",
    "        x='Status_Code',\n",
    "        y='avg_response_size',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    \n",
    "    plt.title('Average Response Size by Status Code')\n",
    "    plt.xlabel('HTTP Status Code')\n",
    "    plt.ylabel('Average Response Size (bytes)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student2_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "create_response_visualization(response_analysis)\n",
    "\n",
    "\n",
    "# Cell 10 [Markdown]:\n",
    "'''\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "---\n",
    "'''\n",
    "\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 RDD Analysis - Traffic Pattern Mining\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic RDD Analysis: Parse and Extract (10 marks)\n",
    "def parse_log_entry(line):\n",
    "    import re\n",
    "    try:\n",
    "        pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\d+).*\\[(.*?)\\].*\\\"([A-Z]+)'\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            return {\n",
    "                'ip': match.group(1),\n",
    "                'timestamp': match.group(2),\n",
    "                'method': match.group(3)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "base_rdd = data.rdd.map(lambda x: x['value']) \\\n",
    "                   .map(parse_log_entry) \\\n",
    "                   .filter(lambda x: x is not None)\n",
    "\n",
    "# Advanced Analysis 1: Time-based Traffic Analysis (15 marks)\n",
    "hourly_traffic = base_rdd \\\n",
    "    .map(lambda x: (x['timestamp'][:13], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "\n",
    "print(\"\\nHourly Traffic Sample:\")\n",
    "for hour, count in hourly_traffic.take(5):\n",
    "    print(f\"{hour}: {count} requests\")\n",
    "\n",
    "# Advanced Analysis 2: IP-based Pattern Analysis (15 marks)\n",
    "ip_patterns = base_rdd \\\n",
    "    .map(lambda x: (x['ip'], x['method'])) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda methods: {\n",
    "        'total_requests': len(list(methods)),\n",
    "        'method_distribution': dict(pd.Series(list(methods)).value_counts())\n",
    "    })\n",
    "\n",
    "print(\"\\nIP Pattern Analysis Sample:\")\n",
    "for ip, stats in ip_patterns.take(3):\n",
    "    print(f\"\\nIP: {ip}\")\n",
    "    print(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print(\"Method Distribution:\", stats['method_distribution'])\n",
    "\n",
    "# Cell 11 [Markdown]:\n",
    "'''\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "---\n",
    "'''\n",
    "\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies (5 marks)\n",
    "def evaluate_partition_strategy():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline - Default partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.groupBy('IP_Address').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.repartition(8, 'IP_Address').groupBy('IP_Address').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy()\n",
    "\n",
    "# Method 2: Caching Strategy (5 marks)\n",
    "def evaluate_caching_strategy():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student1.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Method').count().count()\n",
    "    df_uncached.groupBy('IP_Address').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student1.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Method').count().count()\n",
    "    df_cached.groupBy('IP_Address').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy()\n",
    "\n",
    "# Continue with Students 2-4 Task 3 implementations\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student2():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student2.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('Status_Code').count().count()\n",
    "    df_uncached.groupBy('Response_Size').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student2.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('Status_Code').count().count()\n",
    "    df_cached.groupBy('Response_Size').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student2()\n",
    "\n",
    "def evaluate_bucketing_strategy_student2():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student2.select(\n",
    "            col(\"Status_Code\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\"),\n",
    "            col(\"Timestamp\").cast(\"string\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM logs GROUP BY Status_Code\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"Status_Code\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM bucketed_logs GROUP BY Status_Code\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies\n",
    "def evaluate_partition_strategy_student3():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student3.groupBy('URL_Path').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student3.repartition(10, 'URL_Path').groupBy('URL_Path').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student3()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student3():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student3.select(\n",
    "            col(\"URL_Path\").cast(\"string\"),\n",
    "            col(\"IP_Address\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"url_logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM url_logs GROUP BY URL_Path\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"URL_Path\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_url_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM bucketed_url_logs GROUP BY URL_Path\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student4():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student4.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Status_Code').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student4.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Status_Code').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student4()\n",
    "\n",
    "# Method 2: Partition Strategies\n",
    "def evaluate_partition_strategy_student4():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student4.repartition(8, 'HTTP_Status_Code').groupBy('HTTP_Status_Code').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student4()\n",
    "\n",
    "# Clean up resources\n",
    "def cleanup():\n",
    "    try:\n",
    "        # Unpersist cached DataFrames\n",
    "        df_student1.unpersist()\n",
    "        df_student2.unpersist()\n",
    "        df_student3.unpersist()\n",
    "        df_student4.unpersist()\n",
    "        \n",
    "        # Stop Spark session\n",
    "        spark.stop()\n",
    "        print(\"\\nSpark session successfully closed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "# Final Cell [Code]:\n",
    "# Convert notebook to HTML\n",
    "!jupyter nbconvert --to html CN7031_Group136_2024.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
