{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n",
      "\n",
      "Student 1 Analysis - Web Traffic Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "Data Quality Check:\n",
      "Total Records: 3000000\n",
      "Null Values: 0\n",
      "\n",
      "Hourly Traffic Pattern Sample:\n",
      "+--------------------+---------------+-------------+\n",
      "|              window|     IP_Address|request_count|\n",
      "+--------------------+---------------+-------------+\n",
      "|{2022-01-01 00:00...|  57.54.200.233|            1|\n",
      "|{2022-01-01 00:00...|228.230.206.189|            1|\n",
      "|{2022-01-01 00:00...|    95.4.86.192|            1|\n",
      "|{2022-01-01 00:00...|  78.28.179.123|            1|\n",
      "|{2022-01-01 00:00...|  38.254.36.191|            1|\n",
      "+--------------------+---------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "HTTP Method Distribution:\n",
      "+-----------+--------------+----------+\n",
      "|HTTP_Method|total_requests|unique_ips|\n",
      "+-----------+--------------+----------+\n",
      "|        GET|       1001043|   1000932|\n",
      "|       POST|       1000505|   1000390|\n",
      "|        PUT|        998452|    998328|\n",
      "+-----------+--------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 2 Analysis - Response Analysis\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.RANGE_FRAME_INVALID_TYPE] Cannot resolve \"(ORDER BY Timestamp ASC NULLS FIRST RANGE BETWEEN -1800 FOLLOWING AND CURRENT ROW)\" due to data type mismatch: The data type \"TIMESTAMP\" used in the order specification does not match the data type \"BIGINT\" which is used in the range frame.;\n'Project [Status_Code#502, Response_Size#503, timestamp#523, count(1) windowspecdefinition(Timestamp#523 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1800, currentrow$())) AS session_requests#528]\n+- Project [Status_Code#502, Response_Size#503, cast(unix_timestamp(Timestamp#504, dd/MMM/yyyy:HH:mm:ss, Some(Africa/Nairobi), false) as timestamp) AS timestamp#523]\n   +- Project [regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 1) AS Status_Code#502, regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 2) AS Response_Size#503, regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 3) AS Timestamp#504]\n      +- Relation [value#0] text\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 152\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Advanced Analysis 1: Session Analysis (10 marks)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m session_window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mrangeBetween(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1800\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 30-minute sessions\u001b[39;00m\n\u001b[0;32m    150\u001b[0m session_analysis \u001b[38;5;241m=\u001b[39m df_student2 \\\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, unix_timestamp(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd/MMM/yyyy:HH:mm:ss\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m)) \\\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_requests\u001b[39m\u001b[38;5;124m'\u001b[39m, count(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mover(session_window)) \\\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_response_size\u001b[39m\u001b[38;5;124m'\u001b[39m, avg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse_Size\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mover(session_window))\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSession Analysis Sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m session_analysis\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_requests\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_response_size\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5175\u001b[0m     )\n\u001b[1;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.RANGE_FRAME_INVALID_TYPE] Cannot resolve \"(ORDER BY Timestamp ASC NULLS FIRST RANGE BETWEEN -1800 FOLLOWING AND CURRENT ROW)\" due to data type mismatch: The data type \"TIMESTAMP\" used in the order specification does not match the data type \"BIGINT\" which is used in the range frame.;\n'Project [Status_Code#502, Response_Size#503, timestamp#523, count(1) windowspecdefinition(Timestamp#523 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1800, currentrow$())) AS session_requests#528]\n+- Project [Status_Code#502, Response_Size#503, cast(unix_timestamp(Timestamp#504, dd/MMM/yyyy:HH:mm:ss, Some(Africa/Nairobi), false) as timestamp) AS timestamp#523]\n   +- Project [regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 1) AS Status_Code#502, regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 2) AS Response_Size#503, regexp_extract(value#0, \\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\], 3) AS Timestamp#504]\n      +- Relation [value#0] text\n"
     ]
    }
   ],
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "# Group ID: CN7031_Group136_2024\n",
    "\n",
    "# Student Information:\n",
    "# 1. Student 1: Navya Athoti u2793047@uel.ac.uk\n",
    "# 2. Student 2: Phalguna Avalagunta u2811669@uel.ac.uk\n",
    "# 3. Student 3: Nikhil Sai Damera u2810262@uel.ac.uk\n",
    "# 4. Student 4: Sai Kishore Dodda u2773584@uel.ac.uk\n",
    "\n",
    "# Import required libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_extract, col, window, count, avg, sum, \n",
    "    unix_timestamp, hour, date_format, countDistinct\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session with optimized configuration\n",
    "def initialize_spark():\n",
    "    conf = SparkConf().setAppName('CN7031_Group136_2024') \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    \n",
    "    sc = SparkContext(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "    return sc, spark\n",
    "\n",
    "sc, spark = initialize_spark()\n",
    "\n",
    "# Load and validate the dataset\n",
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "data = load_data(spark)\n",
    "\n",
    "# Task 1: Data Processing using PySpark DF [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Analysis - Web Traffic Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student1 = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)\"\n",
    "df_student1 = data.select(\n",
    "    regexp_extract('value', regex_student1, 1).alias('IP_Address'),\n",
    "    regexp_extract('value', regex_student1, 2).alias('Timestamp'),\n",
    "    regexp_extract('value', regex_student1, 3).alias('HTTP_Method')\n",
    ").cache()  # Cache for performance\n",
    "\n",
    "# Validate extracted data\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(f\"Total Records: {df_student1.count()}\")\n",
    "print(f\"Null Values: {df_student1.filter(col('IP_Address') == '').count()}\")\n",
    "\n",
    "# Advanced Analysis 1: Rolling Window Analysis (10 marks)\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withWatermark('timestamp', '1 hour') \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour'),\n",
    "        'IP_Address'\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window.start')\n",
    "\n",
    "print(\"\\nHourly Traffic Pattern Sample:\")\n",
    "windowed_traffic.show(5)\n",
    "\n",
    "# Advanced Analysis 2: HTTP Method Distribution (10 marks)\n",
    "method_distribution = df_student1 \\\n",
    "    .groupBy('HTTP_Method') \\\n",
    "    .agg(\n",
    "        count('*').alias('total_requests'),\n",
    "        countDistinct('IP_Address').alias('unique_ips')\n",
    "    ).orderBy(col('total_requests').desc())\n",
    "\n",
    "print(\"\\nHTTP Method Distribution:\")\n",
    "method_distribution.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "# For Student 1's visualization\n",
    "def create_traffic_visualization(df):\n",
    "    # Convert to pandas and prepare data\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert window struct to datetime\n",
    "    df_pandas['time'] = df_pandas['window'].apply(lambda x: x.start)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create time series plot with proper column names\n",
    "    sns.lineplot(data=df_pandas, \n",
    "                x='time', \n",
    "                y='request_count',\n",
    "                marker='o')\n",
    "    \n",
    "    plt.title('Hourly Web Traffic Pattern')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Request Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student1_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "# Modify the windowed traffic query\n",
    "windowed_traffic = df_student1 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .groupBy(\n",
    "        window('timestamp', '1 hour')\n",
    "    ).agg(\n",
    "        count('*').alias('request_count')\n",
    "    ).orderBy('window')\n",
    "\n",
    "# Create visualization\n",
    "create_traffic_visualization(windowed_traffic)\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Analysis - Response Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DF Creation with REGEX (10 marks)\n",
    "regex_student2 = r\"\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]\"\n",
    "df_student2 = data.select(\n",
    "    regexp_extract('value', regex_student2, 1).alias('Status_Code'),\n",
    "    regexp_extract('value', regex_student2, 2).alias('Response_Size'),\n",
    "    regexp_extract('value', regex_student2, 3).alias('Timestamp')\n",
    ").cache()\n",
    "\n",
    "# Advanced Analysis 1: Session Analysis (10 marks)\n",
    "session_analysis = df_student2 \\\n",
    "    .withColumn('timestamp', unix_timestamp('Timestamp', 'dd/MMM/yyyy:HH:mm:ss').cast('timestamp')) \\\n",
    "    .withColumn(\n",
    "        'session_requests',\n",
    "        count('*').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)  # 30-minute window in seconds\n",
    "        )\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        'avg_response_size',\n",
    "        avg('Response_Size').over(\n",
    "            Window.orderBy('timestamp')\n",
    "            .rangeBetween(-1800, 0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"\\nSession Analysis Sample:\")\n",
    "session_analysis.select('timestamp', 'session_requests', 'avg_response_size').show(5)\n",
    "\n",
    "# Advanced Analysis 2: Response Size Analysis (10 marks)\n",
    "response_analysis = df_student2 \\\n",
    "    .groupBy('Status_Code') \\\n",
    "    .agg(\n",
    "        count('*').alias('request_count'),\n",
    "        avg('Response_Size').alias('avg_response_size'),\n",
    "        max('Response_Size').alias('max_response_size')\n",
    "    ).orderBy('Status_Code')\n",
    "\n",
    "print(\"\\nResponse Size Analysis:\")\n",
    "response_analysis.show()\n",
    "\n",
    "# Visualization (10 marks)\n",
    "def create_response_visualization(df):\n",
    "    # Convert to pandas\n",
    "    df_pandas = df.toPandas()\n",
    "    \n",
    "    # Convert Status_Code to string for better plotting\n",
    "    df_pandas['Status_Code'] = df_pandas['Status_Code'].astype(str)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create bar plot\n",
    "    sns.barplot(\n",
    "        data=df_pandas,\n",
    "        x='Status_Code',\n",
    "        y='avg_response_size',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    \n",
    "    plt.title('Average Response Size by Status Code')\n",
    "    plt.xlabel('HTTP Status Code')\n",
    "    plt.ylabel('Average Response Size (bytes)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    plt.savefig('student2_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "create_response_visualization(response_analysis)\n",
    "\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 RDD Analysis - Traffic Pattern Mining\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic RDD Analysis: Parse and Extract (10 marks)\n",
    "def parse_log_entry(line):\n",
    "    import re\n",
    "    try:\n",
    "        pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\d+).*\\[(.*?)\\].*\\\"([A-Z]+)'\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            return {\n",
    "                'ip': match.group(1),\n",
    "                'timestamp': match.group(2),\n",
    "                'method': match.group(3)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "base_rdd = data.rdd.map(lambda x: x['value']) \\\n",
    "                   .map(parse_log_entry) \\\n",
    "                   .filter(lambda x: x is not None)\n",
    "\n",
    "# Advanced Analysis 1: Time-based Traffic Analysis (15 marks)\n",
    "hourly_traffic = base_rdd \\\n",
    "    .map(lambda x: (x['timestamp'][:13], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "\n",
    "print(\"\\nHourly Traffic Sample:\")\n",
    "for hour, count in hourly_traffic.take(5):\n",
    "    print(f\"{hour}: {count} requests\")\n",
    "\n",
    "# Advanced Analysis 2: IP-based Pattern Analysis (15 marks)\n",
    "ip_patterns = base_rdd \\\n",
    "    .map(lambda x: (x['ip'], x['method'])) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda methods: {\n",
    "        'total_requests': len(list(methods)),\n",
    "        'method_distribution': dict(pd.Series(list(methods)).value_counts())\n",
    "    })\n",
    "\n",
    "print(\"\\nIP Pattern Analysis Sample:\")\n",
    "for ip, stats in ip_patterns.take(3):\n",
    "    print(f\"\\nIP: {ip}\")\n",
    "    print(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print(\"Method Distribution:\", stats['method_distribution'])\n",
    "\n",
    "\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies (5 marks)\n",
    "def evaluate_partition_strategy():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline - Default partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.groupBy('IP_Address').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.repartition(8, 'IP_Address').groupBy('IP_Address').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy()\n",
    "\n",
    "# Method 2: Caching Strategy (5 marks)\n",
    "def evaluate_caching_strategy():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student1.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Method').count().count()\n",
    "    df_uncached.groupBy('IP_Address').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student1.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Method').count().count()\n",
    "    df_cached.groupBy('IP_Address').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy()\n",
    "\n",
    "# Continue with Students 2-4 Task 3 implementations\n",
    "\n",
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student2():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student2.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('Status_Code').count().count()\n",
    "    df_uncached.groupBy('Response_Size').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student2.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('Status_Code').count().count()\n",
    "    df_cached.groupBy('Response_Size').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student2()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student2():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    # Create temporary view\n",
    "    df_student2.createOrReplaceTempView(\"logs\")\n",
    "    \n",
    "    # Create bucketed table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bucketed_logs\n",
    "    USING parquet\n",
    "    CLUSTERED BY (Status_Code) INTO 4 BUCKETS\n",
    "    AS SELECT * FROM logs\n",
    "    \"\"\")\n",
    "    \n",
    "    # Measure query performance\n",
    "    start_time = time.time()\n",
    "    spark.sql(\"SELECT Status_Code, COUNT(*) FROM bucketed_logs GROUP BY Status_Code\").show()\n",
    "    bucketed_time = time.time() - start_time\n",
    "    print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "\n",
    "evaluate_bucketing_strategy_student2()\n",
    "\n",
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies\n",
    "def evaluate_partition_strategy_student3():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student3.groupBy('URL_Path').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student3.repartition(10, 'URL_Path').groupBy('URL_Path').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student3()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student3():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    df_student3.createOrReplaceTempView(\"url_logs\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bucketed_url_logs\n",
    "    USING parquet\n",
    "    CLUSTERED BY (URL_Path) INTO 4 BUCKETS\n",
    "    AS SELECT * FROM url_logs\n",
    "    \"\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    spark.sql(\"SELECT URL_Path, COUNT(*) FROM bucketed_url_logs GROUP BY URL_Path\").show()\n",
    "    bucketed_time = time.time() - start_time\n",
    "    print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "\n",
    "evaluate_bucketing_strategy_student3()\n",
    "\n",
    "# Student 4 (Sai Kishore Dodda u2773584)\n",
    "print(\"\\nStudent 4 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student4():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student4.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Status_Code').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student4.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Status_Code').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student4()\n",
    "\n",
    "# Method 2: Partition Strategies\n",
    "def evaluate_partition_strategy_student4():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student4.repartition(8, 'HTTP_Status_Code').groupBy('HTTP_Status_Code').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student4()\n",
    "\n",
    "# Clean up resources\n",
    "def cleanup():\n",
    "    try:\n",
    "        # Drop temporary tables\n",
    "        spark.sql(\"DROP TABLE IF EXISTS bucketed_logs\")\n",
    "        spark.sql(\"DROP TABLE IF EXISTS bucketed_url_logs\")\n",
    "        \n",
    "        # Stop Spark session\n",
    "        sc.stop()\n",
    "        print(\"\\nSpark session successfully closed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
