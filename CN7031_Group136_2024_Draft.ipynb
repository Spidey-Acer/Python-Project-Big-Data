{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7LkGfi9QpjU"
   },
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "\n",
    "## Group ID: CN7031_Group136_2024\n",
    "\n",
    "### Group Members:\n",
    "1. **Navya Athoti**  \n",
    "    Email: u2793047@uel.ac.uk\n",
    "2. **Phalguna Avalagunta**  \n",
    "    Email: u2811669@uel.ac.uk\n",
    "3. **Nikhil Sai Damera**  \n",
    "    Email: u2810262@uel.ac.uk\n",
    "4. **Sai Kishore Dodda**  \n",
    "    Email: u2773584@uel.ac.uk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and Configure Spark\n",
    "\n",
    "In this section, we will initiate and configure Apache Spark, which is a powerful open-source processing engine for big data. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FdzItAjQzNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "import sys\n",
    "\n",
    "# environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = (SparkSession.builder\n",
    "            .appName('CN7031_Group136_2024')\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFY_ihnakGwV"
   },
   "source": [
    "# Load Unstructured Data\n",
    "\n",
    "In this section, we will load and process unstructured data. Unstructured data refers to information that does not have a predefined data model or is not organized in a predefined manner. This type of data is typically text-heavy, but may also contain data such as dates, numbers, and facts.\n",
    "\n",
    "We will explore various techniques to handle and analyze unstructured data, including tokenization, vectorization, and the use of embeddings to capture semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "e0WKNOD1kh-K",
    "outputId": "9f864819-0942-4542-d184-53133f43c816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n"
     ]
    }
   ],
   "source": [
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the data loading\n",
    "try:\n",
    "    data = load_data(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing using PySpark DataFrame [40 marks]\n",
    "\n",
    "---\n",
    "\n",
    "## Complete code for all students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 DataFrame Schema:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Student 1 Sample Data:\n",
      "+--------------+-------------------+-----------+\n",
      "|ip_address    |timestamp          |http_method|\n",
      "+--------------+-------------------+-----------+\n",
      "|88.211.105.115|2022-03-04 14:17:48|POST       |\n",
      "|144.6.49.142  |2022-09-02 15:16:00|POST       |\n",
      "|231.70.64.145 |2022-07-19 01:31:31|PUT        |\n",
      "|219.42.234.172|2022-02-08 11:34:57|POST       |\n",
      "|183.173.185.94|2023-08-29 03:07:11|GET        |\n",
      "+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 1 Validation Counts:\n",
      "+----------------+---------------+-----------------+\n",
      "|ip_address_count|timestamp_count|http_method_count|\n",
      "+----------------+---------------+-----------------+\n",
      "|         3000000|        3000000|          3000000|\n",
      "+----------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "Student 2 DataFrame Schema:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 2 Sample Data:\n",
      "+-----------+-------------+-------------------+\n",
      "|status_code|response_size|timestamp          |\n",
      "+-----------+-------------+-------------------+\n",
      "|414        |12456        |2022-03-04 14:17:48|\n",
      "|203        |97126        |2022-09-02 15:16:00|\n",
      "|201        |33093        |2022-07-19 01:31:31|\n",
      "|415        |68827        |2022-02-08 11:34:57|\n",
      "|205        |30374        |2023-08-29 03:07:11|\n",
      "+-----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 2 Validation Counts:\n",
      "+-----------------+-------------------+---------------+\n",
      "|status_code_count|response_size_count|timestamp_count|\n",
      "+-----------------+-------------------+---------------+\n",
      "|          3000000|            3000000|        3000000|\n",
      "+-----------------+-------------------+---------------+\n",
      "\n",
      "\n",
      "Student 3 DataFrame Schema:\n",
      "root\n",
      " |-- url_path: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n",
      "\n",
      "Student 3 Sample Data:\n",
      "+---------------------------+--------------+-------------+\n",
      "|url_path                   |ip_address    |response_size|\n",
      "+---------------------------+--------------+-------------+\n",
      "|/history/missions/         |88.211.105.115|12456        |\n",
      "|/security/firewall/        |144.6.49.142  |97126        |\n",
      "|/web-development/countdown/|231.70.64.145 |33093        |\n",
      "|/networking/technology/    |219.42.234.172|68827        |\n",
      "|/security/firewall/        |183.173.185.94|30374        |\n",
      "+---------------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 Validation Counts:\n",
      "+--------------+----------------+-------------------+\n",
      "|url_path_count|ip_address_count|response_size_count|\n",
      "+--------------+----------------+-------------------+\n",
      "|       3000000|         3000000|            3000000|\n",
      "+--------------+----------------+-------------------+\n",
      "\n",
      "\n",
      "Student 4 DataFrame Schema:\n",
      "root\n",
      " |-- log_message: string (nullable = true)\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 4 Sample Data:\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|log_message                             |status_code|timestamp          |\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|POST /history/missions/ HTTP/2.0        |414        |2022-03-04 14:17:48|\n",
      "|POST /security/firewall/ HTTPS/1.0      |203        |2022-09-02 15:16:00|\n",
      "|PUT /web-development/countdown/ HTTP/1.0|201        |2022-07-19 01:31:31|\n",
      "|POST /networking/technology/ HTTP/1.0   |415        |2022-02-08 11:34:57|\n",
      "|GET /security/firewall/ HTTP/2.0        |205        |2023-08-29 03:07:11|\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Validation Counts:\n",
      "+-----------------+-----------------+---------------+\n",
      "|log_message_count|status_code_count|timestamp_count|\n",
      "+-----------------+-----------------+---------------+\n",
      "|          3000000|          3000000|        3000000|\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common imports and Spark initialization for all students\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import regexp_extract, to_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file\n",
    "logs_df = spark.read.text(\"web.log\")\n",
    "\n",
    "# Student 1 (Navya A) - IP Address, Timestamp, HTTP Method\n",
    "student1_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), r'\"(\\w+)', 1).alias(\"http_method\")\n",
    ")\n",
    "\n",
    "# Student 2 - HTTP Status Code, Response Size, Timestamp\n",
    "student2_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Student 3 - URL Path, IP Address, Response Size\n",
    "student3_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"[A-Z]+ (.*?) HTTP', 1).alias(\"url_path\"),\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\")\n",
    ")\n",
    "\n",
    "# Student 4 - Log Message, HTTP Status Code, Timestamp\n",
    "student4_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"(.*?)\"', 1).alias(\"log_message\"),\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Function to validate and show results for each student's DataFrame\n",
    "def validate_dataframe(df, student_num):\n",
    "    print(f\"\\nStudent {student_num} DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nStudent {student_num} Sample Data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Count non-null values for each column\n",
    "    print(f\"\\nStudent {student_num} Validation Counts:\")\n",
    "    df.select([\n",
    "        sum(col(c).isNotNull().cast(\"int\")).alias(f\"{c}_count\")\n",
    "        for c in df.columns\n",
    "    ]).show()\n",
    "\n",
    "# Validate each student's DataFrame\n",
    "validate_dataframe(student1_df, 1)\n",
    "validate_dataframe(student2_df, 2)\n",
    "validate_dataframe(student3_df, 3)\n",
    "validate_dataframe(student4_df, 4)\n",
    "\n",
    "# Register DataFrames as views for SQL queries later\n",
    "student1_df.createOrReplaceTempView(\"student1_logs\")\n",
    "student2_df.createOrReplaceTempView(\"student2_logs\")\n",
    "student3_df.createOrReplaceTempView(\"student3_logs\")\n",
    "student4_df.createOrReplaceTempView(\"student4_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student 1: Navya Athoti (u2793047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Schema:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample Data:\n",
      "+--------------+-------------------+-----------+\n",
      "|ip_address    |timestamp          |http_method|\n",
      "+--------------+-------------------+-----------+\n",
      "|88.211.105.115|2022-03-04 14:17:48|POST       |\n",
      "|144.6.49.142  |2022-09-02 15:16:00|POST       |\n",
      "|231.70.64.145 |2022-07-19 01:31:31|PUT        |\n",
      "|219.42.234.172|2022-02-08 11:34:57|POST       |\n",
      "|183.173.185.94|2023-08-29 03:07:11|GET        |\n",
      "+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Validation Counts:\n",
      "+--------------+---------------------+------------------+\n",
      "|valid_ip_count|valid_timestamp_count|valid_method_count|\n",
      "+--------------+---------------------+------------------+\n",
      "|       3000000|              3000000|           3000000|\n",
      "+--------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import regexp_extract, to_timestamp, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analysis - Navya A\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file\n",
    "# Assuming your log file is uploaded to Colab\n",
    "logs_df = spark.read.text(\"web.log\")\n",
    "\n",
    "# Define the regex patterns for extraction\n",
    "# Based on your log format:\n",
    "# IP pattern: (\\d+\\.\\d+\\.\\d+\\.\\d+)\n",
    "# Timestamp pattern: \\[(.*?)\\]\n",
    "# HTTP Method pattern: \"([A-Z]+)\n",
    "parsed_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), r'\"(\\w+)', 1).alias(\"http_method\")\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a temporary view for SQL queries later\n",
    "parsed_df.createOrReplaceTempView(\"log_data\")\n",
    "\n",
    "# Show the schema and sample data\n",
    "print(\"DataFrame Schema:\")\n",
    "parsed_df.printSchema()\n",
    "\n",
    "print(\"\\nSample Data:\")\n",
    "parsed_df.show(5, truncate=False)\n",
    "\n",
    "# Add some basic validations\n",
    "print(\"\\nValidation Counts:\")\n",
    "parsed_df.select(\n",
    "    col(\"ip_address\").isNotNull().cast(\"int\").alias(\"valid_ip\"),\n",
    "    col(\"timestamp\").isNotNull().cast(\"int\").alias(\"valid_timestamp\"),\n",
    "    col(\"http_method\").isNotNull().cast(\"int\").alias(\"valid_method\")\n",
    ").agg(\n",
    "    sum(\"valid_ip\").alias(\"valid_ip_count\"),\n",
    "    sum(\"valid_timestamp\").alias(\"valid_timestamp_count\"),\n",
    "    sum(\"valid_method\").alias(\"valid_method_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DataFrame Creation with REGEX (10 marks)\n",
    "- Description of the task and methodology used for creating the DataFrame using REGEX.\n",
    "- Code snippets and explanations.\n",
    "- Example outputs and results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Two Advanced DataFrame Analysis (20 marks)\n",
    "- Detailed description of the two advanced analyses performed on the DataFrame.\n",
    "- Code snippets and explanations for each analysis.\n",
    "- Visualizations and interpretations of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Visualization (10 marks)\n",
    "- Explanation of the data visualization techniques used.\n",
    "- Code snippets for generating the visualizations.\n",
    "- Example visualizations and their interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F4fH4d-LJhH"
   },
   "source": [
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "---\n",
    "\n",
    "## Student 2: Phalguna Avalagunta (u2811669)\n",
    "\n",
    "### 1. RDD Creation and Transformation (10 marks)\n",
    "- Description of the task and methodology used for creating and transforming the RDD.\n",
    "- Code snippets and explanations.\n",
    "- Example outputs and results.\n",
    "\n",
    "### 2. Two Advanced RDD Analysis (20 marks)\n",
    "- Detailed description of the two advanced analyses performed on the RDD.\n",
    "- Code snippets and explanations for each analysis.\n",
    "- Visualizations and interpretations of the results.\n",
    "\n",
    "### 3. Data Visualization (10 marks)\n",
    "- Explanation of the data visualization techniques used.\n",
    "- Code snippets for generating the visualizations.\n",
    "- Example visualizations and their interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "NyQQvVRrmhlQ",
    "outputId": "8b3fac55-bd76-43ab-dafe-1b0358025729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 RDD Analysis - Traffic Pattern Mining\n",
      "==================================================\n",
      "\n",
      "Hourly Traffic Sample:\n",
      "01/Apr/2022:0: 1869 requests\n",
      "01/Apr/2022:1: 1839 requests\n",
      "01/Apr/2022:2: 729 requests\n",
      "01/Apr/2023:0: 1889 requests\n",
      "01/Apr/2023:1: 1879 requests\n",
      "\n",
      "IP Pattern Analysis Sample:\n",
      "\n",
      "IP: 220.182.78.75\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n",
      "\n",
      "IP: 143.238.50.180\n",
      "Total Requests: 1\n",
      "Method Distribution: {'POST': 1}\n",
      "\n",
      "IP: 155.22.118.135\n",
      "Total Requests: 1\n",
      "Method Distribution: {'GET': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Task 2: Data Processing using PySpark RDD [40 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 RDD Analysis - Traffic Pattern Mining\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic RDD Analysis: Parse and Extract (10 marks)\n",
    "def parse_log_entry(line):\n",
    "    import re\n",
    "    try:\n",
    "        pattern = r'(\\d+\\.\\d+\\.\\d+\\.\\d+).*\\[(.*?)\\].*\\\"([A-Z]+)'\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            return {\n",
    "                'ip': match.group(1),\n",
    "                'timestamp': match.group(2),\n",
    "                'method': match.group(3)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {str(e)}\")\n",
    "    return None\n",
    "\n",
    "base_rdd = data.rdd.map(lambda x: x['value']) \\\n",
    "                   .map(parse_log_entry) \\\n",
    "                   .filter(lambda x: x is not None)\n",
    "\n",
    "# Advanced Analysis 1: Time-based Traffic Analysis (15 marks)\n",
    "hourly_traffic = base_rdd \\\n",
    "    .map(lambda x: (x['timestamp'][:13], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortByKey()\n",
    "\n",
    "print(\"\\nHourly Traffic Sample:\")\n",
    "for hour, count in hourly_traffic.take(5):\n",
    "    print(f\"{hour}: {count} requests\")\n",
    "\n",
    "# Advanced Analysis 2: IP-based Pattern Analysis (15 marks)\n",
    "ip_patterns = base_rdd \\\n",
    "    .map(lambda x: (x['ip'], x['method'])) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda methods: {\n",
    "        'total_requests': len(list(methods)),\n",
    "        'method_distribution': dict(pd.Series(list(methods)).value_counts())\n",
    "    })\n",
    "\n",
    "print(\"\\nIP Pattern Analysis Sample:\")\n",
    "for ip, stats in ip_patterns.take(3):\n",
    "    print(f\"\\nIP: {ip}\")\n",
    "    print(f\"Total Requests: {stats['total_requests']}\")\n",
    "    print(\"Method Distribution:\", stats['method_distribution'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOrBapHUot5U"
   },
   "source": [
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "---\n",
    "\n",
    "In this task, we will focus on optimization techniques and considerations for LSEPI (Large-Scale Enterprise Process Integration). The goal is to enhance the performance and efficiency of our data processing workflows.\n",
    "\n",
    "## Objectives:\n",
    "1. **Optimization Techniques (5 marks)**\n",
    "    - Description of the optimization techniques applied.\n",
    "    - Code snippets and explanations.\n",
    "    - Example outputs and results.\n",
    "\n",
    "2. **LSEPI Considerations (5 marks)**\n",
    "    - Detailed description of the considerations for LSEPI.\n",
    "    - Code snippets and explanations for each consideration.\n",
    "    - Visualizations and interpretations of the results.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Optimization Techniques (5 marks)\n",
    "- **Description:**\n",
    "  - Provide a detailed description of the optimization techniques used.\n",
    "  - Explain the methodology and rationale behind each technique.\n",
    "- **Code Snippets:**\n",
    "  - Include relevant code snippets demonstrating the optimization techniques.\n",
    "- **Example Outputs:**\n",
    "  - Show example outputs and results to illustrate the effectiveness of the optimizations.\n",
    "\n",
    "### 2. LSEPI Considerations (5 marks)\n",
    "- **Description:**\n",
    "  - Discuss the key considerations for LSEPI.\n",
    "  - Explain how these considerations impact the overall workflow.\n",
    "- **Code Snippets:**\n",
    "  - Provide code snippets that address LSEPI considerations.\n",
    "- **Visualizations:**\n",
    "  - Include visualizations to support the explanations and interpretations of the results.\n",
    "\n",
    "---\n",
    "\n",
    "This section aims to provide a comprehensive understanding of optimization and LSEPI considerations, ensuring efficient and effective data processing workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 4.44 seconds\n",
      "Optimized execution time: 4.08 seconds\n",
      "Performance improvement: 8.01%\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 11.75 seconds\n",
      "Cached execution time: 4.21 seconds\n",
      "Caching improvement: 64.15%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Task 3: Optimization and LSEPI Considerations [10 marks]\n",
    "\n",
    "# Student 1 (Navya Athoti u2793047)\n",
    "print(\"\\nStudent 1 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies (5 marks)\n",
    "def evaluate_partition_strategy():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline - Default partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.groupBy('IP_Address').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student1.repartition(8, 'IP_Address').groupBy('IP_Address').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy()\n",
    "\n",
    "# Method 2: Caching Strategy (5 marks)\n",
    "def evaluate_caching_strategy():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student1.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('HTTP_Method').count().count()\n",
    "    df_uncached.groupBy('IP_Address').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student1.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('HTTP_Method').count().count()\n",
    "    df_cached.groupBy('IP_Address').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 2 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 14.07 seconds\n",
      "Cached execution time: 0.63 seconds\n",
      "Caching improvement: 95.52%\n"
     ]
    }
   ],
   "source": [
    "# Student 2 (Phalguna Avalagunta u2811669)\n",
    "print(\"\\nStudent 2 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Caching Strategy\n",
    "def evaluate_caching_strategy_student2():\n",
    "    print(\"\\nCaching Strategy Evaluation\")\n",
    "    \n",
    "    # Without caching\n",
    "    df_uncached = df_student2.unpersist()\n",
    "    start_time = time.time()\n",
    "    df_uncached.groupBy('Status_Code').count().count()\n",
    "    df_uncached.groupBy('Response_Size').count().count()\n",
    "    uncached_time = time.time() - start_time\n",
    "    print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "    \n",
    "    # With caching\n",
    "    df_cached = df_student2.cache()\n",
    "    df_cached.count()  # Materialize cache\n",
    "    start_time = time.time()\n",
    "    df_cached.groupBy('Status_Code').count().count()\n",
    "    df_cached.groupBy('Response_Size').count().count()\n",
    "    cached_time = time.time() - start_time\n",
    "    print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "    print(f\"Caching improvement: {((uncached_time - cached_time) / uncached_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_caching_strategy_student2()\n",
    "\n",
    "def evaluate_bucketing_strategy_student2():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student2.select(\n",
    "            col(\"Status_Code\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\"),\n",
    "            col(\"Timestamp\").cast(\"string\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM logs GROUP BY Status_Code\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"Status_Code\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT Status_Code, COUNT(*) FROM bucketed_logs GROUP BY Status_Code\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 3 Optimization Analysis\n",
      "==================================================\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 0.42 seconds\n",
      "Optimized execution time: 1.55 seconds\n",
      "Performance improvement: -264.13%\n"
     ]
    }
   ],
   "source": [
    "# Student 3 (Nikhil Sai Damera u2810262)\n",
    "print(\"\\nStudent 3 Optimization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Partition Strategies\n",
    "def evaluate_partition_strategy_student3():\n",
    "    print(\"\\nPartitioning Strategy Evaluation\")\n",
    "    \n",
    "    # Baseline\n",
    "    start_time = time.time()\n",
    "    df_student3.groupBy('URL_Path').count().count()\n",
    "    baseline_time = time.time() - start_time\n",
    "    print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "    \n",
    "    # Custom partitioning\n",
    "    start_time = time.time()\n",
    "    df_student3.repartition(10, 'URL_Path').groupBy('URL_Path').count().count()\n",
    "    optimized_time = time.time() - start_time\n",
    "    print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "    print(f\"Performance improvement: {((baseline_time - optimized_time) / baseline_time) * 100:.2f}%\")\n",
    "\n",
    "evaluate_partition_strategy_student3()\n",
    "\n",
    "# Method 2: Bucketing & Indexing\n",
    "def evaluate_bucketing_strategy_student3():\n",
    "    print(\"\\nBucketing Strategy Evaluation\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with proper schema\n",
    "        df_for_bucket = df_student3.select(\n",
    "            col(\"URL_Path\").cast(\"string\"),\n",
    "            col(\"IP_Address\").cast(\"string\"),\n",
    "            col(\"Response_Size\").cast(\"long\")\n",
    "        )\n",
    "        \n",
    "        # Create temporary view\n",
    "        df_for_bucket.createOrReplaceTempView(\"url_logs\")\n",
    "        \n",
    "        # Measure query performance without bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM url_logs GROUP BY URL_Path\").show()\n",
    "        unbucketed_time = time.time() - start_time\n",
    "        print(f\"Query time without bucketing: {unbucketed_time:.2f} seconds\")\n",
    "        \n",
    "        # Create bucketed DataFrame directly\n",
    "        bucketed_df = df_for_bucket.repartition(4, \"URL_Path\")\n",
    "        bucketed_df.createOrReplaceTempView(\"bucketed_url_logs\")\n",
    "        \n",
    "        # Measure query performance with bucketing\n",
    "        start_time = time.time()\n",
    "        spark.sql(\"SELECT URL_Path, COUNT(*) FROM bucketed_url_logs GROUP BY URL_Path\").show()\n",
    "        bucketed_time = time.time() - start_time\n",
    "        print(f\"Query time with bucketing: {bucketed_time:.2f} seconds\")\n",
    "        print(f\"Performance improvement: {((unbucketed_time - bucketed_time) / unbucketed_time) * 100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in bucketing strategy: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing log file: c:\\Users\\HP\\University\\Python-Projects\\web.log\n",
      "\n",
      "Sample of processed data:\n",
      "+----------------+---------+-----------+\n",
      "|HTTP_Status_Code|Timestamp|Log_Message|\n",
      "+----------------+---------+-----------+\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "|                |         |           |\n",
      "+----------------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Caching Strategy Evaluation\n",
      "Uncached execution time: 8.73 seconds\n",
      "Cached execution time: 0.23 seconds\n",
      "Caching improvement: 97.31%\n",
      "\n",
      "Partitioning Strategy Evaluation\n",
      "Baseline execution time: 0.22 seconds\n",
      "Optimized execution time: 1.06 seconds\n",
      "Performance improvement: -373.47%\n",
      "\n",
      "Spark session successfully closed\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import os\n",
    "\n",
    "class Student4Analysis:\n",
    "    def __init__(self):\n",
    "        self.spark = None\n",
    "        self.df_student4 = None\n",
    "    \n",
    "    def initialize_spark(self):\n",
    "        \"\"\"Initialize Spark session\"\"\"\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Student4_Analysis\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.executor.memory\", \"2g\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    def load_data(self, input_data):\n",
    "        \"\"\"Load and process the log data\"\"\"\n",
    "        try:\n",
    "            # Read the raw data\n",
    "            raw_logs = self.spark.read.text(input_data)\n",
    "            \n",
    "            # Define the regex pattern for Student 4\n",
    "            regex_pattern = r'\\\".*\\\" (\\d+) .*? \\[(.*?)\\] (.*)'\n",
    "            \n",
    "            # Create DataFrame with extracted fields\n",
    "            self.df_student4 = raw_logs.select(\n",
    "                regexp_extract('value', regex_pattern, 1).alias('HTTP_Status_Code'),\n",
    "                regexp_extract('value', regex_pattern, 2).alias('Timestamp'),\n",
    "                regexp_extract('value', regex_pattern, 3).alias('Log_Message')\n",
    "            )\n",
    "            \n",
    "            print(\"\\nSample of processed data:\")\n",
    "            self.df_student4.show(5, truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_caching_strategy(self):\n",
    "        \"\"\"Evaluate caching strategy performance\"\"\"\n",
    "        try:\n",
    "            print(\"\\nCaching Strategy Evaluation\")\n",
    "            \n",
    "            # Ensure DataFrame exists\n",
    "            if self.df_student4 is None:\n",
    "                raise ValueError(\"DataFrame not initialized\")\n",
    "            \n",
    "            # Without caching\n",
    "            self.df_student4.unpersist()  # Ensure clean state\n",
    "            start_time = time.time()\n",
    "            uncached_count = self.df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "            uncached_time = time.time() - start_time\n",
    "            print(f\"Uncached execution time: {uncached_time:.2f} seconds\")\n",
    "            \n",
    "            # With caching\n",
    "            cached_df = self.df_student4.cache()\n",
    "            cached_df.count()  # Materialize cache\n",
    "            start_time = time.time()\n",
    "            cached_count = cached_df.groupBy('HTTP_Status_Code').count().count()\n",
    "            cached_time = time.time() - start_time\n",
    "            improvement = ((uncached_time - cached_time) / uncached_time) * 100\n",
    "            print(f\"Cached execution time: {cached_time:.2f} seconds\")\n",
    "            print(f\"Caching improvement: {improvement:.2f}%\")\n",
    "            \n",
    "            return cached_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in caching strategy evaluation: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_partition_strategy(self):\n",
    "        \"\"\"Evaluate partitioning strategy performance\"\"\"\n",
    "        try:\n",
    "            print(\"\\nPartitioning Strategy Evaluation\")\n",
    "            \n",
    "            # Ensure DataFrame exists\n",
    "            if self.df_student4 is None:\n",
    "                raise ValueError(\"DataFrame not initialized\")\n",
    "            \n",
    "            # Baseline execution\n",
    "            start_time = time.time()\n",
    "            baseline_count = self.df_student4.groupBy('HTTP_Status_Code').count().count()\n",
    "            baseline_time = time.time() - start_time\n",
    "            print(f\"Baseline execution time: {baseline_time:.2f} seconds\")\n",
    "            \n",
    "            # Optimized execution with partitioning\n",
    "            start_time = time.time()\n",
    "            optimized_count = (self.df_student4.repartition(8, 'HTTP_Status_Code')\n",
    "                             .groupBy('HTTP_Status_Code')\n",
    "                             .count()\n",
    "                             .count())\n",
    "            optimized_time = time.time() - start_time\n",
    "            improvement = ((baseline_time - optimized_time) / baseline_time) * 100\n",
    "            print(f\"Optimized execution time: {optimized_time:.2f} seconds\")\n",
    "            print(f\"Performance improvement: {improvement:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in partition strategy evaluation: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up Spark resources\"\"\"\n",
    "        try:\n",
    "            if self.df_student4 is not None:\n",
    "                self.df_student4.unpersist()\n",
    "            if self.spark is not None:\n",
    "                self.spark.stop()\n",
    "                print(\"\\nSpark session successfully closed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Initialize analysis object\n",
    "    analysis = Student4Analysis()\n",
    "    \n",
    "    try:\n",
    "        # Initialize Spark\n",
    "        analysis.initialize_spark()\n",
    "        \n",
    "        # Get the current working directory\n",
    "        current_dir = os.getcwd()\n",
    "        \n",
    "        # Specify the log file path - adjust this to your actual log file path\n",
    "        log_file = os.path.join(current_dir, \"web.log\")\n",
    "        \n",
    "        print(f\"\\nProcessing log file: {log_file}\")\n",
    "        \n",
    "        # Load and process data\n",
    "        analysis.load_data(log_file)\n",
    "        \n",
    "        # Run optimization tests\n",
    "        analysis.evaluate_caching_strategy()\n",
    "        analysis.evaluate_partition_strategy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {str(e)}\")\n",
    "    finally:\n",
    "        # Ensure cleanup happens even if there's an error\n",
    "        analysis.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "playing-word-embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
