{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7LkGfi9QpjU"
   },
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "\n",
    "## Group ID: CN7031_Group136_2024\n",
    "\n",
    "### Group Members:\n",
    "1. **Navya Athoti**  \n",
    "    Email: u2793047@uel.ac.uk\n",
    "2. **Phalguna Avalagunta**  \n",
    "    Email: u2811669@uel.ac.uk\n",
    "3. **Nikhil Sai Damera**  \n",
    "    Email: u2810262@uel.ac.uk\n",
    "4. **Sai Kishore Dodda**  \n",
    "    Email: u2773584@uel.ac.uk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and Configure Spark\n",
    "\n",
    "In this section, we will initiate and configure Apache Spark, which is a powerful open-source processing engine for big data. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FdzItAjQzNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "import sys\n",
    "\n",
    "# environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = (SparkSession.builder\n",
    "            .appName('CN7031_Group136_2024')\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFY_ihnakGwV"
   },
   "source": [
    "# Load Unstructured Data\n",
    "\n",
    "In this section, we will load and process unstructured data. Unstructured data refers to information that does not have a predefined data model or is not organized in a predefined manner. This type of data is typically text-heavy, but may also contain data such as dates, numbers, and facts.\n",
    "\n",
    "We will explore various techniques to handle and analyze unstructured data, including tokenization, vectorization, and the use of embeddings to capture semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "e0WKNOD1kh-K",
    "outputId": "9f864819-0942-4542-d184-53133f43c816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n"
     ]
    }
   ],
   "source": [
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the data loading\n",
    "try:\n",
    "    data = load_data(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing using PySpark DataFrame [40 marks]\n",
    "\n",
    "---\n",
    "\n",
    "## DataFrame Creation with REGEX (10 marks)\n",
    "\n",
    "Each member will define a custom schema using REGEX to extract specific metrics from the dataset.\n",
    "\n",
    "### Student Metrics to Extract\n",
    "\n",
    "- **Student 1: IP Address, Timestamp, HTTP Method**\n",
    "    - **REGEX Example:** `(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)`\n",
    "\n",
    "- **Student 2: HTTP Status Code, Response Size, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]`\n",
    "\n",
    "- **Student 3: URL Path, IP Address, Response Size**\n",
    "    - **REGEX Example:** `\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)`\n",
    "\n",
    "- **Student 4: Log Message, HTTP Status Code, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) .* \\[(.*?)\\] (.*)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 DataFrame Schema:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Student 1 Sample Data:\n",
      "+--------------+-------------------+-----------+\n",
      "|ip_address    |timestamp          |http_method|\n",
      "+--------------+-------------------+-----------+\n",
      "|88.211.105.115|2022-03-04 14:17:48|POST       |\n",
      "|144.6.49.142  |2022-09-02 15:16:00|POST       |\n",
      "|231.70.64.145 |2022-07-19 01:31:31|PUT        |\n",
      "|219.42.234.172|2022-02-08 11:34:57|POST       |\n",
      "|183.173.185.94|2023-08-29 03:07:11|GET        |\n",
      "+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 1 Validation Counts:\n",
      "+----------------+---------------+-----------------+\n",
      "|ip_address_count|timestamp_count|http_method_count|\n",
      "+----------------+---------------+-----------------+\n",
      "|         3000000|        3000000|          3000000|\n",
      "+----------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "Student 2 DataFrame Schema:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 2 Sample Data:\n",
      "+-----------+-------------+-------------------+\n",
      "|status_code|response_size|timestamp          |\n",
      "+-----------+-------------+-------------------+\n",
      "|414        |12456        |2022-03-04 14:17:48|\n",
      "|203        |97126        |2022-09-02 15:16:00|\n",
      "|201        |33093        |2022-07-19 01:31:31|\n",
      "|415        |68827        |2022-02-08 11:34:57|\n",
      "|205        |30374        |2023-08-29 03:07:11|\n",
      "+-----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 2 Validation Counts:\n",
      "+-----------------+-------------------+---------------+\n",
      "|status_code_count|response_size_count|timestamp_count|\n",
      "+-----------------+-------------------+---------------+\n",
      "|          3000000|            3000000|        3000000|\n",
      "+-----------------+-------------------+---------------+\n",
      "\n",
      "\n",
      "Student 3 DataFrame Schema:\n",
      "root\n",
      " |-- url_path: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n",
      "\n",
      "Student 3 Sample Data:\n",
      "+---------------------------+--------------+-------------+\n",
      "|url_path                   |ip_address    |response_size|\n",
      "+---------------------------+--------------+-------------+\n",
      "|/history/missions/         |88.211.105.115|12456        |\n",
      "|/security/firewall/        |144.6.49.142  |97126        |\n",
      "|/web-development/countdown/|231.70.64.145 |33093        |\n",
      "|/networking/technology/    |219.42.234.172|68827        |\n",
      "|/security/firewall/        |183.173.185.94|30374        |\n",
      "+---------------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 Validation Counts:\n",
      "+--------------+----------------+-------------------+\n",
      "|url_path_count|ip_address_count|response_size_count|\n",
      "+--------------+----------------+-------------------+\n",
      "|       3000000|         3000000|            3000000|\n",
      "+--------------+----------------+-------------------+\n",
      "\n",
      "\n",
      "Student 4 DataFrame Schema:\n",
      "root\n",
      " |-- log_message: string (nullable = true)\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 4 Sample Data:\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|log_message                             |status_code|timestamp          |\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|POST /history/missions/ HTTP/2.0        |414        |2022-03-04 14:17:48|\n",
      "|POST /security/firewall/ HTTPS/1.0      |203        |2022-09-02 15:16:00|\n",
      "|PUT /web-development/countdown/ HTTP/1.0|201        |2022-07-19 01:31:31|\n",
      "|POST /networking/technology/ HTTP/1.0   |415        |2022-02-08 11:34:57|\n",
      "|GET /security/firewall/ HTTP/2.0        |205        |2023-08-29 03:07:11|\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Validation Counts:\n",
      "+-----------------+-----------------+---------------+\n",
      "|log_message_count|status_code_count|timestamp_count|\n",
      "+-----------------+-----------------+---------------+\n",
      "|          3000000|          3000000|        3000000|\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common imports and Spark initialization for all students\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import regexp_extract, to_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file\n",
    "logs_df = spark.read.text(\"web.log\")\n",
    "\n",
    "# Student 1 (Navya A) - IP Address, Timestamp, HTTP Method\n",
    "student1_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), r'\"(\\w+)', 1).alias(\"http_method\")\n",
    ")\n",
    "\n",
    "# Student 2 - HTTP Status Code, Response Size, Timestamp\n",
    "student2_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Student 3 - URL Path, IP Address, Response Size\n",
    "student3_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"[A-Z]+ (.*?) HTTP', 1).alias(\"url_path\"),\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\")\n",
    ")\n",
    "\n",
    "# Student 4 - Log Message, HTTP Status Code, Timestamp\n",
    "student4_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"(.*?)\"', 1).alias(\"log_message\"),\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Function to validate and show results for each student's DataFrame\n",
    "def validate_dataframe(df, student_num):\n",
    "    print(f\"\\nStudent {student_num} DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nStudent {student_num} Sample Data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Count non-null values for each column\n",
    "    print(f\"\\nStudent {student_num} Validation Counts:\")\n",
    "    df.select([\n",
    "        sum(col(c).isNotNull().cast(\"int\")).alias(f\"{c}_count\")\n",
    "        for c in df.columns\n",
    "    ]).show()\n",
    "\n",
    "# Validate each student's DataFrame\n",
    "validate_dataframe(student1_df, 1)\n",
    "validate_dataframe(student2_df, 2)\n",
    "validate_dataframe(student3_df, 3)\n",
    "validate_dataframe(student4_df, 4)\n",
    "\n",
    "# Register DataFrames as views for SQL queries later\n",
    "student1_df.createOrReplaceTempView(\"student1_logs\")\n",
    "student2_df.createOrReplaceTempView(\"student2_logs\")\n",
    "student3_df.createOrReplaceTempView(\"student3_logs\")\n",
    "student4_df.createOrReplaceTempView(\"student4_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F4fH4d-LJhH"
   },
   "source": [
    "# Task 2: Two Advanced DataFrame Analysis (20 marks)\n",
    "\n",
    "Each member will write unique SQL queries for the analysis:\n",
    "\n",
    "## SQL Query 1: Window Functions\n",
    "\n",
    "- **Student 1: Rolling hourly traffic per IP**\n",
    "    - **Description:** Calculate traffic count per IP over a sliding window.\n",
    "\n",
    "- **Student 2: Session identification**\n",
    "    - **Description:** Identify sessions based on timestamp gaps.\n",
    "\n",
    "- **Student 3: Unique visitors per hour**\n",
    "    - **Description:** Count distinct IPs for each hour.\n",
    "\n",
    "- **Student 4: Average response size per status code**\n",
    "    - **Description:** Compute averages grouped by status codes.\n",
    "\n",
    "## SQL Query 2: Aggregation Functions\n",
    "\n",
    "- **Student 1: Traffic patterns by URL path**\n",
    "    - **Description:** Analyze URL visits by hour.\n",
    "\n",
    "- **Student 2: Top 10 failed requests by size**\n",
    "    - **Description:** Identify the largest failed requests.\n",
    "\n",
    "- **Student 3: Response size distribution by status**\n",
    "    - **Description:** Show min, max, and avg sizes for each status.\n",
    "\n",
    "- **Student 4: Daily unique visitors**\n",
    "    - **Description:** Count unique IPs per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "NyQQvVRrmhlQ",
    "outputId": "8b3fac55-bd76-43ab-dafe-1b0358025729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in student1_logs:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Available columns in student2_logs:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "=== Window Functions Analysis ===\n",
      "\n",
      "Student 2 - Session Identification:\n",
      "+-------------------+-----------+-----------+\n",
      "|          timestamp|status_code|new_session|\n",
      "+-------------------+-----------+-----------+\n",
      "|2022-01-01 00:01:34|        203|          1|\n",
      "|2022-01-01 00:01:50|        414|          0|\n",
      "|2022-01-01 00:02:15|        200|          0|\n",
      "|2022-01-01 00:02:30|        203|          0|\n",
      "|2022-01-01 00:02:45|        308|          0|\n",
      "+-------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 - Hourly Visit Counts:\n",
      "+-------------------+------------+\n",
      "|               hour|total_visits|\n",
      "+-------------------+------------+\n",
      "|2022-01-01 00:00:00|         196|\n",
      "|2022-01-01 01:00:00|         203|\n",
      "|2022-01-01 02:00:00|         201|\n",
      "|2022-01-01 03:00:00|         187|\n",
      "|2022-01-01 04:00:00|         195|\n",
      "+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 - Response Size Analysis:\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|status_code|         avg_size|request_count|min_size|max_size|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|        200|50565.73821994404|       214791|    1000|  100000|\n",
      "|        201|50542.00092736712|       214586|    1000|  100000|\n",
      "|        202|50454.74877758012|       214738|    1001|  100000|\n",
      "|        203|50423.19351991519|       214133|    1000|  100000|\n",
      "|        204|50402.07495866336|       213491|    1000|  100000|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's confirm what columns we have in each DataFrame\n",
    "print(\"Available columns in student1_logs:\")\n",
    "spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "print(\"\\nAvailable columns in student2_logs:\")\n",
    "spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Now let's modify our functions to only use available columns\n",
    "\n",
    "# Student 2: Session identification (Modified)\n",
    "def analyze_sessions():\n",
    "    query = \"\"\"\n",
    "    WITH time_gaps AS (\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            LAG(timestamp) OVER (\n",
    "                ORDER BY timestamp\n",
    "            ) as prev_timestamp,\n",
    "            status_code\n",
    "        FROM student2_logs\n",
    "    )\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        status_code,\n",
    "        CASE \n",
    "            WHEN (unix_timestamp(timestamp) - unix_timestamp(prev_timestamp)) > 1800 \n",
    "            OR prev_timestamp IS NULL \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END as new_session\n",
    "    FROM time_gaps\n",
    "    ORDER BY timestamp\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 3: Unique visitors per hour (Modified)\n",
    "def analyze_unique_visitors():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        date_trunc('hour', timestamp) as hour,\n",
    "        COUNT(*) as total_visits\n",
    "    FROM student1_logs\n",
    "    GROUP BY date_trunc('hour', timestamp)\n",
    "    ORDER BY hour\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 4: Average response size per status code (Original - should work)\n",
    "def analyze_avg_response_size():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        status_code,\n",
    "        AVG(response_size) as avg_size,\n",
    "        COUNT(*) as request_count,\n",
    "        MIN(response_size) as min_size,\n",
    "        MAX(response_size) as max_size\n",
    "    FROM student2_logs\n",
    "    GROUP BY status_code\n",
    "    ORDER BY status_code\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Try executing the queries\n",
    "try:\n",
    "    print(\"\\n=== Window Functions Analysis ===\")\n",
    "    \n",
    "    print(\"\\nStudent 2 - Session Identification:\")\n",
    "    analyze_sessions().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 3 - Hourly Visit Counts:\")\n",
    "    analyze_unique_visitors().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 4 - Response Size Analysis:\")\n",
    "    analyze_avg_response_size().show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    # Print the actual data for debugging\n",
    "    print(\"\\nSample data from student2_logs:\")\n",
    "    spark.sql(\"SELECT * FROM student2_logs LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOrBapHUot5U"
   },
   "source": [
    "# Task 3: Data Visualization (10 marks)\n",
    "\n",
    "Each member will visualize the results of their unique SQL queries using different chart types.\n",
    "\n",
    "### Student Visualization Type Examples\n",
    "\n",
    "- **Student 1: Line Chart (Hourly Traffic)**\n",
    "  - **Tool:** Matplotlib for rolling traffic visualization.\n",
    "\n",
    "- **Student 2: Bar Chart (Top 10 Failed Requests)**\n",
    "  - **Tool:** Seaborn for aggregated failure counts.\n",
    "\n",
    "- **Student 3: Heatmap (Hourly Unique Visitors)**\n",
    "  - **Tool:** Seaborn for visualizing traffic density.\n",
    "\n",
    "- **Student 4: Pie Chart (Response Code Distribution)**\n",
    "  - **Tool:** Matplotlib for status code proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations for all students...\n",
      "\n",
      "Student 1 - Rolling Traffic Line Chart:\n",
      "An error occurred while generating visualizations: 'hour'\n",
      "\n",
      "Debugging information:\n",
      "Available columns in the DataFrames:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ===================== Student 1: Line Chart of Rolling Traffic =====================\n",
    "def analyze_rolling_traffic():\n",
    "    # Example implementation, replace with actual logic\n",
    "    return student1_df\n",
    "\n",
    "def visualize_rolling_traffic():\n",
    "    # Get data from our previous SQL query\n",
    "    rolling_traffic_df = analyze_rolling_traffic()\n",
    "    pdf = rolling_traffic_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create line plot\n",
    "    plt.plot(pdf['hour'], pdf['rolling_3hour_traffic'], \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.title('3-Hour Rolling Traffic by Hour', fontsize=14, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Rolling Traffic Count', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations for peaks\n",
    "    max_traffic = pdf['rolling_3hour_traffic'].max()\n",
    "    plt.axhline(y=max_traffic, color='r', linestyle='--', alpha=0.3)\n",
    "# ===================== Student 2: Bar Chart of Failed Requests =====================\n",
    "def analyze_failed_requests():\n",
    "    # Example implementation, replace with actual logic\n",
    "    return student2_df\n",
    "\n",
    "def visualize_failed_requests():\n",
    "    # Get data from previous SQL query\n",
    "    failed_requests_df = analyze_failed_requests()\n",
    "    pdf = failed_requests_df.toPandas()\n",
    "def visualize_failed_requests():\n",
    "    # Get data from previous SQL query\n",
    "    failed_requests_df = analyze_failed_requests()\n",
    "    pdf = failed_requests_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(24, 12))\n",
    "    \n",
    "    # Create bar plot using Seaborn\n",
    "    sns.barplot(x='status_code', y='response_size', data=pdf)\n",
    "    \n",
    "    plt.title('Top 10 Failed Requests by Response Size', fontsize=14, pad=20)\n",
    "    plt.xlabel('Status Code', fontsize=12)\n",
    "    plt.ylabel('Response Size', fontsize=12)\n",
    "# ===================== Student 3: Heatmap of Hourly Traffic =====================\n",
    "def analyze_unique_visitors():\n",
    "    # Example implementation, replace with actual logic\n",
    "    return student3_df\n",
    "\n",
    "def visualize_hourly_traffic():\n",
    "    # Get data from previous SQL query\n",
    "    unique_visitors_df = analyze_unique_visitors()\n",
    "    pdf = unique_visitors_df.toPandas()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 3: Heatmap of Hourly Traffic =====================\n",
    "def visualize_hourly_traffic():\n",
    "    # Get data from previous SQL query\n",
    "    unique_visitors_df = analyze_unique_visitors()\n",
    "    pdf = unique_visitors_df.toPandas()\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    pdf['hour_of_day'] = pd.to_datetime(pdf['hour']).dt.hour\n",
    "    pdf['day'] = pd.to_datetime(pdf['hour']).dt.date\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = pdf.pivot_table(\n",
    "        values='total_visits', \n",
    "        index='day',\n",
    "        columns='hour_of_day',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_table, \n",
    "                cmap=\"YlGnBu\", \n",
    "                linewidths=.5, \n",
    "                annot=True, \n",
    "                fmt=\"d\")\n",
    "# ===================== Student 4: Pie Chart of Response Codes =====================\n",
    "def analyze_avg_response_size():\n",
    "    # Example implementation, replace with actual logic\n",
    "    return student4_df\n",
    "\n",
    "def visualize_response_distribution():\n",
    "    # Get data from previous SQL query\n",
    "    response_dist_df = analyze_avg_response_size()\n",
    "    pdf = response_dist_df.toPandas()\n",
    "    plt.title('Traffic Density by Hour and Day', fontsize=14, pad=20)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Date', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 4: Pie Chart of Response Codes =====================\n",
    "def visualize_response_distribution():\n",
    "    # Get data from previous SQL query\n",
    "    response_dist_df = analyze_avg_response_size()\n",
    "    pdf = response_dist_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.pie(pdf['request_count'], \n",
    "            labels=pdf['status_code'],\n",
    "            autopct='%1.1f%%',\n",
    "            explode=[0.05] * len(pdf),\n",
    "            shadow=True)\n",
    "    \n",
    "    plt.title('Distribution of HTTP Status Codes', fontsize=14, pad=20)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to generate all visualizations\n",
    "def generate_all_visualizations():\n",
    "    try:\n",
    "        print(\"Generating visualizations for all students...\")\n",
    "        \n",
    "        print(\"\\nStudent 1 - Rolling Traffic Line Chart:\")\n",
    "        visualize_rolling_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 2 - Failed Requests Bar Chart:\")\n",
    "        visualize_failed_requests()\n",
    "        \n",
    "        print(\"\\nStudent 3 - Traffic Density Heatmap:\")\n",
    "        visualize_hourly_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 4 - Response Codes Pie Chart:\")\n",
    "        visualize_response_distribution()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating visualizations: {str(e)}\")\n",
    "        \n",
    "        # Print debugging information\n",
    "        print(\"\\nDebugging information:\")\n",
    "        print(\"Available columns in the DataFrames:\")\n",
    "        spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "        spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Generate all visualizations\n",
    "generate_all_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing using PySpark RDD\n",
    "\n",
    "## Task 1: Basic RDD Analysis (10 marks)\n",
    "Each member will create a custom function to parse and process the log entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Basic Extraction Examples\n",
    "- **Student 1**: Extract Timestamp and IP  \n",
    "    **Description**: Parse timestamp and IP address from logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student 2**: Extract URL and HTTP Method  \n",
    "    **Description**: Parse URL path and HTTP method from logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student 3**: Extract Status Code and Response Size  \n",
    "    **Description**: Parse HTTP status and response size from logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Student 4**: Extract Log Message and IP Address  \n",
    "    **Description**: Parse log messages and corresponding IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing log entries...\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 1 - TIMESTAMP AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address: 88.211.105.115\n",
      "Timestamp:  04/Mar/2022:14:17:48\n",
      "\n",
      "IP Address: 144.6.49.142\n",
      "Timestamp:  02/Sep/2022:15:16:00\n",
      "\n",
      "IP Address: 231.70.64.145\n",
      "Timestamp:  19/Jul/2022:01:31:31\n",
      "\n",
      "IP Address: 219.42.234.172\n",
      "Timestamp:  08/Feb/2022:11:34:57\n",
      "\n",
      "IP Address: 183.173.185.94\n",
      "Timestamp:  29/Aug/2023:03:07:11\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 2 - URL AND HTTP METHOD EXTRACTION\n",
      "----------------------------------------\n",
      "HTTP Method: POST\n",
      "URL Path:    /history/missions/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "HTTP Method: PUT\n",
      "URL Path:    /web-development/countdown/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /networking/technology/\n",
      "\n",
      "HTTP Method: GET\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\n",
      "----------------------------------------\n",
      "Status Code:   414\n",
      "Response Size: 12456 bytes\n",
      "\n",
      "Status Code:   0\n",
      "Response Size: 0 bytes\n",
      "\n",
      "Status Code:   201\n",
      "Response Size: 33093 bytes\n",
      "\n",
      "Status Code:   415\n",
      "Response Size: 68827 bytes\n",
      "\n",
      "Status Code:   205\n",
      "Response Size: 30374 bytes\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  144.6.49.142\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  219.42.234.172\n",
      "Log Message: Detailed system state information.\n",
      "\n",
      "IP Address:  183.173.185.94\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Student 1: Extract Timestamp and IP\n",
    "def extract_timestamp_ip(log_line):\n",
    "    \"\"\"Extract timestamp and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    timestamp_pattern = r'\\[([^\\]]+)\\]'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        timestamp = re.search(timestamp_pattern, text).group(1)\n",
    "        return (ip, timestamp)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 2: Extract URL and HTTP Method\n",
    "def extract_url_method(log_line):\n",
    "    \"\"\"Extract URL path and HTTP method from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'\"(GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (match.group(1), match.group(2))\n",
    "        return (\"Not Found\", \"Not Found\")\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 3: Extract Status Code and Response Size\n",
    "def extract_status_size(log_line):\n",
    "    \"\"\"Extract HTTP status code and response size from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'HTTP/[\\d.]+\" (\\d{3}) (\\d+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (int(match.group(1)), int(match.group(2)))\n",
    "        return (0, 0)\n",
    "    except Exception as e:\n",
    "        return (0, 0)\n",
    "\n",
    "# Student 4: Extract Log Message and IP Address\n",
    "def extract_message_ip(log_line):\n",
    "    \"\"\"Extract log message and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    message_pattern = r'(?:Warning|Update|Debug|Error|Info):(.*?)(?:\\s*$)'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        message = re.search(message_pattern, text).group(1).strip()\n",
    "        return (ip, message)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "def analyze_logs(data):\n",
    "    \"\"\"Analyze logs using the existing Spark session\"\"\"\n",
    "    print(\"\\nProcessing log entries...\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to RDD of log lines\n",
    "        logs_rdd = data.rdd.map(lambda row: row[0])\n",
    "        \n",
    "        # Student 1 Analysis\n",
    "        print(\"STUDENT 1 - TIMESTAMP AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd1_results = logs_rdd.map(extract_timestamp_ip).collect()\n",
    "        for ip, timestamp in rdd1_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address: {ip}\")\n",
    "            print(f\"Timestamp:  {timestamp}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 2 Analysis\n",
    "        print(\"STUDENT 2 - URL AND HTTP METHOD EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd2_results = logs_rdd.map(extract_url_method).collect()\n",
    "        for method, url in rdd2_results[:5]:  # Show first 5 results\n",
    "            print(f\"HTTP Method: {method}\")\n",
    "            print(f\"URL Path:    {url}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 3 Analysis\n",
    "        print(\"STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd3_results = logs_rdd.map(extract_status_size).collect()\n",
    "        for status, size in rdd3_results[:5]:  # Show first 5 results\n",
    "            print(f\"Status Code:   {status}\")\n",
    "            print(f\"Response Size: {size} bytes\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 4 Analysis\n",
    "        print(\"STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd4_results = logs_rdd.map(extract_message_ip).collect()\n",
    "        for ip, message in rdd4_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address:  {ip}\")\n",
    "            print(f\"Log Message: {message}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "\n",
    "# Execute the analysis\n",
    "analyze_logs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Two Advanced RDD Analysis (30 marks)\n",
    "Each member will perform unique advanced processing tasks.\n",
    "\n",
    "### Student Advanced Analysis Examples\n",
    "- **Student 1**: Calculate hourly visit counts per IP  \n",
    "    **Description**: Count visits grouped by hour and IP.\n",
    "- **Student 2**: Identify top 10 URLs by visit count  \n",
    "    **Description**: Aggregate visit counts and rank top URLs.\n",
    "- **Student 3**: Find average response size per URL  \n",
    "    **Description**: Compute average response size for each URL.\n",
    "- **Student 4**: Detect failed requests per IP  \n",
    "    **Description**: Identify IPs with the most failed requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced RDD Analysis Results\n",
      "==================================================\n",
      "\n",
      "Student 1 - Hourly Visit Counts per IP\n",
      "----------------------------------------\n",
      "\n",
      "Sample of Hourly Visit Counts:\n",
      "IP: 1.1.104.46, Hour: 14:00, Visits: 1\n",
      "IP: 1.1.108.121, Hour: 07:00, Visits: 1\n",
      "IP: 1.1.118.39, Hour: 16:00, Visits: 1\n",
      "IP: 1.1.119.183, Hour: 20:00, Visits: 1\n",
      "IP: 1.1.120.92, Hour: 12:00, Visits: 1\n",
      "IP: 1.1.126.167, Hour: 07:00, Visits: 1\n",
      "IP: 1.1.134.30, Hour: 06:00, Visits: 1\n",
      "IP: 1.1.134.82, Hour: 06:00, Visits: 1\n",
      "IP: 1.1.138.78, Hour: 14:00, Visits: 1\n",
      "IP: 1.1.153.25, Hour: 14:00, Visits: 1\n",
      "\n",
      "Top 5 IPs by Total Visits:\n",
      "IP: 10.45.78.214, Total Visits: 2\n",
      "IP: 100.59.95.187, Total Visits: 2\n",
      "IP: 102.169.147.90, Total Visits: 2\n",
      "IP: 104.126.92.191, Total Visits: 2\n",
      "IP: 108.226.151.237, Total Visits: 2\n",
      "\n",
      "==================================================\n",
      "\n",
      "Student 2 - Top 10 URLs by Visit Count\n",
      "----------------------------------------\n",
      "\n",
      "Top 10 Most Visited URLs:\n",
      "URL: /data-analysis/apollo/\n",
      "Visit Count: 25440\n",
      "\n",
      "URL: /software/data/\n",
      "Visit Count: 25382\n",
      "\n",
      "URL: /web-development/missions/\n",
      "Visit Count: 25343\n",
      "\n",
      "URL: /history/countdown/\n",
      "Visit Count: 25338\n",
      "\n",
      "URL: /software/technology/\n",
      "Visit Count: 25327\n",
      "\n",
      "URL: /security/technology/\n",
      "Visit Count: 25307\n",
      "\n",
      "URL: /networking/deep-learning/\n",
      "Visit Count: 25300\n",
      "\n",
      "URL: /networking/firewall/\n",
      "Visit Count: 25249\n",
      "\n",
      "URL: /security/countdown/\n",
      "Visit Count: 25239\n",
      "\n",
      "URL: /history/apollo/\n",
      "Visit Count: 25238\n",
      "\n",
      "\n",
      "Traffic Distribution:\n",
      "URL: /data-analysis/apollo/\n",
      "Percentage of Total Traffic: 0.85%\n",
      "\n",
      "URL: /software/data/\n",
      "Percentage of Total Traffic: 0.85%\n",
      "\n",
      "URL: /web-development/missions/\n",
      "Percentage of Total Traffic: 0.84%\n",
      "\n",
      "URL: /history/countdown/\n",
      "Percentage of Total Traffic: 0.84%\n",
      "\n",
      "URL: /software/technology/\n",
      "Percentage of Total Traffic: 0.84%\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Student 3 - Average Response Size per URL\n",
      "----------------------------------------\n",
      "\n",
      "URL Response Size Analysis:\n",
      "URL: /machine-learning/firewall/\n",
      "Average Response Size: 51108.07 bytes\n",
      "\n",
      "URL: /shuttle/deep-learning/\n",
      "Average Response Size: 51033.46 bytes\n",
      "\n",
      "URL: /cloud-computing/firewall/\n",
      "Average Response Size: 50966.29 bytes\n",
      "\n",
      "URL: /security/aws-certification/\n",
      "Average Response Size: 50944.89 bytes\n",
      "\n",
      "URL: /images/launch/\n",
      "Average Response Size: 50903.06 bytes\n",
      "\n",
      "URL: /shuttle/firewall/\n",
      "Average Response Size: 50881.76 bytes\n",
      "\n",
      "URL: /security/apollo-11/\n",
      "Average Response Size: 50873.90 bytes\n",
      "\n",
      "URL: /history/deep-learning/\n",
      "Average Response Size: 50865.10 bytes\n",
      "\n",
      "URL: /data-analysis/aws-certification/\n",
      "Average Response Size: 50815.88 bytes\n",
      "\n",
      "URL: /images/countdown/\n",
      "Average Response Size: 50781.23 bytes\n",
      "\n",
      "\n",
      "Response Size Statistics:\n",
      "Error during analysis: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Execute the analysis\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m advanced_rdd_analysis(data)\n",
      "Cell \u001b[1;32mIn[21], line 251\u001b[0m, in \u001b[0;36madvanced_rdd_analysis\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    249\u001b[0m student2_analysis(logs_rdd)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m--> 251\u001b[0m student3_analysis(logs_rdd)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    253\u001b[0m student4_analysis(logs_rdd)\n",
      "Cell \u001b[1;32mIn[21], line 116\u001b[0m, in \u001b[0;36madvanced_rdd_analysis.<locals>.student3_analysis\u001b[1;34m(logs_rdd)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_sizes:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse Size Statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum Average Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(all_sizes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinimum Average Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(all_sizes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Average Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(all_sizes)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_sizes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py:687\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(col: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;124;03m    Aggregate function: returns the maximum value of the expression in a group.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03m    +-------+\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function_over_columns(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, col)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py:105\u001b[0m, in \u001b[0;36m_invoke_function_over_columns\u001b[1;34m(name, *cols)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\functions.py:105\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke_function_over_columns\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    Invokes n-ary JVM function identified by name\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    and wraps the result with :class:`~pyspark.sql.Column`.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(name, \u001b[38;5;241m*\u001b[39m(_to_java_column(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m     66\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     67\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got list."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re # Regular expressions\n",
    "\n",
    "def advanced_rdd_analysis(data):\n",
    "    \"\"\"Perform advanced RDD analysis for all students\"\"\"\n",
    "    # Convert DataFrame to RDD\n",
    "    logs_rdd = data.rdd.map(lambda row: row[0])\n",
    "    \n",
    "    print(\"\\nAdvanced RDD Analysis Results\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Student 1: Calculate hourly visit counts per IP\n",
    "    def student1_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 1 - Hourly Visit Counts per IP\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_hour_ip(log_line):\n",
    "            ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "            timestamp_pattern = r'\\[([^\\]]+)\\]'\n",
    "            try:\n",
    "                ip = re.search(ip_pattern, log_line).group(1)\n",
    "                timestamp_str = re.search(timestamp_pattern, log_line).group(1)\n",
    "                # Parse timestamp and extract hour\n",
    "                dt = datetime.strptime(timestamp_str, '%d/%b/%Y:%H:%M:%S')\n",
    "                hour = dt.strftime('%H:00')\n",
    "                return ((ip, hour), 1)\n",
    "            except Exception as e:\n",
    "                return ((\"Error\", \"Error\"), 0)\n",
    "\n",
    "        # Map-Reduce to count visits\n",
    "        hourly_counts = logs_rdd \\\n",
    "            .map(extract_hour_ip) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .map(lambda x: (x[0][0], x[0][1], x[1])) \\\n",
    "            .sortBy(lambda x: (x[0], x[1]))\n",
    "\n",
    "        print(\"\\nSample of Hourly Visit Counts:\")\n",
    "        for ip, hour, count in hourly_counts.take(10):\n",
    "            print(f\"IP: {ip}, Hour: {hour}, Visits: {count}\")\n",
    "\n",
    "        # Calculate total visits per IP\n",
    "        total_visits = hourly_counts \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nTop 5 IPs by Total Visits:\")\n",
    "        for ip, total in total_visits.take(5):\n",
    "            print(f\"IP: {ip}, Total Visits: {total}\")\n",
    "\n",
    "    # Student 2: Identify top 10 URLs by visit count\n",
    "    def student2_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 2 - Top 10 URLs by Visit Count\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_url(log_line):\n",
    "            pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "            try:\n",
    "                url = re.search(pattern, log_line).group(1)\n",
    "                return (url, 1)\n",
    "            except Exception:\n",
    "                return (\"Error\", 0)\n",
    "\n",
    "        # Calculate URL visit counts\n",
    "        url_counts = logs_rdd \\\n",
    "            .map(extract_url) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nTop 10 Most Visited URLs:\")\n",
    "        for url, count in url_counts.take(10):\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Visit Count: {count}\\n\")\n",
    "\n",
    "        # Calculate percentage of total traffic for top URLs\n",
    "        total_visits = url_counts.map(lambda x: x[1]).sum()\n",
    "        print(\"\\nTraffic Distribution:\")\n",
    "        for url, count in url_counts.take(5):\n",
    "            percentage = (count / total_visits) * 100\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Percentage of Total Traffic: {percentage:.2f}%\\n\")\n",
    "\n",
    "    # Student 3: Find average response size per URL\n",
    "    def student3_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 3 - Average Response Size per URL\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_url_size(log_line):\n",
    "            url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "            size_pattern = r'HTTP/[\\d.]+\" \\d{3} (\\d+)'\n",
    "            try:\n",
    "                url = re.search(url_pattern, log_line).group(1)\n",
    "                size = int(re.search(size_pattern, log_line).group(1))\n",
    "                return (url, (size, 1))\n",
    "            except Exception:\n",
    "                return (\"Error\", (0, 0))\n",
    "\n",
    "        # Calculate average response size\n",
    "        avg_sizes = logs_rdd \\\n",
    "            .map(extract_url_size) \\\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "            .mapValues(lambda x: x[0] / x[1] if x[1] > 0 else 0) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nURL Response Size Analysis:\")\n",
    "        for url, avg_size in avg_sizes.take(10):\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Average Response Size: {avg_size:.2f} bytes\\n\")\n",
    "\n",
    "        # Calculate size distribution statistics\n",
    "        all_sizes = avg_sizes.map(lambda x: x[1]).collect()\n",
    "        if all_sizes:\n",
    "            print(\"\\nResponse Size Statistics:\")\n",
    "            print(f\"Maximum Average Size: {max(all_sizes):.2f} bytes\")\n",
    "            print(f\"Minimum Average Size: {min(all_sizes):.2f} bytes\")\n",
    "            print(f\"Overall Average Size: {sum(all_sizes)/len(all_sizes):.2f} bytes\")\n",
    "\n",
    "   # Student 4: Detect failed requests per IP\n",
    "    def student4_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 4 - Failed Requests Analysis per IP\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_ip_status(log_line):\n",
    "            ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "            status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "            try:\n",
    "                ip = re.search(ip_pattern, log_line).group(1)\n",
    "                status = int(re.search(status_pattern, log_line).group(1))\n",
    "                is_failed = 1 if status >= 400 else 0\n",
    "                return (ip, (is_failed, 1))  # Count both failed and total requests\n",
    "            except Exception:\n",
    "                return (\"Error\", (0, 0))\n",
    "\n",
    "        # Calculate failed requests statistics\n",
    "        ip_stats = logs_rdd \\\n",
    "            .map(extract_ip_status) \\\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "            .mapValues(lambda x: {\n",
    "                'failed': x[0],\n",
    "                'total': x[1],\n",
    "                'failure_rate': (x[0] / x[1] * 100) if x[1] > 0 else 0\n",
    "            }) \\\n",
    "            .sortBy(lambda x: x[1]['failed'], ascending=False)\n",
    "\n",
    "        print(\"\\nFailed Requests Analysis:\")\n",
    "        for ip, stats in ip_stats.take(10):\n",
    "            print(f\"IP: {ip}\")\n",
    "            print(f\"Failed Requests: {stats['failed']}\")\n",
    "            print(f\"Total Requests: {stats['total']}\")\n",
    "            print(f\"Failure Rate: {stats['failure_rate']:.2f}%\\n\")\n",
    "\n",
    "        # Calculate overall statistics\n",
    "        stats_collected = ip_stats.collect()\n",
    "        if stats_collected:\n",
    "            total_failed = sum(stats[1]['failed'] for stats in stats_collected)\n",
    "            total_requests = sum(stats[1]['total'] for stats in stats_collected)\n",
    "            overall_failure_rate = (total_failed / total_requests * 100) if total_requests > 0 else 0\n",
    "            \n",
    "            print(\"\\nOverall Statistics:\")\n",
    "            print(f\"Total Failed Requests: {total_failed}\")\n",
    "            print(f\"Total Requests: {total_requests}\")\n",
    "            print(f\"Overall Failure Rate: {overall_failure_rate:.2f}%\")\n",
    "\n",
    "            # Additional Analysis\n",
    "            # Find IPs with highest failure rates (minimum 10 requests)\n",
    "            high_failure_ips = [(ip, stats) for ip, stats in stats_collected \n",
    "                              if stats['total'] >= 10]\n",
    "            high_failure_ips.sort(key=lambda x: x[1]['failure_rate'], reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 5 IPs with Highest Failure Rates (min. 10 requests):\")\n",
    "            for ip, stats in high_failure_ips[:5]:\n",
    "                print(f\"IP: {ip}\")\n",
    "                print(f\"Failure Rate: {stats['failure_rate']:.2f}%\")\n",
    "                print(f\"Failed Requests: {stats['failed']}\")\n",
    "                print(f\"Total Requests: {stats['total']}\\n\")\n",
    "    \n",
    "\n",
    "    # Student 4: Detect failed requests per IP\n",
    "    def student4_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 4 - Failed Requests Analysis per IP\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_ip_status(log_line):\n",
    "            ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "            status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "            try:\n",
    "                ip = re.search(ip_pattern, log_line).group(1)\n",
    "                status = int(re.search(status_pattern, log_line).group(1))\n",
    "                is_failed = 1 if status >= 400 else 0\n",
    "                return (ip, (is_failed, 1))\n",
    "            except Exception as e:\n",
    "                return (\"Error\", (0, 0))\n",
    "\n",
    "        # Process the data\n",
    "        ip_stats = logs_rdd \\\n",
    "            .map(extract_ip_status) \\\n",
    "            .filter(lambda x: x[0] != \"Error\") \\\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "            .map(lambda x: (\n",
    "                x[0], \n",
    "                {\n",
    "                    'failed': x[1][0],\n",
    "                    'total': x[1][1],\n",
    "                    'failure_rate': round((x[1][0] / x[1][1] * 100), 2) if x[1][1] > 0 else 0\n",
    "                }\n",
    "            ))\n",
    "\n",
    "        # Sort by number of failed requests\n",
    "        top_failed = ip_stats.sortBy(lambda x: x[1]['failed'], ascending=False)\n",
    "\n",
    "        print(\"\\nTop 10 IPs by Failed Requests:\")\n",
    "        for ip, stats in top_failed.take(10):\n",
    "            print(f\"IP: {ip}\")\n",
    "            print(f\"Failed Requests: {stats['failed']}\")\n",
    "            print(f\"Total Requests: {stats['total']}\")\n",
    "            print(f\"Failure Rate: {stats['failure_rate']:.2f}%\\n\")\n",
    "\n",
    "        # Calculate overall statistics\n",
    "        all_stats = ip_stats.collect()\n",
    "        if all_stats:\n",
    "            total_failed = sum(stats[1]['failed'] for stats in all_stats)\n",
    "            total_requests = sum(stats[1]['total'] for stats in all_stats)\n",
    "            overall_rate = round((total_failed / total_requests * 100), 2) if total_requests > 0 else 0\n",
    "\n",
    "            print(\"Overall Statistics:\")\n",
    "            print(f\"Total Failed Requests: {total_failed}\")\n",
    "            print(f\"Total Requests: {total_requests}\")\n",
    "            print(f\"Overall Failure Rate: {overall_rate:.2f}%\\n\")\n",
    "\n",
    "            # Find IPs with highest failure rates (min 10 requests)\n",
    "            high_failure_rates = ip_stats \\\n",
    "                .filter(lambda x: x[1]['total'] >= 10) \\\n",
    "                .sortBy(lambda x: x[1]['failure_rate'], ascending=False)\n",
    "\n",
    "            print(\"Top 5 IPs with Highest Failure Rates (min 10 requests):\")\n",
    "            for ip, stats in high_failure_rates.take(5):\n",
    "                print(f\"IP: {ip}\")\n",
    "                print(f\"Failure Rate: {stats['failure_rate']:.2f}%\")\n",
    "                print(f\"Failed: {stats['failed']}/{stats['total']} requests\\n\")\n",
    "        else:\n",
    "            print(\"No valid data found for analysis\")\n",
    "\n",
    "    # Execute all students' analyses\n",
    "    try:\n",
    "        student1_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student2_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student3_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student4_analysis(logs_rdd)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute the analysis\n",
    "advanced_rdd_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Optimization and LSEPI Considerations (10 marks)\n",
    "Each member chooses two unique methods for optimization.\n",
    "\n",
    "### Student Optimization Methods\n",
    "- **Student 1**: Partition Strategies, Caching\n",
    "- **Student 2**: Caching, Bucketing & Indexing\n",
    "- **Student 3**: Partition Strategies, Bucketing & Indexing\n",
    "- **Student 4**: Caching, Partition Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Optimization Analyses for All Students\n",
      "============================================================\n",
      "\n",
      "Student 1 - Partition Strategies and Caching\n",
      "========================================\n",
      "\n",
      "Student 1 - Baseline Analysis (No Optimization)\n",
      "--------------------------------------------------\n",
      "Execution time: 38.66 seconds\n",
      "\n",
      "Student 1 - Analysis with Custom Partitioning\n",
      "--------------------------------------------------\n",
      "Execution time: 52.99 seconds\n",
      "\n",
      "Student 1 - Analysis with Caching\n",
      "--------------------------------------------------\n",
      "Execution time: 45.80 seconds\n",
      "\n",
      "Performance Improvement:\n",
      "Partitioning: -37.06% faster\n",
      "Caching: -18.46% faster\n",
      "\n",
      "Student 2 - Caching and Bucketing\n",
      "========================================\n",
      "\n",
      "Student 2 - Baseline Analysis (No Optimization)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 303.0 failed 1 times, most recent failure: Lost task 7.0 in stage 303.0 (TID 1127) (192.168.1.159 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7876\\2416317130.py\", line 90, in <lambda>\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7876\\2416317130.py\", line 90, in <lambda>\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 331\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitioning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((time1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtime3)\u001b[38;5;241m/\u001b[39mtime1)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% faster\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Run all optimizations with SparkSession\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m run_all_optimizations(data, spark)\n",
      "Cell \u001b[1;32mIn[26], line 301\u001b[0m, in \u001b[0;36mrun_all_optimizations\u001b[1;34m(data, spark)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m    300\u001b[0m s2 \u001b[38;5;241m=\u001b[39m Student2Optimization(data, spark)\n\u001b[1;32m--> 301\u001b[0m baseline2, time1 \u001b[38;5;241m=\u001b[39m s2\u001b[38;5;241m.\u001b[39mbaseline_analysis()\n\u001b[0;32m    302\u001b[0m cache2, time2 \u001b[38;5;241m=\u001b[39m s2\u001b[38;5;241m.\u001b[39moptimized_with_caching()\n\u001b[0;32m    303\u001b[0m bucket2, time3 \u001b[38;5;241m=\u001b[39m s2\u001b[38;5;241m.\u001b[39moptimized_with_bucketing()\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mmeasure_execution_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      9\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 10\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[26], line 91\u001b[0m, in \u001b[0;36mStudent2Optimization.baseline_analysis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     87\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     88\u001b[0m result \u001b[38;5;241m=\u001b[39m (rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_url_status)\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;241m.\u001b[39mgroupByKey()\n\u001b[0;32m     90\u001b[0m             \u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m x))\n\u001b[1;32m---> 91\u001b[0m             \u001b[38;5;241m.\u001b[39mcollect())\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 303.0 failed 1 times, most recent failure: Lost task 7.0 in stage 303.0 (TID 1127) (192.168.1.159 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7876\\2416317130.py\", line 90, in <lambda>\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 4260, in map_values_fn\n    return kv[0], f(kv[1])\n                  ^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7876\\2416317130.py\", line 90, in <lambda>\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 866, in sum\n    return _invoke_function_over_columns(\"sum\", col)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in _invoke_function_over_columns\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 105, in <genexpr>\n    return _invoke_function(name, *(_to_java_column(col) for col in cols))\n                                    ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\column.py\", line 65, in _to_java_column\n    raise PySparkTypeError(\npyspark.errors.exceptions.base.PySparkTypeError: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got generator.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import builtins\n",
    "import time\n",
    "import re\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "class Student1Optimization:\n",
    "    \"\"\"Student 1: Partition Strategies and Caching\"\"\"\n",
    "    \n",
    "    def __init__(self, data, spark):\n",
    "        self.data = data\n",
    "        self.spark = spark\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 1 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = rdd.map(self.extract_data).groupByKey().mapValues(len).collect()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 1 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def custom_partitioner(key):\n",
    "            return builtins.sum(ord(c) for c in str(key)) % 10\n",
    "            return sum(ord(c) for c in str(key)) % 10\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_data)\n",
    "                    .partitionBy(10, custom_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(len)\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 1 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_data).cache()\n",
    "        result = cached_rdd.groupByKey().mapValues(len).collect()\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_data(log_line):\n",
    "        \"\"\"Extract IP and timestamp from log line\"\"\"\n",
    "        ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "        try:\n",
    "            ip = re.search(ip_pattern, log_line).group(1)\n",
    "            return (ip, 1)\n",
    "        except:\n",
    "            return (\"error\", 1)\n",
    "\n",
    "class Student2Optimization:\n",
    "    \"\"\"Student 2: Caching and Bucketing & Indexing\"\"\"\n",
    "    \n",
    "    def __init__(self, data, spark):\n",
    "        self.data = data\n",
    "        self.spark = spark\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 2 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_status)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for _ in x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 2 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_url_status).cache()\n",
    "        result = (cached_rdd.groupByKey()\n",
    "                          .mapValues(lambda x: sum(1 for _ in x))\n",
    "                          .collect())\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_bucketing(self):\n",
    "        \"\"\"Analysis with bucketing\"\"\"\n",
    "        print(\"\\nStudent 2 - Analysis with Bucketing\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create temporary view\n",
    "        df = self.data.selectExpr(\"value as log_line\")\n",
    "        df.createOrReplaceTempView(\"logs\")\n",
    "        \n",
    "        # SQL query with bucketing\n",
    "        bucketed_df = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                regexp_extract(log_line, '\"(?:GET|POST|PUT|DELETE)\\\\s+([^\"\\\\s]+)', 1) as url,\n",
    "                count(*) as count\n",
    "            FROM logs\n",
    "            GROUP BY url\n",
    "            ORDER BY count DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        return bucketed_df.collect()\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_url_status(log_line):\n",
    "        \"\"\"Extract URL and status code from log line\"\"\"\n",
    "        url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "        status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "        try:\n",
    "            url = re.search(url_pattern, log_line).group(1)\n",
    "            status = re.search(status_pattern, log_line).group(1)\n",
    "            return (url, status)\n",
    "        except:\n",
    "            return (\"error\", \"0\")\n",
    "\n",
    "class Student3Optimization:\n",
    "    \"\"\"Student 3: Partition Strategies and Bucketing & Indexing\"\"\"\n",
    "    \n",
    "    def __init__(self, data, spark):\n",
    "        self.data = data\n",
    "        self.spark = spark\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 3 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_size)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(x)/len(x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 3 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def url_partitioner(url):\n",
    "            return sum(ord(c) for c in str(url)) % 8\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_size)\n",
    "                    .partitionBy(8, url_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(x)/len(x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_bucketing(self):\n",
    "        \"\"\"Analysis with bucketing\"\"\"\n",
    "        print(\"\\nStudent 3 - Analysis with Bucketing\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Create temporary view\n",
    "        df = self.data.selectExpr(\"value as log_line\")\n",
    "        df.createOrReplaceTempView(\"logs\")\n",
    "        \n",
    "        # SQL query with bucketing\n",
    "        bucketed_df = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                regexp_extract(log_line, '\"(?:GET|POST|PUT|DELETE)\\\\s+([^\"\\\\s]+)', 1) as url,\n",
    "                avg(cast(regexp_extract(log_line, 'HTTP/[\\\\d.]+\" \\\\d{3} (\\\\d+)', 1) as int)) as avg_size\n",
    "            FROM logs\n",
    "            GROUP BY url\n",
    "            ORDER BY avg_size DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        return bucketed_df.collect()\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_url_size(log_line):\n",
    "        \"\"\"Extract URL and response size from log line\"\"\"\n",
    "        url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "        size_pattern = r'HTTP/[\\d.]+\" \\d{3} (\\d+)'\n",
    "        try:\n",
    "            url = re.search(url_pattern, log_line).group(1)\n",
    "            size = int(re.search(size_pattern, log_line).group(1))\n",
    "            return (url, size)\n",
    "        except:\n",
    "            return (\"error\", 0)\n",
    "\n",
    "class Student4Optimization:\n",
    "    \"\"\"Student 4: Caching and Partition Strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, data, spark):\n",
    "        self.data = data\n",
    "        self.spark = spark\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 4 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_ip_status)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 4 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_ip_status).cache()\n",
    "        result = (cached_rdd.groupByKey()\n",
    "                          .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                          .collect())\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 4 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def ip_partitioner(ip):\n",
    "            try:\n",
    "                first_octet = sum(ord(c) for c in str(ip.split('.')[0]))\n",
    "                return first_octet % 8\n",
    "            except:\n",
    "                return 0\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_ip_status)\n",
    "                    .partitionBy(8, ip_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_ip_status(log_line):\n",
    "        \"\"\"Extract IP and status code from log line\"\"\"\n",
    "        ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "        status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "        try:\n",
    "            ip = re.search(ip_pattern, log_line).group(1)\n",
    "            status = re.search(status_pattern, log_line).group(1)\n",
    "            return (ip, status)\n",
    "        except:\n",
    "            return (\"error\", \"0\")\n",
    "\n",
    "def run_all_optimizations(data, spark):\n",
    "    \"\"\"Run all students' optimization analyses\"\"\"\n",
    "    \n",
    "    print(\"\\nRunning Optimization Analyses for All Students\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Student 1\n",
    "    print(\"\\nStudent 1 - Partition Strategies and Caching\")\n",
    "    print(\"=\" * 40)\n",
    "    s1 = Student1Optimization(data, spark)\n",
    "    baseline1, time1 = s1.baseline_analysis()\n",
    "    partition1, time2 = s1.optimized_with_partitioning()\n",
    "    cache1, time3 = s1.optimized_with_caching()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Partitioning: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Caching: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 2\n",
    "    print(\"\\nStudent 2 - Caching and Bucketing\")\n",
    "    print(\"=\" * 40)\n",
    "    s2 = Student2Optimization(data, spark)\n",
    "    baseline2, time1 = s2.baseline_analysis()\n",
    "    cache2, time2 = s2.optimized_with_caching()\n",
    "    bucket2, time3 = s2.optimized_with_bucketing()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Caching: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Bucketing: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 3\n",
    "    print(\"\\nStudent 3 - Partition Strategies and Bucketing\")\n",
    "    print(\"=\" * 40)\n",
    "    s3 = Student3Optimization(data, spark)\n",
    "    baseline3, time1 = s3.baseline_analysis()\n",
    "    partition3, time2 = s3.optimized_with_partitioning()\n",
    "    bucket3, time3 = s3.optimized_with_bucketing()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Partitioning: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Bucketing: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 4\n",
    "    print(\"\\nStudent 4 - Caching and Partition Strategies\")\n",
    "    print(\"=\" * 40)\n",
    "    s4 = Student4Optimization(data, spark)\n",
    "    baseline4, time1 = s4.baseline_analysis()\n",
    "    cache4, time2 = s4.optimized_with_caching()\n",
    "    partition4, time3 = s4.optimized_with_partitioning()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Caching: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Partitioning: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "\n",
    "# Run all optimizations with SparkSession\n",
    "run_all_optimizations(data, spark)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "playing-word-embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
