{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7LkGfi9QpjU"
   },
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "\n",
    "## Group ID: CN7031_Group136_2024\n",
    "\n",
    "### Group Members:\n",
    "1. **Navya Athoti**  \n",
    "    Email: u2793047@uel.ac.uk\n",
    "2. **Phalguna Avalagunta**  \n",
    "    Email: u2811669@uel.ac.uk\n",
    "3. **Nikhil Sai Damera**  \n",
    "    Email: u2810262@uel.ac.uk\n",
    "4. **Sai Kishore Dodda**  \n",
    "    Email: u2773584@uel.ac.uk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and Configure Spark\n",
    "\n",
    "In this section, we will initiate and configure Apache Spark, which is a powerful open-source processing engine for big data. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FdzItAjQzNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "import sys\n",
    "\n",
    "# environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = (SparkSession.builder\n",
    "            .appName('CN7031_Group136_2024')\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFY_ihnakGwV"
   },
   "source": [
    "# Load Unstructured Data\n",
    "\n",
    "In this section, we will load and process unstructured data. Unstructured data refers to information that does not have a predefined data model or is not organized in a predefined manner. This type of data is typically text-heavy, but may also contain data such as dates, numbers, and facts.\n",
    "\n",
    "We will explore various techniques to handle and analyze unstructured data, including tokenization, vectorization, and the use of embeddings to capture semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "e0WKNOD1kh-K",
    "outputId": "9f864819-0942-4542-d184-53133f43c816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n"
     ]
    }
   ],
   "source": [
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the data loading\n",
    "try:\n",
    "    data = load_data(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing using PySpark DataFrame [40 marks]\n",
    "\n",
    "---\n",
    "\n",
    "## DataFrame Creation with REGEX (10 marks)\n",
    "\n",
    "Each member will define a custom schema using REGEX to extract specific metrics from the dataset.\n",
    "\n",
    "### Student Metrics to Extract\n",
    "\n",
    "- **Student 1: IP Address, Timestamp, HTTP Method**\n",
    "    - **REGEX Example:** `(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)`\n",
    "\n",
    "- **Student 2: HTTP Status Code, Response Size, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]`\n",
    "\n",
    "- **Student 3: URL Path, IP Address, Response Size**\n",
    "    - **REGEX Example:** `\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)`\n",
    "\n",
    "- **Student 4: Log Message, HTTP Status Code, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) .* \\[(.*?)\\] (.*)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 DataFrame Schema:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Student 1 Sample Data:\n",
      "+--------------+-------------------+-----------+\n",
      "|ip_address    |timestamp          |http_method|\n",
      "+--------------+-------------------+-----------+\n",
      "|88.211.105.115|2022-03-04 14:17:48|POST       |\n",
      "|144.6.49.142  |2022-09-02 15:16:00|POST       |\n",
      "|231.70.64.145 |2022-07-19 01:31:31|PUT        |\n",
      "|219.42.234.172|2022-02-08 11:34:57|POST       |\n",
      "|183.173.185.94|2023-08-29 03:07:11|GET        |\n",
      "+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 1 Validation Counts:\n",
      "+----------------+---------------+-----------------+\n",
      "|ip_address_count|timestamp_count|http_method_count|\n",
      "+----------------+---------------+-----------------+\n",
      "|         3000000|        3000000|          3000000|\n",
      "+----------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "Student 2 DataFrame Schema:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 2 Sample Data:\n",
      "+-----------+-------------+-------------------+\n",
      "|status_code|response_size|timestamp          |\n",
      "+-----------+-------------+-------------------+\n",
      "|414        |12456        |2022-03-04 14:17:48|\n",
      "|203        |97126        |2022-09-02 15:16:00|\n",
      "|201        |33093        |2022-07-19 01:31:31|\n",
      "|415        |68827        |2022-02-08 11:34:57|\n",
      "|205        |30374        |2023-08-29 03:07:11|\n",
      "+-----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 2 Validation Counts:\n",
      "+-----------------+-------------------+---------------+\n",
      "|status_code_count|response_size_count|timestamp_count|\n",
      "+-----------------+-------------------+---------------+\n",
      "|          3000000|            3000000|        3000000|\n",
      "+-----------------+-------------------+---------------+\n",
      "\n",
      "\n",
      "Student 3 DataFrame Schema:\n",
      "root\n",
      " |-- url_path: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n",
      "\n",
      "Student 3 Sample Data:\n",
      "+---------------------------+--------------+-------------+\n",
      "|url_path                   |ip_address    |response_size|\n",
      "+---------------------------+--------------+-------------+\n",
      "|/history/missions/         |88.211.105.115|12456        |\n",
      "|/security/firewall/        |144.6.49.142  |97126        |\n",
      "|/web-development/countdown/|231.70.64.145 |33093        |\n",
      "|/networking/technology/    |219.42.234.172|68827        |\n",
      "|/security/firewall/        |183.173.185.94|30374        |\n",
      "+---------------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 Validation Counts:\n",
      "+--------------+----------------+-------------------+\n",
      "|url_path_count|ip_address_count|response_size_count|\n",
      "+--------------+----------------+-------------------+\n",
      "|       3000000|         3000000|            3000000|\n",
      "+--------------+----------------+-------------------+\n",
      "\n",
      "\n",
      "Student 4 DataFrame Schema:\n",
      "root\n",
      " |-- log_message: string (nullable = true)\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 4 Sample Data:\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|log_message                             |status_code|timestamp          |\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|POST /history/missions/ HTTP/2.0        |414        |2022-03-04 14:17:48|\n",
      "|POST /security/firewall/ HTTPS/1.0      |203        |2022-09-02 15:16:00|\n",
      "|PUT /web-development/countdown/ HTTP/1.0|201        |2022-07-19 01:31:31|\n",
      "|POST /networking/technology/ HTTP/1.0   |415        |2022-02-08 11:34:57|\n",
      "|GET /security/firewall/ HTTP/2.0        |205        |2023-08-29 03:07:11|\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Validation Counts:\n",
      "+-----------------+-----------------+---------------+\n",
      "|log_message_count|status_code_count|timestamp_count|\n",
      "+-----------------+-----------------+---------------+\n",
      "|          3000000|          3000000|        3000000|\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common imports and Spark initialization for all students\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import regexp_extract, to_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file\n",
    "logs_df = spark.read.text(\"web.log\")\n",
    "\n",
    "# Student 1 (Navya A) - IP Address, Timestamp, HTTP Method\n",
    "student1_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), r'\"(\\w+)', 1).alias(\"http_method\")\n",
    ")\n",
    "\n",
    "# Student 2 - HTTP Status Code, Response Size, Timestamp\n",
    "student2_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Student 3 - URL Path, IP Address, Response Size\n",
    "student3_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"[A-Z]+ (.*?) HTTP', 1).alias(\"url_path\"),\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\")\n",
    ")\n",
    "\n",
    "# Student 4 - Log Message, HTTP Status Code, Timestamp\n",
    "student4_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"(.*?)\"', 1).alias(\"log_message\"),\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Function to validate and show results for each student's DataFrame\n",
    "def validate_dataframe(df, student_num):\n",
    "    print(f\"\\nStudent {student_num} DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nStudent {student_num} Sample Data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Count non-null values for each column\n",
    "    print(f\"\\nStudent {student_num} Validation Counts:\")\n",
    "    df.select([\n",
    "        sum(col(c).isNotNull().cast(\"int\")).alias(f\"{c}_count\")\n",
    "        for c in df.columns\n",
    "    ]).show()\n",
    "\n",
    "# Validate each student's DataFrame\n",
    "validate_dataframe(student1_df, 1)\n",
    "validate_dataframe(student2_df, 2)\n",
    "validate_dataframe(student3_df, 3)\n",
    "validate_dataframe(student4_df, 4)\n",
    "\n",
    "# Register DataFrames as views for SQL queries later\n",
    "student1_df.createOrReplaceTempView(\"student1_logs\")\n",
    "student2_df.createOrReplaceTempView(\"student2_logs\")\n",
    "student3_df.createOrReplaceTempView(\"student3_logs\")\n",
    "student4_df.createOrReplaceTempView(\"student4_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F4fH4d-LJhH"
   },
   "source": [
    "# Task 2: Two Advanced DataFrame Analysis (20 marks)\n",
    "\n",
    "Each member will write unique SQL queries for the analysis:\n",
    "\n",
    "## SQL Query 1: Window Functions\n",
    "\n",
    "- **Student 1: Rolling hourly traffic per IP**\n",
    "    - **Description:** Calculate traffic count per IP over a sliding window.\n",
    "\n",
    "- **Student 2: Session identification**\n",
    "    - **Description:** Identify sessions based on timestamp gaps.\n",
    "\n",
    "- **Student 3: Unique visitors per hour**\n",
    "    - **Description:** Count distinct IPs for each hour.\n",
    "\n",
    "- **Student 4: Average response size per status code**\n",
    "    - **Description:** Compute averages grouped by status codes.\n",
    "\n",
    "## SQL Query 2: Aggregation Functions\n",
    "\n",
    "- **Student 1: Traffic patterns by URL path**\n",
    "    - **Description:** Analyze URL visits by hour.\n",
    "\n",
    "- **Student 2: Top 10 failed requests by size**\n",
    "    - **Description:** Identify the largest failed requests.\n",
    "\n",
    "- **Student 3: Response size distribution by status**\n",
    "    - **Description:** Show min, max, and avg sizes for each status.\n",
    "\n",
    "- **Student 4: Daily unique visitors**\n",
    "    - **Description:** Count unique IPs per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "NyQQvVRrmhlQ",
    "outputId": "8b3fac55-bd76-43ab-dafe-1b0358025729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in student1_logs:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Available columns in student2_logs:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "=== Window Functions Analysis ===\n",
      "\n",
      "Student 2 - Session Identification:\n",
      "+-------------------+-----------+-----------+\n",
      "|          timestamp|status_code|new_session|\n",
      "+-------------------+-----------+-----------+\n",
      "|2022-01-01 00:01:34|        203|          1|\n",
      "|2022-01-01 00:01:50|        414|          0|\n",
      "|2022-01-01 00:02:15|        200|          0|\n",
      "|2022-01-01 00:02:30|        203|          0|\n",
      "|2022-01-01 00:02:45|        308|          0|\n",
      "+-------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 - Hourly Visit Counts:\n",
      "+-------------------+------------+\n",
      "|               hour|total_visits|\n",
      "+-------------------+------------+\n",
      "|2022-01-01 00:00:00|         196|\n",
      "|2022-01-01 01:00:00|         203|\n",
      "|2022-01-01 02:00:00|         201|\n",
      "|2022-01-01 03:00:00|         187|\n",
      "|2022-01-01 04:00:00|         195|\n",
      "+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 - Response Size Analysis:\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|status_code|         avg_size|request_count|min_size|max_size|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|        200|50565.73821994404|       214791|    1000|  100000|\n",
      "|        201|50542.00092736712|       214586|    1000|  100000|\n",
      "|        202|50454.74877758012|       214738|    1001|  100000|\n",
      "|        203|50423.19351991519|       214133|    1000|  100000|\n",
      "|        204|50402.07495866336|       213491|    1000|  100000|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's confirm what columns we have in each DataFrame\n",
    "print(\"Available columns in student1_logs:\")\n",
    "spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "print(\"\\nAvailable columns in student2_logs:\")\n",
    "spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Now let's modify our functions to only use available columns\n",
    "\n",
    "# Student 2: Session identification (Modified)\n",
    "def analyze_sessions():\n",
    "    query = \"\"\"\n",
    "    WITH time_gaps AS (\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            LAG(timestamp) OVER (\n",
    "                ORDER BY timestamp\n",
    "            ) as prev_timestamp,\n",
    "            status_code\n",
    "        FROM student2_logs\n",
    "    )\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        status_code,\n",
    "        CASE \n",
    "            WHEN (unix_timestamp(timestamp) - unix_timestamp(prev_timestamp)) > 1800 \n",
    "            OR prev_timestamp IS NULL \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END as new_session\n",
    "    FROM time_gaps\n",
    "    ORDER BY timestamp\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 3: Unique visitors per hour (Modified)\n",
    "def analyze_unique_visitors():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        date_trunc('hour', timestamp) as hour,\n",
    "        COUNT(*) as total_visits\n",
    "    FROM student1_logs\n",
    "    GROUP BY date_trunc('hour', timestamp)\n",
    "    ORDER BY hour\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 4: Average response size per status code (Original - should work)\n",
    "def analyze_avg_response_size():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        status_code,\n",
    "        AVG(response_size) as avg_size,\n",
    "        COUNT(*) as request_count,\n",
    "        MIN(response_size) as min_size,\n",
    "        MAX(response_size) as max_size\n",
    "    FROM student2_logs\n",
    "    GROUP BY status_code\n",
    "    ORDER BY status_code\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Try executing the queries\n",
    "try:\n",
    "    print(\"\\n=== Window Functions Analysis ===\")\n",
    "    \n",
    "    print(\"\\nStudent 2 - Session Identification:\")\n",
    "    analyze_sessions().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 3 - Hourly Visit Counts:\")\n",
    "    analyze_unique_visitors().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 4 - Response Size Analysis:\")\n",
    "    analyze_avg_response_size().show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    # Print the actual data for debugging\n",
    "    print(\"\\nSample data from student2_logs:\")\n",
    "    spark.sql(\"SELECT * FROM student2_logs LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOrBapHUot5U"
   },
   "source": [
    "# Task 3: Data Visualization (10 marks)\n",
    "\n",
    "Each member will visualize the results of their unique SQL queries using different chart types.\n",
    "\n",
    "### Student Visualization Type Examples\n",
    "\n",
    "- **Student 1: Line Chart (Hourly Traffic)**\n",
    "  - **Tool:** Matplotlib for rolling traffic visualization.\n",
    "\n",
    "- **Student 2: Bar Chart (Top 10 Failed Requests)**\n",
    "  - **Tool:** Seaborn for aggregated failure counts.\n",
    "\n",
    "- **Student 3: Heatmap (Hourly Unique Visitors)**\n",
    "  - **Tool:** Seaborn for visualizing traffic density.\n",
    "\n",
    "- **Student 4: Pie Chart (Response Code Distribution)**\n",
    "  - **Tool:** Matplotlib for status code proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations for all students...\n",
      "\n",
      "Student 1 - Rolling Traffic Line Chart:\n",
      "An error occurred while generating visualizations: name 'analyze_rolling_traffic' is not defined\n",
      "\n",
      "Debugging information:\n",
      "Available columns in the DataFrames:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ===================== Student 1: Line Chart of Rolling Traffic =====================\n",
    "def visualize_rolling_traffic():\n",
    "    # Get data from our previous SQL query\n",
    "    rolling_traffic_df = analyze_rolling_traffic()\n",
    "    pdf = rolling_traffic_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create line plot\n",
    "    plt.plot(pdf['hour'], pdf['rolling_3hour_traffic'], \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.title('3-Hour Rolling Traffic by Hour', fontsize=14, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Rolling Traffic Count', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations for peaks\n",
    "    max_traffic = pdf['rolling_3hour_traffic'].max()\n",
    "    plt.axhline(y=max_traffic, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 2: Bar Chart of Failed Requests =====================\n",
    "def visualize_failed_requests():\n",
    "    # Get data from previous SQL query\n",
    "    failed_requests_df = analyze_failed_requests()\n",
    "    pdf = failed_requests_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(24, 12))\n",
    "    \n",
    "    # Create bar plot using Seaborn\n",
    "    sns.barplot(x='status_code', y='response_size', data=pdf)\n",
    "    \n",
    "    plt.title('Top 10 Failed Requests by Response Size', fontsize=14, pad=20)\n",
    "    plt.xlabel('Status Code', fontsize=12)\n",
    "    plt.ylabel('Response Size', fontsize=12)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(pdf['response_size']):\n",
    "        plt.text(i, v, str(int(v)), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 3: Heatmap of Hourly Traffic =====================\n",
    "def visualize_hourly_traffic():\n",
    "    # Get data from previous SQL query\n",
    "    unique_visitors_df = analyze_unique_visitors()\n",
    "    pdf = unique_visitors_df.toPandas()\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    pdf['hour_of_day'] = pd.to_datetime(pdf['hour']).dt.hour\n",
    "    pdf['day'] = pd.to_datetime(pdf['hour']).dt.date\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = pdf.pivot_table(\n",
    "        values='total_visits', \n",
    "        index='day',\n",
    "        columns='hour_of_day',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_table, \n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='.0f',\n",
    "                cbar_kws={'label': 'Number of Visits'})\n",
    "    \n",
    "    plt.title('Traffic Density by Hour and Day', fontsize=14, pad=20)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Date', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 4: Pie Chart of Response Codes =====================\n",
    "def visualize_response_distribution():\n",
    "    # Get data from previous SQL query\n",
    "    response_dist_df = analyze_avg_response_size()\n",
    "    pdf = response_dist_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.pie(pdf['request_count'], \n",
    "            labels=pdf['status_code'],\n",
    "            autopct='%1.1f%%',\n",
    "            explode=[0.05] * len(pdf),\n",
    "            shadow=True)\n",
    "    \n",
    "    plt.title('Distribution of HTTP Status Codes', fontsize=14, pad=20)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to generate all visualizations\n",
    "def generate_all_visualizations():\n",
    "    try:\n",
    "        print(\"Generating visualizations for all students...\")\n",
    "        \n",
    "        print(\"\\nStudent 1 - Rolling Traffic Line Chart:\")\n",
    "        visualize_rolling_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 2 - Failed Requests Bar Chart:\")\n",
    "        visualize_failed_requests()\n",
    "        \n",
    "        print(\"\\nStudent 3 - Traffic Density Heatmap:\")\n",
    "        visualize_hourly_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 4 - Response Codes Pie Chart:\")\n",
    "        visualize_response_distribution()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating visualizations: {str(e)}\")\n",
    "        \n",
    "        # Print debugging information\n",
    "        print(\"\\nDebugging information:\")\n",
    "        print(\"Available columns in the DataFrames:\")\n",
    "        spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "        spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Generate all visualizations\n",
    "generate_all_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing using PySpark RDD\n",
    "\n",
    "## Task 1: Basic RDD Analysis (10 marks)\n",
    "Each member will create a custom function to parse and process the log entries.\n",
    "\n",
    "### Student Basic Extraction Examples\n",
    "- **Student 1**: Extract Timestamp and IP  \n",
    "    **Description**: Parse timestamp and IP address from logs.\n",
    "- **Student 2**: Extract URL and HTTP Method  \n",
    "    **Description**: Parse URL path and HTTP method from logs.\n",
    "- **Student 3**: Extract Status Code and Response Size  \n",
    "    **Description**: Parse HTTP status and response size from logs.\n",
    "- **Student 4**: Extract Log Message and IP Address  \n",
    "    **Description**: Parse log messages and corresponding IP addresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing log entries...\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 1 - TIMESTAMP AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address: 88.211.105.115\n",
      "Timestamp:  04/Mar/2022:14:17:48\n",
      "\n",
      "IP Address: 144.6.49.142\n",
      "Timestamp:  02/Sep/2022:15:16:00\n",
      "\n",
      "IP Address: 231.70.64.145\n",
      "Timestamp:  19/Jul/2022:01:31:31\n",
      "\n",
      "IP Address: 219.42.234.172\n",
      "Timestamp:  08/Feb/2022:11:34:57\n",
      "\n",
      "IP Address: 183.173.185.94\n",
      "Timestamp:  29/Aug/2023:03:07:11\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 2 - URL AND HTTP METHOD EXTRACTION\n",
      "----------------------------------------\n",
      "HTTP Method: POST\n",
      "URL Path:    /history/missions/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "HTTP Method: PUT\n",
      "URL Path:    /web-development/countdown/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /networking/technology/\n",
      "\n",
      "HTTP Method: GET\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\n",
      "----------------------------------------\n",
      "Status Code:   414\n",
      "Response Size: 12456 bytes\n",
      "\n",
      "Status Code:   0\n",
      "Response Size: 0 bytes\n",
      "\n",
      "Status Code:   201\n",
      "Response Size: 33093 bytes\n",
      "\n",
      "Status Code:   415\n",
      "Response Size: 68827 bytes\n",
      "\n",
      "Status Code:   205\n",
      "Response Size: 30374 bytes\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  144.6.49.142\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  219.42.234.172\n",
      "Log Message: Detailed system state information.\n",
      "\n",
      "IP Address:  183.173.185.94\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Student 1: Extract Timestamp and IP\n",
    "def extract_timestamp_ip(log_line):\n",
    "    \"\"\"Extract timestamp and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    timestamp_pattern = r'\\[([^\\]]+)\\]'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        timestamp = re.search(timestamp_pattern, text).group(1)\n",
    "        return (ip, timestamp)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 2: Extract URL and HTTP Method\n",
    "def extract_url_method(log_line):\n",
    "    \"\"\"Extract URL path and HTTP method from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'\"(GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (match.group(1), match.group(2))\n",
    "        return (\"Not Found\", \"Not Found\")\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 3: Extract Status Code and Response Size\n",
    "def extract_status_size(log_line):\n",
    "    \"\"\"Extract HTTP status code and response size from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'HTTP/[\\d.]+\" (\\d{3}) (\\d+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (int(match.group(1)), int(match.group(2)))\n",
    "        return (0, 0)\n",
    "    except Exception as e:\n",
    "        return (0, 0)\n",
    "\n",
    "# Student 4: Extract Log Message and IP Address\n",
    "def extract_message_ip(log_line):\n",
    "    \"\"\"Extract log message and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    message_pattern = r'(?:Warning|Update|Debug|Error|Info):(.*?)(?:\\s*$)'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        message = re.search(message_pattern, text).group(1).strip()\n",
    "        return (ip, message)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "def analyze_logs(data):\n",
    "    \"\"\"Analyze logs using the existing Spark session\"\"\"\n",
    "    print(\"\\nProcessing log entries...\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to RDD of log lines\n",
    "        logs_rdd = data.rdd.map(lambda row: row[0])\n",
    "        \n",
    "        # Student 1 Analysis\n",
    "        print(\"STUDENT 1 - TIMESTAMP AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd1_results = logs_rdd.map(extract_timestamp_ip).collect()\n",
    "        for ip, timestamp in rdd1_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address: {ip}\")\n",
    "            print(f\"Timestamp:  {timestamp}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 2 Analysis\n",
    "        print(\"STUDENT 2 - URL AND HTTP METHOD EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd2_results = logs_rdd.map(extract_url_method).collect()\n",
    "        for method, url in rdd2_results[:5]:  # Show first 5 results\n",
    "            print(f\"HTTP Method: {method}\")\n",
    "            print(f\"URL Path:    {url}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 3 Analysis\n",
    "        print(\"STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd3_results = logs_rdd.map(extract_status_size).collect()\n",
    "        for status, size in rdd3_results[:5]:  # Show first 5 results\n",
    "            print(f\"Status Code:   {status}\")\n",
    "            print(f\"Response Size: {size} bytes\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 4 Analysis\n",
    "        print(\"STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd4_results = logs_rdd.map(extract_message_ip).collect()\n",
    "        for ip, message in rdd4_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address:  {ip}\")\n",
    "            print(f\"Log Message: {message}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "\n",
    "# Execute the analysis\n",
    "analyze_logs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Two Advanced RDD Analysis (30 marks)\n",
    "Each member will perform unique advanced processing tasks.\n",
    "\n",
    "### Student Advanced Analysis Examples\n",
    "- **Student 1**: Calculate hourly visit counts per IP  \n",
    "    **Description**: Count visits grouped by hour and IP.\n",
    "- **Student 2**: Identify top 10 URLs by visit count  \n",
    "    **Description**: Aggregate visit counts and rank top URLs.\n",
    "- **Student 3**: Find average response size per URL  \n",
    "    **Description**: Compute average response size for each URL.\n",
    "- **Student 4**: Detect failed requests per IP  \n",
    "    **Description**: Identify IPs with the most failed requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advanced RDD Analysis Results\n",
      "==================================================\n",
      "\n",
      "Student 1 - Hourly Visit Counts per IP\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def advanced_rdd_analysis(data):\n",
    "    \"\"\"Perform advanced RDD analysis for all students\"\"\"\n",
    "    # Convert DataFrame to RDD\n",
    "    logs_rdd = data.rdd.map(lambda row: row[0])\n",
    "    \n",
    "    print(\"\\nAdvanced RDD Analysis Results\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Student 1: Calculate hourly visit counts per IP\n",
    "    def student1_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 1 - Hourly Visit Counts per IP\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_hour_ip(log_line):\n",
    "            ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "            timestamp_pattern = r'\\[([^\\]]+)\\]'\n",
    "            try:\n",
    "                ip = re.search(ip_pattern, log_line).group(1)\n",
    "                timestamp_str = re.search(timestamp_pattern, log_line).group(1)\n",
    "                # Parse timestamp and extract hour\n",
    "                dt = datetime.strptime(timestamp_str, '%d/%b/%Y:%H:%M:%S')\n",
    "                hour = dt.strftime('%H:00')\n",
    "                return ((ip, hour), 1)\n",
    "            except Exception as e:\n",
    "                return ((\"Error\", \"Error\"), 0)\n",
    "\n",
    "        # Map-Reduce to count visits\n",
    "        hourly_counts = logs_rdd \\\n",
    "            .map(extract_hour_ip) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .map(lambda x: (x[0][0], x[0][1], x[1])) \\\n",
    "            .sortBy(lambda x: (x[0], x[1]))\n",
    "\n",
    "        print(\"\\nSample of Hourly Visit Counts:\")\n",
    "        for ip, hour, count in hourly_counts.take(10):\n",
    "            print(f\"IP: {ip}, Hour: {hour}, Visits: {count}\")\n",
    "\n",
    "        # Calculate total visits per IP\n",
    "        total_visits = hourly_counts \\\n",
    "            .map(lambda x: (x[0], x[2])) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nTop 5 IPs by Total Visits:\")\n",
    "        for ip, total in total_visits.take(5):\n",
    "            print(f\"IP: {ip}, Total Visits: {total}\")\n",
    "\n",
    "    # Student 2: Identify top 10 URLs by visit count\n",
    "    def student2_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 2 - Top 10 URLs by Visit Count\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_url(log_line):\n",
    "            pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "            try:\n",
    "                url = re.search(pattern, log_line).group(1)\n",
    "                return (url, 1)\n",
    "            except Exception:\n",
    "                return (\"Error\", 0)\n",
    "\n",
    "        # Calculate URL visit counts\n",
    "        url_counts = logs_rdd \\\n",
    "            .map(extract_url) \\\n",
    "            .reduceByKey(lambda x, y: x + y) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nTop 10 Most Visited URLs:\")\n",
    "        for url, count in url_counts.take(10):\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Visit Count: {count}\\n\")\n",
    "\n",
    "        # Calculate percentage of total traffic for top URLs\n",
    "        total_visits = url_counts.map(lambda x: x[1]).sum()\n",
    "        print(\"\\nTraffic Distribution:\")\n",
    "        for url, count in url_counts.take(5):\n",
    "            percentage = (count / total_visits) * 100\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Percentage of Total Traffic: {percentage:.2f}%\\n\")\n",
    "\n",
    "    # Student 3: Find average response size per URL\n",
    "    def student3_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 3 - Average Response Size per URL\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_url_size(log_line):\n",
    "            url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "            size_pattern = r'HTTP/[\\d.]+\" \\d{3} (\\d+)'\n",
    "            try:\n",
    "                url = re.search(url_pattern, log_line).group(1)\n",
    "                size = int(re.search(size_pattern, log_line).group(1))\n",
    "                return (url, (size, 1))\n",
    "            except Exception:\n",
    "                return (\"Error\", (0, 0))\n",
    "\n",
    "        # Calculate average response size\n",
    "        avg_sizes = logs_rdd \\\n",
    "            .map(extract_url_size) \\\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "            .mapValues(lambda x: x[0] / x[1] if x[1] > 0 else 0) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "        print(\"\\nURL Response Size Analysis:\")\n",
    "        for url, avg_size in avg_sizes.take(10):\n",
    "            print(f\"URL: {url}\")\n",
    "            print(f\"Average Response Size: {avg_size:.2f} bytes\\n\")\n",
    "\n",
    "        # Calculate size distribution statistics\n",
    "        all_sizes = avg_sizes.map(lambda x: x[1]).collect()\n",
    "        if all_sizes:\n",
    "            print(\"\\nResponse Size Statistics:\")\n",
    "            print(f\"Maximum Average Size: {max(all_sizes):.2f} bytes\")\n",
    "            print(f\"Minimum Average Size: {min(all_sizes):.2f} bytes\")\n",
    "            print(f\"Overall Average Size: {sum(all_sizes)/len(all_sizes):.2f} bytes\")\n",
    "\n",
    "   # Student 4: Detect failed requests per IP\n",
    "    def student4_analysis(logs_rdd):\n",
    "        print(\"\\nStudent 4 - Failed Requests Analysis per IP\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        def extract_ip_status(log_line):\n",
    "            ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "            status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "            try:\n",
    "                ip = re.search(ip_pattern, log_line).group(1)\n",
    "                status = int(re.search(status_pattern, log_line).group(1))\n",
    "                is_failed = 1 if status >= 400 else 0\n",
    "                return (ip, (is_failed, 1))  # Count both failed and total requests\n",
    "            except Exception:\n",
    "                return (\"Error\", (0, 0))\n",
    "\n",
    "        # Calculate failed requests statistics\n",
    "        ip_stats = logs_rdd \\\n",
    "            .map(extract_ip_status) \\\n",
    "            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "            .mapValues(lambda x: {\n",
    "                'failed': x[0],\n",
    "                'total': x[1],\n",
    "                'failure_rate': (x[0] / x[1] * 100) if x[1] > 0 else 0\n",
    "            }) \\\n",
    "            .sortBy(lambda x: x[1]['failed'], ascending=False)\n",
    "\n",
    "        print(\"\\nFailed Requests Analysis:\")\n",
    "        for ip, stats in ip_stats.take(10):\n",
    "            print(f\"IP: {ip}\")\n",
    "            print(f\"Failed Requests: {stats['failed']}\")\n",
    "            print(f\"Total Requests: {stats['total']}\")\n",
    "            print(f\"Failure Rate: {stats['failure_rate']:.2f}%\\n\")\n",
    "\n",
    "        # Calculate overall statistics\n",
    "        stats_collected = ip_stats.collect()\n",
    "        if stats_collected:\n",
    "            total_failed = sum(stats[1]['failed'] for stats in stats_collected)\n",
    "            total_requests = sum(stats[1]['total'] for stats in stats_collected)\n",
    "            overall_failure_rate = (total_failed / total_requests * 100) if total_requests > 0 else 0\n",
    "            \n",
    "            print(\"\\nOverall Statistics:\")\n",
    "            print(f\"Total Failed Requests: {total_failed}\")\n",
    "            print(f\"Total Requests: {total_requests}\")\n",
    "            print(f\"Overall Failure Rate: {overall_failure_rate:.2f}%\")\n",
    "\n",
    "            # Additional Analysis\n",
    "            # Find IPs with highest failure rates (minimum 10 requests)\n",
    "            high_failure_ips = [(ip, stats) for ip, stats in stats_collected \n",
    "                              if stats['total'] >= 10]\n",
    "            high_failure_ips.sort(key=lambda x: x[1]['failure_rate'], reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 5 IPs with Highest Failure Rates (min. 10 requests):\")\n",
    "            for ip, stats in high_failure_ips[:5]:\n",
    "                print(f\"IP: {ip}\")\n",
    "                print(f\"Failure Rate: {stats['failure_rate']:.2f}%\")\n",
    "                print(f\"Failed Requests: {stats['failed']}\")\n",
    "                print(f\"Total Requests: {stats['total']}\\n\")\n",
    "\n",
    "    # Execute all students' analyses\n",
    "    try:\n",
    "        student1_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student2_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student3_analysis(logs_rdd)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        student4_analysis(logs_rdd)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "\n",
    "# Execute the analysis\n",
    "advanced_rdd_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Optimization and LSEPI Considerations (10 marks)\n",
    "Each member chooses two unique methods for optimization.\n",
    "\n",
    "### Student Optimization Methods\n",
    "- **Student 1**: Partition Strategies, Caching\n",
    "- **Student 2**: Caching, Bucketing & Indexing\n",
    "- **Student 3**: Partition Strategies, Bucketing & Indexing\n",
    "- **Student 4**: Caching, Partition Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Optimization Analyses for All Students\n",
      "============================================================\n",
      "\n",
      "Student 1 - Partition Strategies and Caching\n",
      "========================================\n",
      "\n",
      "Student 1 - Baseline Analysis (No Optimization)\n",
      "--------------------------------------------------\n",
      "Execution time: 46.72 seconds\n",
      "\n",
      "Student 1 - Analysis with Custom Partitioning\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 77.0 failed 1 times, most recent failure: Lost task 7.0 in stage 77.0 (TID 280) (192.168.1.159 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 3867, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))  # type: ignore[operator]\n            ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16936\\19262722.py\", line 39, in custom_partitioner\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 8315, in hash\n    return _invoke_function_over_seq_of_columns(\"hash\", cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 113, in _invoke_function_over_seq_of_columns\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 3867, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))  # type: ignore[operator]\n            ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16936\\19262722.py\", line 39, in custom_partitioner\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 8315, in hash\n    return _invoke_function_over_seq_of_columns(\"hash\", cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 113, in _invoke_function_over_seq_of_columns\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 332\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitioning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((time1\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtime3)\u001b[38;5;241m/\u001b[39mtime1)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% faster\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Run all optimizations\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m run_all_optimizations(data)\n",
      "Cell \u001b[1;32mIn[9], line 292\u001b[0m, in \u001b[0;36mrun_all_optimizations\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    290\u001b[0m s1 \u001b[38;5;241m=\u001b[39m Student1Optimization(data)\n\u001b[0;32m    291\u001b[0m baseline1, time1 \u001b[38;5;241m=\u001b[39m s1\u001b[38;5;241m.\u001b[39mbaseline_analysis()\n\u001b[1;32m--> 292\u001b[0m partition1, time2 \u001b[38;5;241m=\u001b[39m s1\u001b[38;5;241m.\u001b[39moptimized_with_partitioning()\n\u001b[0;32m    293\u001b[0m cache1, time3 \u001b[38;5;241m=\u001b[39m s1\u001b[38;5;241m.\u001b[39moptimized_with_caching()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerformance Improvement:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m, in \u001b[0;36mmeasure_execution_time.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      7\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      9\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     10\u001b[0m     execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m, in \u001b[0;36mStudent1Optimization.optimized_with_partitioning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(key) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Using 10 partitions\u001b[39;00m\n\u001b[0;32m     41\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     42\u001b[0m result \u001b[38;5;241m=\u001b[39m (rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_data)\n\u001b[0;32m     43\u001b[0m             \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;241m10\u001b[39m, custom_partitioner)\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;241m.\u001b[39mgroupByKey()\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;241m.\u001b[39mmapValues(\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m             \u001b[38;5;241m.\u001b[39mcollect())\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 77.0 failed 1 times, most recent failure: Lost task 7.0 in stage 77.0 (TID 280) (192.168.1.159 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 3867, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))  # type: ignore[operator]\n            ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16936\\19262722.py\", line 39, in custom_partitioner\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 8315, in hash\n    return _invoke_function_over_seq_of_columns(\"hash\", cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 113, in _invoke_function_over_seq_of_columns\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1247, in main\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 1239, in process\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py\", line 3867, in add_shuffle_key\n    buckets[partitionFunc(k) % numPartitions].append((k, v))  # type: ignore[operator]\n            ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16936\\19262722.py\", line 39, in custom_partitioner\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 174, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 8315, in hash\n    return _invoke_function_over_seq_of_columns(\"hash\", cols)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\functions.py\", line 113, in _invoke_function_over_seq_of_columns\n    sc = get_active_spark_context()\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\", line 248, in get_active_spark_context\n    raise RuntimeError(\"SparkContext or SparkSession should be created first.\")\nRuntimeError: SparkContext or SparkSession should be created first.\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "class Student1Optimization:\n",
    "    \"\"\"Student 1: Partition Strategies and Caching\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 1 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = rdd.map(self.extract_data).groupByKey().mapValues(len).collect()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 1 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Custom partitioner based on IP address\n",
    "        def custom_partitioner(key):\n",
    "            return hash(key) % 10  # Using 10 partitions\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_data)\n",
    "                    .partitionBy(10, custom_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(len)\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 1 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_data).cache()\n",
    "        result = cached_rdd.groupByKey().mapValues(len).collect()\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_data(log_line):\n",
    "        \"\"\"Extract IP and timestamp from log line\"\"\"\n",
    "        ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "        try:\n",
    "            ip = re.search(ip_pattern, log_line).group(1)\n",
    "            return (ip, 1)\n",
    "        except:\n",
    "            return (\"error\", 1)\n",
    "\n",
    "class Student2Optimization:\n",
    "    \"\"\"Student 2: Caching and Bucketing & Indexing\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 2 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_status)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for _ in x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 2 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_url_status).cache()\n",
    "        result = (cached_rdd.groupByKey()\n",
    "                          .mapValues(lambda x: sum(1 for _ in x))\n",
    "                          .collect())\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_bucketing(self):\n",
    "        \"\"\"Analysis with bucketing\"\"\"\n",
    "        print(\"\\nStudent 2 - Analysis with Bucketing\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert to DataFrame for bucketing\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"url\", StringType(), True),\n",
    "            StructField(\"status\", IntegerType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Extract and convert to DataFrame\n",
    "        rdd = (self.data.rdd.map(lambda row: row[0])\n",
    "                           .map(self.extract_url_status)\n",
    "                           .map(lambda x: (x[0], int(x[1]))))\n",
    "        \n",
    "        df = rdd.toDF(schema)\n",
    "        df.write.bucketBy(4, \"url\").mode(\"overwrite\").saveAsTable(\"bucketed_logs\")\n",
    "        \n",
    "        # Read and analyze bucketed data\n",
    "        result = spark.table(\"bucketed_logs\").groupBy(\"url\").count().collect()\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_url_status(log_line):\n",
    "        \"\"\"Extract URL and status code from log line\"\"\"\n",
    "        url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "        status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "        try:\n",
    "            url = re.search(url_pattern, log_line).group(1)\n",
    "            status = re.search(status_pattern, log_line).group(1)\n",
    "            return (url, status)\n",
    "        except:\n",
    "            return (\"error\", \"0\")\n",
    "\n",
    "class Student3Optimization:\n",
    "    \"\"\"Student 3: Partition Strategies and Bucketing & Indexing\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 3 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_size)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(x)/len(x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 3 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def url_partitioner(url):\n",
    "            return hash(url.split('/')[1] if '/' in url else url) % 8\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_url_size)\n",
    "                    .partitionBy(8, url_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(x)/len(x))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_bucketing(self):\n",
    "        \"\"\"Analysis with bucketing\"\"\"\n",
    "        print(\"\\nStudent 3 - Analysis with Bucketing\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert to DataFrame for bucketing\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "        \n",
    "        schema = StructType([\n",
    "            StructField(\"url\", StringType(), True),\n",
    "            StructField(\"size\", LongType(), True)\n",
    "        ])\n",
    "        \n",
    "        rdd = (self.data.rdd.map(lambda row: row[0])\n",
    "                           .map(self.extract_url_size)\n",
    "                           .map(lambda x: (x[0], int(x[1]))))\n",
    "        \n",
    "        df = rdd.toDF(schema)\n",
    "        df.write.bucketBy(4, \"url\").mode(\"overwrite\").saveAsTable(\"size_bucketed_logs\")\n",
    "        \n",
    "        result = spark.table(\"size_bucketed_logs\").groupBy(\"url\").avg(\"size\").collect()\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_url_size(log_line):\n",
    "        \"\"\"Extract URL and response size from log line\"\"\"\n",
    "        url_pattern = r'\"(?:GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "        size_pattern = r'HTTP/[\\d.]+\" \\d{3} (\\d+)'\n",
    "        try:\n",
    "            url = re.search(url_pattern, log_line).group(1)\n",
    "            size = int(re.search(size_pattern, log_line).group(1))\n",
    "            return (url, size)\n",
    "        except:\n",
    "            return (\"error\", 0)\n",
    "\n",
    "class Student4Optimization:\n",
    "    \"\"\"Student 4: Caching and Partition Strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def baseline_analysis(self):\n",
    "        \"\"\"Baseline analysis without optimizations\"\"\"\n",
    "        print(\"\\nStudent 4 - Baseline Analysis (No Optimization)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_ip_status)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_caching(self):\n",
    "        \"\"\"Analysis with caching\"\"\"\n",
    "        print(\"\\nStudent 4 - Analysis with Caching\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        cached_rdd = rdd.map(self.extract_ip_status).cache()\n",
    "        result = (cached_rdd.groupByKey()\n",
    "                          .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                          .collect())\n",
    "        cached_rdd.unpersist()\n",
    "        return result\n",
    "        \n",
    "    @measure_execution_time\n",
    "    def optimized_with_partitioning(self):\n",
    "        \"\"\"Analysis with custom partitioning\"\"\"\n",
    "        print(\"\\nStudent 4 - Analysis with Custom Partitioning\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        def ip_partitioner(ip):\n",
    "            # Partition based on the first octet of IP\n",
    "            try:\n",
    "                first_octet = int(ip.split('.')[0])\n",
    "                return first_octet % 8\n",
    "            except:\n",
    "                return 0\n",
    "            \n",
    "        rdd = self.data.rdd.map(lambda row: row[0])\n",
    "        result = (rdd.map(self.extract_ip_status)\n",
    "                    .partitionBy(8, ip_partitioner)\n",
    "                    .groupByKey()\n",
    "                    .mapValues(lambda x: sum(1 for status in x if int(status) >= 400))\n",
    "                    .collect())\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_ip_status(log_line):\n",
    "        \"\"\"Extract IP and status code from log line\"\"\"\n",
    "        ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "        status_pattern = r'HTTP/[\\d.]+\" (\\d{3})'\n",
    "        try:\n",
    "            ip = re.search(ip_pattern, log_line).group(1)\n",
    "            status = re.search(status_pattern, log_line).group(1)\n",
    "            return (ip, status)\n",
    "        except:\n",
    "            return (\"error\", \"0\")\n",
    "\n",
    "def run_all_optimizations(data):\n",
    "    \"\"\"Run all students' optimization analyses\"\"\"\n",
    "    \n",
    "    print(\"\\nRunning Optimization Analyses for All Students\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Student 1\n",
    "    print(\"\\nStudent 1 - Partition Strategies and Caching\")\n",
    "    print(\"=\" * 40)\n",
    "    s1 = Student1Optimization(data)\n",
    "    baseline1, time1 = s1.baseline_analysis()\n",
    "    partition1, time2 = s1.optimized_with_partitioning()\n",
    "    cache1, time3 = s1.optimized_with_caching()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Partitioning: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Caching: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 2\n",
    "    print(\"\\nStudent 2 - Caching and Bucketing\")\n",
    "    print(\"=\" * 40)\n",
    "    s2 = Student2Optimization(data)\n",
    "    baseline2, time1 = s2.baseline_analysis()\n",
    "    cache2, time2 = s2.optimized_with_caching()\n",
    "    bucket2, time3 = s2.optimized_with_bucketing()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Caching: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Bucketing: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 3\n",
    "    print(\"\\nStudent 3 - Partition Strategies and Bucketing\")\n",
    "    print(\"=\" * 40)\n",
    "    s3 = Student3Optimization(data)\n",
    "    baseline3, time1 = s3.baseline_analysis()\n",
    "    partition3, time2 = s3.optimized_with_partitioning()\n",
    "    bucket3, time3 = s3.optimized_with_bucketing()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Partitioning: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Bucketing: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "    \n",
    "    # Student 4\n",
    "    print(\"\\nStudent 4 - Caching and Partition Strategies\")\n",
    "    print(\"=\" * 40)\n",
    "    s4 = Student4Optimization(data)\n",
    "    baseline4, time1 = s4.baseline_analysis()\n",
    "    cache4, time2 = s4.optimized_with_caching()\n",
    "    partition4, time3 = s4.optimized_with_partitioning()\n",
    "    print(f\"\\nPerformance Improvement:\")\n",
    "    print(f\"Caching: {((time1 - time2)/time1)*100:.2f}% faster\")\n",
    "    print(f\"Partitioning: {((time1 - time3)/time1)*100:.2f}% faster\")\n",
    "\n",
    "# Run all optimizations\n",
    "run_all_optimizations(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "playing-word-embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
