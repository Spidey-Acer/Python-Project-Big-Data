{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A7LkGfi9QpjU"
   },
   "source": [
    "# Big Data Analytics [CN7031] CRWK 2024-25\n",
    "\n",
    "## Group ID: CN7031_Group136_2024\n",
    "\n",
    "### Group Members:\n",
    "1. **Navya Athoti**  \n",
    "    Email: u2793047@uel.ac.uk\n",
    "2. **Phalguna Avalagunta**  \n",
    "    Email: u2811669@uel.ac.uk\n",
    "3. **Nikhil Sai Damera**  \n",
    "    Email: u2810262@uel.ac.uk\n",
    "4. **Sai Kishore Dodda**  \n",
    "    Email: u2773584@uel.ac.uk\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate and Configure Spark\n",
    "\n",
    "In this section, we will initiate and configure Apache Spark, which is a powerful open-source processing engine for big data. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FdzItAjQzNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark\n",
    "\n",
    "# Cell 4 [Code]:\n",
    "# Import required libraries\n",
    "import os\n",
    "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME', 'Not set')}\")\n",
    "import sys\n",
    "\n",
    "# environment variables\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "def initialize_spark():\n",
    "    spark = (SparkSession.builder\n",
    "            .appName('CN7031_Group136_2024')\n",
    "            .config(\"spark.driver.memory\", \"4g\")\n",
    "            .config(\"spark.executor.memory\", \"4g\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "            .master(\"local[*]\")\n",
    "            .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFY_ihnakGwV"
   },
   "source": [
    "# Load Unstructured Data\n",
    "\n",
    "In this section, we will load and process unstructured data. Unstructured data refers to information that does not have a predefined data model or is not organized in a predefined manner. This type of data is typically text-heavy, but may also contain data such as dates, numbers, and facts.\n",
    "\n",
    "We will explore various techniques to handle and analyze unstructured data, including tokenization, vectorization, and the use of embeddings to capture semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "e0WKNOD1kh-K",
    "outputId": "9f864819-0942-4542-d184-53133f43c816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3000000 log entries\n"
     ]
    }
   ],
   "source": [
    "def load_data(spark, path=\"web.log\"):\n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "            \n",
    "        data = spark.read.text(path)\n",
    "        print(f\"Successfully loaded {data.count()} log entries\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the data loading\n",
    "try:\n",
    "    data = load_data(spark)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing using PySpark DataFrame [40 marks]\n",
    "\n",
    "---\n",
    "\n",
    "## DataFrame Creation with REGEX (10 marks)\n",
    "\n",
    "Each member will define a custom schema using REGEX to extract specific metrics from the dataset.\n",
    "\n",
    "### Student Metrics to Extract\n",
    "\n",
    "- **Student 1: IP Address, Timestamp, HTTP Method**\n",
    "    - **REGEX Example:** `(\\d+\\.\\d+\\.\\d+\\.\\d+) - - \\[(.*?)\\] \\\"([A-Z]+)`\n",
    "\n",
    "- **Student 2: HTTP Status Code, Response Size, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) (\\d+) \\[(.*?)\\]`\n",
    "\n",
    "- **Student 3: URL Path, IP Address, Response Size**\n",
    "    - **REGEX Example:** `\\\"[A-Z]+ (\\/.*?) HTTP.* (\\d+\\.\\d+\\.\\d+\\.\\d+) (\\d+)`\n",
    "\n",
    "- **Student 4: Log Message, HTTP Status Code, Timestamp**\n",
    "    - **REGEX Example:** `\\\".*\\\" (\\d+) .* \\[(.*?)\\] (.*)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Student 1 DataFrame Schema:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Student 1 Sample Data:\n",
      "+--------------+-------------------+-----------+\n",
      "|ip_address    |timestamp          |http_method|\n",
      "+--------------+-------------------+-----------+\n",
      "|88.211.105.115|2022-03-04 14:17:48|POST       |\n",
      "|144.6.49.142  |2022-09-02 15:16:00|POST       |\n",
      "|231.70.64.145 |2022-07-19 01:31:31|PUT        |\n",
      "|219.42.234.172|2022-02-08 11:34:57|POST       |\n",
      "|183.173.185.94|2023-08-29 03:07:11|GET        |\n",
      "+--------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 1 Validation Counts:\n",
      "+----------------+---------------+-----------------+\n",
      "|ip_address_count|timestamp_count|http_method_count|\n",
      "+----------------+---------------+-----------------+\n",
      "|         3000000|        3000000|          3000000|\n",
      "+----------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "Student 2 DataFrame Schema:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 2 Sample Data:\n",
      "+-----------+-------------+-------------------+\n",
      "|status_code|response_size|timestamp          |\n",
      "+-----------+-------------+-------------------+\n",
      "|414        |12456        |2022-03-04 14:17:48|\n",
      "|203        |97126        |2022-09-02 15:16:00|\n",
      "|201        |33093        |2022-07-19 01:31:31|\n",
      "|415        |68827        |2022-02-08 11:34:57|\n",
      "|205        |30374        |2023-08-29 03:07:11|\n",
      "+-----------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 2 Validation Counts:\n",
      "+-----------------+-------------------+---------------+\n",
      "|status_code_count|response_size_count|timestamp_count|\n",
      "+-----------------+-------------------+---------------+\n",
      "|          3000000|            3000000|        3000000|\n",
      "+-----------------+-------------------+---------------+\n",
      "\n",
      "\n",
      "Student 3 DataFrame Schema:\n",
      "root\n",
      " |-- url_path: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      "\n",
      "\n",
      "Student 3 Sample Data:\n",
      "+---------------------------+--------------+-------------+\n",
      "|url_path                   |ip_address    |response_size|\n",
      "+---------------------------+--------------+-------------+\n",
      "|/history/missions/         |88.211.105.115|12456        |\n",
      "|/security/firewall/        |144.6.49.142  |97126        |\n",
      "|/web-development/countdown/|231.70.64.145 |33093        |\n",
      "|/networking/technology/    |219.42.234.172|68827        |\n",
      "|/security/firewall/        |183.173.185.94|30374        |\n",
      "+---------------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 Validation Counts:\n",
      "+--------------+----------------+-------------------+\n",
      "|url_path_count|ip_address_count|response_size_count|\n",
      "+--------------+----------------+-------------------+\n",
      "|       3000000|         3000000|            3000000|\n",
      "+--------------+----------------+-------------------+\n",
      "\n",
      "\n",
      "Student 4 DataFrame Schema:\n",
      "root\n",
      " |-- log_message: string (nullable = true)\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Student 4 Sample Data:\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|log_message                             |status_code|timestamp          |\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "|POST /history/missions/ HTTP/2.0        |414        |2022-03-04 14:17:48|\n",
      "|POST /security/firewall/ HTTPS/1.0      |203        |2022-09-02 15:16:00|\n",
      "|PUT /web-development/countdown/ HTTP/1.0|201        |2022-07-19 01:31:31|\n",
      "|POST /networking/technology/ HTTP/1.0   |415        |2022-02-08 11:34:57|\n",
      "|GET /security/firewall/ HTTP/2.0        |205        |2023-08-29 03:07:11|\n",
      "+----------------------------------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 Validation Counts:\n",
      "+-----------------+-----------------+---------------+\n",
      "|log_message_count|status_code_count|timestamp_count|\n",
      "+-----------------+-----------------+---------------+\n",
      "|          3000000|          3000000|        3000000|\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Common imports and Spark initialization for all students\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import regexp_extract, to_timestamp, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the log file\n",
    "logs_df = spark.read.text(\"web.log\")\n",
    "\n",
    "# Student 1 (Navya A) - IP Address, Timestamp, HTTP Method\n",
    "student1_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\"),\n",
    "    regexp_extract(col(\"value\"), r'\"(\\w+)', 1).alias(\"http_method\")\n",
    ")\n",
    "\n",
    "# Student 2 - HTTP Status Code, Response Size, Timestamp\n",
    "student2_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Student 3 - URL Path, IP Address, Response Size\n",
    "student3_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"[A-Z]+ (.*?) HTTP', 1).alias(\"url_path\"),\n",
    "    regexp_extract(col(\"value\"), r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\", 1).alias(\"ip_address\"),\n",
    "    regexp_extract(col(\"value\"), r'\" \\d{3} (\\d+)', 1).cast(IntegerType()).alias(\"response_size\")\n",
    ")\n",
    "\n",
    "# Student 4 - Log Message, HTTP Status Code, Timestamp\n",
    "student4_df = logs_df.select(\n",
    "    regexp_extract(col(\"value\"), r'\"(.*?)\"', 1).alias(\"log_message\"),\n",
    "    regexp_extract(col(\"value\"), r'\" (\\d{3})', 1).alias(\"status_code\"),\n",
    "    to_timestamp(\n",
    "        regexp_extract(col(\"value\"), r\"\\[(.*?)\\]\", 1),\n",
    "        \"dd/MMM/yyyy:HH:mm:ss\"\n",
    "    ).alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Function to validate and show results for each student's DataFrame\n",
    "def validate_dataframe(df, student_num):\n",
    "    print(f\"\\nStudent {student_num} DataFrame Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    print(f\"\\nStudent {student_num} Sample Data:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Count non-null values for each column\n",
    "    print(f\"\\nStudent {student_num} Validation Counts:\")\n",
    "    df.select([\n",
    "        sum(col(c).isNotNull().cast(\"int\")).alias(f\"{c}_count\")\n",
    "        for c in df.columns\n",
    "    ]).show()\n",
    "\n",
    "# Validate each student's DataFrame\n",
    "validate_dataframe(student1_df, 1)\n",
    "validate_dataframe(student2_df, 2)\n",
    "validate_dataframe(student3_df, 3)\n",
    "validate_dataframe(student4_df, 4)\n",
    "\n",
    "# Register DataFrames as views for SQL queries later\n",
    "student1_df.createOrReplaceTempView(\"student1_logs\")\n",
    "student2_df.createOrReplaceTempView(\"student2_logs\")\n",
    "student3_df.createOrReplaceTempView(\"student3_logs\")\n",
    "student4_df.createOrReplaceTempView(\"student4_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F4fH4d-LJhH"
   },
   "source": [
    "# Task 2: Two Advanced DataFrame Analysis (20 marks)\n",
    "\n",
    "Each member will write unique SQL queries for the analysis:\n",
    "\n",
    "## SQL Query 1: Window Functions\n",
    "\n",
    "- **Student 1: Rolling hourly traffic per IP**\n",
    "    - **Description:** Calculate traffic count per IP over a sliding window.\n",
    "\n",
    "- **Student 2: Session identification**\n",
    "    - **Description:** Identify sessions based on timestamp gaps.\n",
    "\n",
    "- **Student 3: Unique visitors per hour**\n",
    "    - **Description:** Count distinct IPs for each hour.\n",
    "\n",
    "- **Student 4: Average response size per status code**\n",
    "    - **Description:** Compute averages grouped by status codes.\n",
    "\n",
    "## SQL Query 2: Aggregation Functions\n",
    "\n",
    "- **Student 1: Traffic patterns by URL path**\n",
    "    - **Description:** Analyze URL visits by hour.\n",
    "\n",
    "- **Student 2: Top 10 failed requests by size**\n",
    "    - **Description:** Identify the largest failed requests.\n",
    "\n",
    "- **Student 3: Response size distribution by status**\n",
    "    - **Description:** Show min, max, and avg sizes for each status.\n",
    "\n",
    "- **Student 4: Daily unique visitors**\n",
    "    - **Description:** Count unique IPs per day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "NyQQvVRrmhlQ",
    "outputId": "8b3fac55-bd76-43ab-dafe-1b0358025729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in student1_logs:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "\n",
      "Available columns in student2_logs:\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "=== Window Functions Analysis ===\n",
      "\n",
      "Student 2 - Session Identification:\n",
      "+-------------------+-----------+-----------+\n",
      "|          timestamp|status_code|new_session|\n",
      "+-------------------+-----------+-----------+\n",
      "|2022-01-01 00:01:34|        203|          1|\n",
      "|2022-01-01 00:01:50|        414|          0|\n",
      "|2022-01-01 00:02:15|        200|          0|\n",
      "|2022-01-01 00:02:30|        203|          0|\n",
      "|2022-01-01 00:02:45|        308|          0|\n",
      "+-------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 3 - Hourly Visit Counts:\n",
      "+-------------------+------------+\n",
      "|               hour|total_visits|\n",
      "+-------------------+------------+\n",
      "|2022-01-01 00:00:00|         196|\n",
      "|2022-01-01 01:00:00|         203|\n",
      "|2022-01-01 02:00:00|         201|\n",
      "|2022-01-01 03:00:00|         187|\n",
      "|2022-01-01 04:00:00|         195|\n",
      "+-------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Student 4 - Response Size Analysis:\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|status_code|         avg_size|request_count|min_size|max_size|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "|        200|50565.73821994404|       214791|    1000|  100000|\n",
      "|        201|50542.00092736712|       214586|    1000|  100000|\n",
      "|        202|50454.74877758012|       214738|    1001|  100000|\n",
      "|        203|50423.19351991519|       214133|    1000|  100000|\n",
      "|        204|50402.07495866336|       213491|    1000|  100000|\n",
      "+-----------+-----------------+-------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, let's confirm what columns we have in each DataFrame\n",
    "print(\"Available columns in student1_logs:\")\n",
    "spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "print(\"\\nAvailable columns in student2_logs:\")\n",
    "spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Now let's modify our functions to only use available columns\n",
    "\n",
    "# Student 2: Session identification (Modified)\n",
    "def analyze_sessions():\n",
    "    query = \"\"\"\n",
    "    WITH time_gaps AS (\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            LAG(timestamp) OVER (\n",
    "                ORDER BY timestamp\n",
    "            ) as prev_timestamp,\n",
    "            status_code\n",
    "        FROM student2_logs\n",
    "    )\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        status_code,\n",
    "        CASE \n",
    "            WHEN (unix_timestamp(timestamp) - unix_timestamp(prev_timestamp)) > 1800 \n",
    "            OR prev_timestamp IS NULL \n",
    "            THEN 1 \n",
    "            ELSE 0 \n",
    "        END as new_session\n",
    "    FROM time_gaps\n",
    "    ORDER BY timestamp\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 3: Unique visitors per hour (Modified)\n",
    "def analyze_unique_visitors():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        date_trunc('hour', timestamp) as hour,\n",
    "        COUNT(*) as total_visits\n",
    "    FROM student1_logs\n",
    "    GROUP BY date_trunc('hour', timestamp)\n",
    "    ORDER BY hour\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Student 4: Average response size per status code (Original - should work)\n",
    "def analyze_avg_response_size():\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        status_code,\n",
    "        AVG(response_size) as avg_size,\n",
    "        COUNT(*) as request_count,\n",
    "        MIN(response_size) as min_size,\n",
    "        MAX(response_size) as max_size\n",
    "    FROM student2_logs\n",
    "    GROUP BY status_code\n",
    "    ORDER BY status_code\n",
    "    \"\"\"\n",
    "    return spark.sql(query)\n",
    "\n",
    "# Try executing the queries\n",
    "try:\n",
    "    print(\"\\n=== Window Functions Analysis ===\")\n",
    "    \n",
    "    print(\"\\nStudent 2 - Session Identification:\")\n",
    "    analyze_sessions().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 3 - Hourly Visit Counts:\")\n",
    "    analyze_unique_visitors().show(5)\n",
    "    \n",
    "    print(\"\\nStudent 4 - Response Size Analysis:\")\n",
    "    analyze_avg_response_size().show(5)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    # Print the actual data for debugging\n",
    "    print(\"\\nSample data from student2_logs:\")\n",
    "    spark.sql(\"SELECT * FROM student2_logs LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOrBapHUot5U"
   },
   "source": [
    "# Task 3: Data Visualization (10 marks)\n",
    "\n",
    "Each member will visualize the results of their unique SQL queries using different chart types.\n",
    "\n",
    "### Student Visualization Type Examples\n",
    "\n",
    "- **Student 1: Line Chart (Hourly Traffic)**\n",
    "  - **Tool:** Matplotlib for rolling traffic visualization.\n",
    "\n",
    "- **Student 2: Bar Chart (Top 10 Failed Requests)**\n",
    "  - **Tool:** Seaborn for aggregated failure counts.\n",
    "\n",
    "- **Student 3: Heatmap (Hourly Unique Visitors)**\n",
    "  - **Tool:** Seaborn for visualizing traffic density.\n",
    "\n",
    "- **Student 4: Pie Chart (Response Code Distribution)**\n",
    "  - **Tool:** Matplotlib for status code proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualizations for all students...\n",
      "\n",
      "Student 1 - Rolling Traffic Line Chart:\n",
      "An error occurred while generating visualizations: name 'analyze_rolling_traffic' is not defined\n",
      "\n",
      "Debugging information:\n",
      "Available columns in the DataFrames:\n",
      "root\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- http_method: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- status_code: string (nullable = true)\n",
      " |-- response_size: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ===================== Student 1: Line Chart of Rolling Traffic =====================\n",
    "def visualize_rolling_traffic():\n",
    "    # Get data from our previous SQL query\n",
    "    rolling_traffic_df = analyze_rolling_traffic()\n",
    "    pdf = rolling_traffic_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create line plot\n",
    "    plt.plot(pdf['hour'], pdf['rolling_3hour_traffic'], \n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "    \n",
    "    plt.title('3-Hour Rolling Traffic by Hour', fontsize=14, pad=20)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel('Rolling Traffic Count', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations for peaks\n",
    "    max_traffic = pdf['rolling_3hour_traffic'].max()\n",
    "    plt.axhline(y=max_traffic, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 2: Bar Chart of Failed Requests =====================\n",
    "def visualize_failed_requests():\n",
    "    # Get data from previous SQL query\n",
    "    failed_requests_df = analyze_failed_requests()\n",
    "    pdf = failed_requests_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(24, 12))\n",
    "    \n",
    "    # Create bar plot using Seaborn\n",
    "    sns.barplot(x='status_code', y='response_size', data=pdf)\n",
    "    \n",
    "    plt.title('Top 10 Failed Requests by Response Size', fontsize=14, pad=20)\n",
    "    plt.xlabel('Status Code', fontsize=12)\n",
    "    plt.ylabel('Response Size', fontsize=12)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(pdf['response_size']):\n",
    "        plt.text(i, v, str(int(v)), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 3: Heatmap of Hourly Traffic =====================\n",
    "def visualize_hourly_traffic():\n",
    "    # Get data from previous SQL query\n",
    "    unique_visitors_df = analyze_unique_visitors()\n",
    "    pdf = unique_visitors_df.toPandas()\n",
    "    \n",
    "    # Reshape data for heatmap\n",
    "    pdf['hour_of_day'] = pd.to_datetime(pdf['hour']).dt.hour\n",
    "    pdf['day'] = pd.to_datetime(pdf['hour']).dt.date\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = pdf.pivot_table(\n",
    "        values='total_visits', \n",
    "        index='day',\n",
    "        columns='hour_of_day',\n",
    "        aggfunc='sum'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_table, \n",
    "                cmap='YlOrRd',\n",
    "                annot=True,\n",
    "                fmt='.0f',\n",
    "                cbar_kws={'label': 'Number of Visits'})\n",
    "    \n",
    "    plt.title('Traffic Density by Hour and Day', fontsize=14, pad=20)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Date', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================== Student 4: Pie Chart of Response Codes =====================\n",
    "def visualize_response_distribution():\n",
    "    # Get data from previous SQL query\n",
    "    response_dist_df = analyze_avg_response_size()\n",
    "    pdf = response_dist_df.toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.pie(pdf['request_count'], \n",
    "            labels=pdf['status_code'],\n",
    "            autopct='%1.1f%%',\n",
    "            explode=[0.05] * len(pdf),\n",
    "            shadow=True)\n",
    "    \n",
    "    plt.title('Distribution of HTTP Status Codes', fontsize=14, pad=20)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to generate all visualizations\n",
    "def generate_all_visualizations():\n",
    "    try:\n",
    "        print(\"Generating visualizations for all students...\")\n",
    "        \n",
    "        print(\"\\nStudent 1 - Rolling Traffic Line Chart:\")\n",
    "        visualize_rolling_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 2 - Failed Requests Bar Chart:\")\n",
    "        visualize_failed_requests()\n",
    "        \n",
    "        print(\"\\nStudent 3 - Traffic Density Heatmap:\")\n",
    "        visualize_hourly_traffic()\n",
    "        \n",
    "        print(\"\\nStudent 4 - Response Codes Pie Chart:\")\n",
    "        visualize_response_distribution()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating visualizations: {str(e)}\")\n",
    "        \n",
    "        # Print debugging information\n",
    "        print(\"\\nDebugging information:\")\n",
    "        print(\"Available columns in the DataFrames:\")\n",
    "        spark.sql(\"SELECT * FROM student1_logs\").printSchema()\n",
    "        spark.sql(\"SELECT * FROM student2_logs\").printSchema()\n",
    "\n",
    "# Generate all visualizations\n",
    "generate_all_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing using PySpark RDD\n",
    "\n",
    "## Task 1: Basic RDD Analysis (10 marks)\n",
    "Each member will create a custom function to parse and process the log entries.\n",
    "\n",
    "### Student Basic Extraction Examples\n",
    "- **Student 1**: Extract Timestamp and IP  \n",
    "    **Description**: Parse timestamp and IP address from logs.\n",
    "- **Student 2**: Extract URL and HTTP Method  \n",
    "    **Description**: Parse URL path and HTTP method from logs.\n",
    "- **Student 3**: Extract Status Code and Response Size  \n",
    "    **Description**: Parse HTTP status and response size from logs.\n",
    "- **Student 4**: Extract Log Message and IP Address  \n",
    "    **Description**: Parse log messages and corresponding IP addresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing log entries...\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 1 - TIMESTAMP AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address: 88.211.105.115\n",
      "Timestamp:  04/Mar/2022:14:17:48\n",
      "\n",
      "IP Address: 144.6.49.142\n",
      "Timestamp:  02/Sep/2022:15:16:00\n",
      "\n",
      "IP Address: 231.70.64.145\n",
      "Timestamp:  19/Jul/2022:01:31:31\n",
      "\n",
      "IP Address: 219.42.234.172\n",
      "Timestamp:  08/Feb/2022:11:34:57\n",
      "\n",
      "IP Address: 183.173.185.94\n",
      "Timestamp:  29/Aug/2023:03:07:11\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 2 - URL AND HTTP METHOD EXTRACTION\n",
      "----------------------------------------\n",
      "HTTP Method: POST\n",
      "URL Path:    /history/missions/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "HTTP Method: PUT\n",
      "URL Path:    /web-development/countdown/\n",
      "\n",
      "HTTP Method: POST\n",
      "URL Path:    /networking/technology/\n",
      "\n",
      "HTTP Method: GET\n",
      "URL Path:    /security/firewall/\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\n",
      "----------------------------------------\n",
      "Status Code:   414\n",
      "Response Size: 12456 bytes\n",
      "\n",
      "Status Code:   0\n",
      "Response Size: 0 bytes\n",
      "\n",
      "Status Code:   201\n",
      "Response Size: 33093 bytes\n",
      "\n",
      "Status Code:   415\n",
      "Response Size: 68827 bytes\n",
      "\n",
      "Status Code:   205\n",
      "Response Size: 30374 bytes\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\n",
      "----------------------------------------\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  144.6.49.142\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n",
      "IP Address:  Error\n",
      "Log Message: Error\n",
      "\n",
      "IP Address:  219.42.234.172\n",
      "Log Message: Detailed system state information.\n",
      "\n",
      "IP Address:  183.173.185.94\n",
      "Log Message: Unusual behavior detected. Investigate further.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Student 1: Extract Timestamp and IP\n",
    "def extract_timestamp_ip(log_line):\n",
    "    \"\"\"Extract timestamp and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    timestamp_pattern = r'\\[([^\\]]+)\\]'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        timestamp = re.search(timestamp_pattern, text).group(1)\n",
    "        return (ip, timestamp)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 2: Extract URL and HTTP Method\n",
    "def extract_url_method(log_line):\n",
    "    \"\"\"Extract URL path and HTTP method from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'\"(GET|POST|PUT|DELETE)\\s+([^\"\\s]+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (match.group(1), match.group(2))\n",
    "        return (\"Not Found\", \"Not Found\")\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "# Student 3: Extract Status Code and Response Size\n",
    "def extract_status_size(log_line):\n",
    "    \"\"\"Extract HTTP status code and response size from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    pattern = r'HTTP/[\\d.]+\" (\\d{3}) (\\d+)'\n",
    "    \n",
    "    try:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return (int(match.group(1)), int(match.group(2)))\n",
    "        return (0, 0)\n",
    "    except Exception as e:\n",
    "        return (0, 0)\n",
    "\n",
    "# Student 4: Extract Log Message and IP Address\n",
    "def extract_message_ip(log_line):\n",
    "    \"\"\"Extract log message and IP address from log entry\"\"\"\n",
    "    if isinstance(log_line, str):\n",
    "        text = log_line\n",
    "    else:\n",
    "        text = log_line.value  # For DataFrame rows\n",
    "        \n",
    "    ip_pattern = r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n",
    "    message_pattern = r'(?:Warning|Update|Debug|Error|Info):(.*?)(?:\\s*$)'\n",
    "    \n",
    "    try:\n",
    "        ip = re.search(ip_pattern, text).group(1)\n",
    "        message = re.search(message_pattern, text).group(1).strip()\n",
    "        return (ip, message)\n",
    "    except Exception as e:\n",
    "        return (\"Error\", \"Error\")\n",
    "\n",
    "def analyze_logs(data):\n",
    "    \"\"\"Analyze logs using the existing Spark session\"\"\"\n",
    "    print(\"\\nProcessing log entries...\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to RDD of log lines\n",
    "        logs_rdd = data.rdd.map(lambda row: row[0])\n",
    "        \n",
    "        # Student 1 Analysis\n",
    "        print(\"STUDENT 1 - TIMESTAMP AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd1_results = logs_rdd.map(extract_timestamp_ip).collect()\n",
    "        for ip, timestamp in rdd1_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address: {ip}\")\n",
    "            print(f\"Timestamp:  {timestamp}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 2 Analysis\n",
    "        print(\"STUDENT 2 - URL AND HTTP METHOD EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd2_results = logs_rdd.map(extract_url_method).collect()\n",
    "        for method, url in rdd2_results[:5]:  # Show first 5 results\n",
    "            print(f\"HTTP Method: {method}\")\n",
    "            print(f\"URL Path:    {url}\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 3 Analysis\n",
    "        print(\"STUDENT 3 - STATUS CODE AND RESPONSE SIZE EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd3_results = logs_rdd.map(extract_status_size).collect()\n",
    "        for status, size in rdd3_results[:5]:  # Show first 5 results\n",
    "            print(f\"Status Code:   {status}\")\n",
    "            print(f\"Response Size: {size} bytes\\n\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Student 4 Analysis\n",
    "        print(\"STUDENT 4 - LOG MESSAGE AND IP EXTRACTION\")\n",
    "        print(\"-\" * 40)\n",
    "        rdd4_results = logs_rdd.map(extract_message_ip).collect()\n",
    "        for ip, message in rdd4_results[:5]:  # Show first 5 results\n",
    "            print(f\"IP Address:  {ip}\")\n",
    "            print(f\"Log Message: {message}\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "\n",
    "# Execute the analysis\n",
    "analyze_logs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Two Advanced RDD Analysis (30 marks)\n",
    "Each member will perform unique advanced processing tasks.\n",
    "\n",
    "### Student Advanced Analysis Examples\n",
    "- **Student 1**: Calculate hourly visit counts per IP  \n",
    "    **Description**: Count visits grouped by hour and IP.\n",
    "- **Student 2**: Identify top 10 URLs by visit count  \n",
    "    **Description**: Aggregate visit counts and rank top URLs.\n",
    "- **Student 3**: Find average response size per URL  \n",
    "    **Description**: Compute average response size for each URL.\n",
    "- **Student 4**: Detect failed requests per IP  \n",
    "    **Description**: Identify IPs with the most failed requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Optimization and LSEPI Considerations (10 marks)\n",
    "Each member chooses two unique methods for optimization.\n",
    "\n",
    "### Student Optimization Methods\n",
    "- **Student 1**: Partition Strategies, Caching\n",
    "- **Student 2**: Caching, Bucketing & Indexing\n",
    "- **Student 3**: Partition Strategies, Bucketing & Indexing\n",
    "- **Student 4**: Caching, Partition Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "playing-word-embeddings.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
