
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PySpark Analysis Report</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0 20px;
        }
        header, footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
        }
        section {
            margin-bottom: 20px;
        }
        h1, h2, h3 {
            color: #333;
        }
        pre {
            background: #f4f4f4;
            padding: 10px;
            border: 1px solid #ddd;
            overflow-x: auto;
        }
        code {
            background: #f4f4f4;
            padding: 2px 4px;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <header>
        <h1>PySpark Data Processing Analysis</h1>
    </header>

    <section>
        <h2>Introduction</h2>
        <p>This report showcases the implementation of various data processing tasks using PySpark DataFrames and RDDs.</p>
    </section>

    <section>
        <h2>Data Processing Using PySpark DF</h2>
        <h3>1. DF Creation with REGEX</h3>
        <p>Each team member defines a custom schema using REGEX to extract specific metrics. Example:</p>
        <pre><code>from pyspark.sql.types import StructType, StructField, StringType
import re

# Sample REGEX
log_regex = r'(?P<ip>\d+\.\d+\.\d+\.\d+) - - \[(?P<timestamp>.*?)\] "(?P<method>[A-Z]+)'
</code></pre>
        <p>See the full implementation: <a href="task1_df_regex.py">task1_df_regex.py</a></p>

        <h3>2. Advanced DF Analysis</h3>
        <p>SQL queries using window and aggregation functions are used to derive insights. Example:</p>
        <pre><code>df.createOrReplaceTempView("logs")
query = '''
SELECT ip, COUNT(*) as request_count
FROM logs
GROUP BY ip
ORDER BY request_count DESC
LIMIT 10
'''
spark.sql(query).show()</code></pre>
        <p>See the full implementation: <a href="task2_df_analysis.py">task2_df_analysis.py</a></p>
    </section>

    <section>
        <h2>Data Processing Using PySpark RDD</h2>
        <h3>1. Basic RDD Analysis</h3>
        <p>Log entries are parsed and basic transformations are applied. Example:</p>
        <pre><code>from pyspark import SparkContext

sc = SparkContext()
logs_rdd = sc.textFile("weblogs.txt")

# Parse log entries
def parse_log(line):
    fields = line.split()
    return fields[0], fields[3], fields[5]

parsed_logs = logs_rdd.map(parse_log)
</code></pre>
        <p>See the full implementation: <a href="task1_rdd_basic.py">task1_rdd_basic.py</a></p>
    </section>

    <footer>
        <p>Generated by PySpark Research Assistant</p>
    </footer>
</body>
</html>
